{"meta":{"title":"CloudCver","subtitle":"","description":"","author":"CloudCver","url":"http://yoursite.com","root":"/"},"pages":[{"title":"关于","date":"2019-11-22T14:27:31.913Z","updated":"2019-11-22T14:27:31.889Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"archives","date":"2019-11-22T14:01:49.000Z","updated":"2019-11-22T14:02:16.993Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2019-11-22T13:57:11.000Z","updated":"2019-11-22T14:26:05.270Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"推荐博客&公众号","date":"2019-11-22T14:35:42.035Z","updated":"2019-11-22T14:35:41.691Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-11-22T14:27:02.430Z","updated":"2019-11-22T14:27:02.410Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"screen功能 linux后台运行程序","slug":"screen功能 linux后台运行程序","date":"2019-11-22T14:57:01.193Z","updated":"2019-11-22T14:55:57.286Z","comments":true,"path":"2019/11/22/screen功能 linux后台运行程序/","link":"","permalink":"http://yoursite.com/2019/11/22/screen%E5%8A%9F%E8%83%BD%20linux%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"screen功能 linux后台运行程序 1创建screen进程1screen #进入screen环境 2进程下运行文件1python a.py#运行a.py 3分离窗口先ctrl + a 进入命令模式，再按d分离窗口 4列出当前进程12345screen -ls #列出当前进程There are screens on: 22013.pts-37.hpcgpu30 (Detached) 9915.pts-33.hpcgpu30 (Detached) 5再次进入窗口1screen -r 22013 6杀死某个窗口杀死22013窗口 1kill -9 22013","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-11-21T14:24:30.256Z","updated":"2019-11-21T14:24:30.256Z","comments":true,"path":"2019/11/21/hello-world/","link":"","permalink":"http://yoursite.com/2019/11/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Pycharm更新代码到服务器端","slug":"Pycharm更新代码到服务器端","date":"2019-09-18T09:45:39.000Z","updated":"2019-11-22T15:00:36.628Z","comments":true,"path":"2019/09/18/Pycharm更新代码到服务器端/","link":"","permalink":"http://yoursite.com/2019/09/18/Pycharm%E6%9B%B4%E6%96%B0%E4%BB%A3%E7%A0%81%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF/","excerpt":"","text":"到设置-Deploment部署里，点加号，添加一个。选择SFTP 然后，随便起个名字喽。再设置Connection选项。如下图之后需要设置Mappings，设置本地代码地址以及服务器代码地址，服务器代码地址点击右侧打开后去找就行。 设置好后点击ok随后再代码部分右击会有deployment，分别有上传，下载，还有sync这样会同步所有不一样的，可以设置。sync下是这样界面，点击那个箭头可以改变更新方向，默认是按照时间更新。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"Pycharm","slug":"Pycharm","permalink":"http://yoursite.com/tags/Pycharm/"}]},{"title":"WGAN代码解读及实验总结","slug":"Faiss源码安装","date":"2019-09-16T09:45:39.000Z","updated":"2019-11-22T14:59:26.282Z","comments":true,"path":"2019/09/16/Faiss源码安装/","link":"","permalink":"http://yoursite.com/2019/09/16/Faiss%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85/","excerpt":"本篇文章提供了了Faiss安装的教程（CPU版安好了，GPU版有点问题，后补）","text":"本篇文章提供了了Faiss安装的教程（CPU版安好了，GPU版有点问题，后补） Step 1 下载FAISS1git clone https://github.com/facebookresearch/faiss Step 2 生成配置12cd faiss./configure --with-cuda=/usr/local/cuda-9.0 --prefix=/home/cver/software/faiss/ --with-python=python3 configure生成makfile文件，这里需要配置cuda目录，安装目录prefix Step 3 安装faiss我运行时存在一些问题不能安装GPU版本，这里我把GPU部分屏蔽了，以后再解决。 1234567891011vim makefile.inc-------------------------------NVCC = /usr/local/cuda-9.0/bin/nvccCUDA_ROOT = /usr/local/cuda-9.0//CUDA_ARCH = -gencode=arch=compute_35,code=compute_35 \\//-gencode=arch=compute_52,code=compute_52 \\//-gencode=arch=compute_60,code=compute_60 \\//-gencode=arch=compute_61,code=compute_61 \\//-gencode=arch=compute_70,code=compute_70 \\//-gencode=arch=compute_75,code=compute_75 修改之后，编译，安装 12makemake install Step 4 安装swing3swing3是C++转python的桥梁…具体可以查一下。首先要下载swing3 12wget https://sourceforge.net/projects/swig/files/swig/swig-3.0.12/swig-3.0.12.tar.gztar -zxvf swig-3.0.12.tar.gz 然后配置config 123bash ./configure --prefix=/home/cver/software/swing3 --without-pcremakemake install 配置环境变量 12345vim ~/.bashrc-----------------------添加以下两句export SWIG_PATH=/home/cver/software/swing3/binexport PATH=$SWIG_PATH:$PATH Step 5 生成python包回到faiss目录下 1make py 仍然需要修改配置文件 12345cd pythonvim Makefile---------------------我没有sudo权限，所以我安装到自己目录下$(PYTHON) setup.py install --prefix=/home/cver/.local 配置好后编译，安装 1make install","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"人脸识别","slug":"计算机视觉/人脸识别","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"faiss","slug":"faiss","permalink":"http://yoursite.com/tags/faiss/"}]},{"title":"python + web操作 爬虫 自动填写表单","slug":"python + web操作 爬虫 自动填写表单","date":"2019-08-21T02:41:15.000Z","updated":"2019-08-25T08:42:10.435Z","comments":true,"path":"2019/08/21/python + web操作 爬虫 自动填写表单/","link":"","permalink":"http://yoursite.com/2019/08/21/python%20+%20web%E6%93%8D%E4%BD%9C%20%E7%88%AC%E8%99%AB%20%E8%87%AA%E5%8A%A8%E5%A1%AB%E5%86%99%E8%A1%A8%E5%8D%95/","excerpt":"最近想做一个自动填表单提交的程序，用到了webdriver这个东西，做了简答的总结。","text":"最近想做一个自动填表单提交的程序，用到了webdriver这个东西，做了简答的总结。 Pre install package安装webdriver库(chorme版本)：找到你的版本，下载好放到Google/Chrome/Application文件夹下http://npm.taobao.org/mirrors/chromedriver/配置环境变量C:\\Users\\menglingjun\\AppData\\Local\\Google\\Chrome\\Application把这个加到环境变量，可能你那不一样，就是把chromedriver在的文件夹路径。 安装python selenium库 1pip3 install selenium 基本操作导入web库 1from selenium import webdriver 创建driver对象并读取某网页 12driver= webdriver.Chrome()driver.get('http://www.baidu.com') 获取网页上某元素并改内容网页上有个审查元素，可以右键看审查元素，这个大家都会。google浏览器中叫做“检查” 点击下图中那个按钮，就可以进入选取状态，此时我们选取左侧某个内容，右侧就会对应到他那一行代码，我们可以看到它的id，name等信息。根据这些id、name找到它的位置。这里可以by_id也可以by_name等等，然后send_key更改信息，代码如下： 12name = driver.find_element_by_id(\"id_username\")name.send_keys(\"data_operation\") 按钮类操作获取按钮，然后点击。 12login_button = driver.find_element_by_class_name(\"submit-row\")login_button.click() 总结通过这些操作，我们可以实现网页表单自动填写。","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://yoursite.com/categories/PYTHON/"}],"tags":[]},{"title":"VIM最近使用常见指令总结","slug":"VIM最近使用常见指令总结","date":"2019-08-15T02:41:15.000Z","updated":"2019-08-25T08:27:15.994Z","comments":true,"path":"2019/08/15/VIM最近使用常见指令总结/","link":"","permalink":"http://yoursite.com/2019/08/15/VIM%E6%9C%80%E8%BF%91%E4%BD%BF%E7%94%A8%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E6%80%BB%E7%BB%93/","excerpt":"本文总结了VIM编辑器的一些常见指令","text":"本文总结了VIM编辑器的一些常见指令 1 基本功 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 1.1 命令模式：用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。 i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 1.2 输入模式在命令模式下按下i就进入了输入模式。输入状态就直接打字就可以，记住以下几个指令 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 记住写完了按下ESC就回到命令模式1.1中 1.3 底线命令模式在命令模式下按下:（英文冒号）就进入了底线命令模式。底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 一般我们写完了，按ESC回退到命令模式，再按:qw回车保存并退出。 2 命令模式其他功能命令模式下页面变换 nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行 gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n n 为数字。光标向下移动 n 行(常用) [Ctrl] +[f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于[Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 搜索替换功能 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！(常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :1,$s/word1/word2/g 或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 参考文献[1]菜鸟教程 https://www.runoob.com/linux/linux-vim.html","categories":[{"name":"LINUX","slug":"LINUX","permalink":"http://yoursite.com/categories/LINUX/"}],"tags":[]},{"title":"python 参数模块argparse使用","slug":"python 参数模块argparse使用","date":"2019-08-13T02:41:15.000Z","updated":"2019-11-22T14:52:43.715Z","comments":true,"path":"2019/08/13/python 参数模块argparse使用/","link":"","permalink":"http://yoursite.com/2019/08/13/python%20%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9D%97argparse%E4%BD%BF%E7%94%A8/","excerpt":"python 参数模块argparse使用最近在用，做下总结！","text":"python 参数模块argparse使用最近在用，做下总结！ argparse是python的一个命令行解析包，用于编写可读性非常好的程序 1创建参数导入包；创建参数 12import argparseparser = argparse.ArgumentParser(\"name\") 2添加参数添加参数，包括参数名data_dir表示为–data-dirdefault默认值help注意：所有参数均为字符串型 1234parser.add_argument('--data-dir', default='', help='')parser.add_argument('--image-size', type=str, default='112,112', help='')parser.add_argument('--output', default='', help='path to save.')parser.add_argument('--num-samepairs',default=100) 3程序中用参数创建对象，然后获取它的值 12args = parser.parse_args()lfw_dir = args.data_dir 4命令行如何用1python3 verification.py --data-dir ../lfw --nfolds 10 --target lfw 5参数分组设置123456789101112add_argument_group()参数分组设置。当有分组命令的需求时可用，输入参数将归于所属分组下.parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='Create an image list or \\ make a record database by reading from an image list') parser.add_argument('prefix', help='prefix of input/output lst and rec files.') cgroup = parser.add_argument_group('Options for creating image lists') cgroup.add_argument('--list', action='store_true', help='') 6参数action有的参数带有action表示在命令行时直接加上这个参数，不赋值就直接按照action行动。例如：下面的行动是store_true，设置list为真，这样 1cgroup.add_argument('--list', action='store_true',help=' ') 这样程序中可以 12if args.list: make_list(args) 运行时直接加–list代表设置为真 1python3 im2rec.py --list --recursive ../../datasets/lfw/lfw ../../datasets/lfw2","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"argparse","slug":"argparse","permalink":"http://yoursite.com/tags/argparse/"}]},{"title":"python 文件操作OS总结","slug":"python 文件操作OS总结","date":"2019-08-13T02:41:15.000Z","updated":"2019-08-25T08:23:09.607Z","comments":true,"path":"2019/08/13/python 文件操作OS总结/","link":"","permalink":"http://yoursite.com/2019/08/13/python%20%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9COS%E6%80%BB%E7%BB%93/","excerpt":"本文总结了关于python文件系统的操作。","text":"本文总结了关于python文件系统的操作。 python 文件操作OS总结最近在用，坐下总结！ os的一些操作 os.listdir(path)列出该文件夹下面的目录，一般这么用 123folders_1 = os.listdir(prefix)for folder in folders_1: ...#遍历这个目录 os.makedirs(path)创建该路径 1os.makedirs(path, mode=0o777) os.path() 模块文件路径模块1、路径拆分 1os.path.split(path) 把路径分割成 dirname 和 basename，返回一个元组 2、路径合并 1os.path.join(path1, path2,...) 把目录和文件名合成一个路径 3、查看路径是否存在 1os.path.exists(path) 路径存在则返回True,路径损坏返回False 一般这么用 12if not os.path.exists(out_dir): os.makedirs(out_dir) open()打开文件 1open(name[, mode[, buffering]]) 模式如下例如： 123456789with open(img_root_path, 'r') as f: for line in f.readlines()[0:]: pair = line.strip().split(',') pairs_end += pairs_F[int(pair[0])],pairs_N[int(pair[1])] if pair == 'same': issame_list.append('True') else: issame_list.append('False')return pairs_end,issame_list 创建文件对象，又会衍生出他的方法 file 对象方法file.read([size])：size 未指定则返回整个文件，如果文件大小 &gt;2 倍内存则有问题，f.read()读到文件尾时返回””(空字串)。file.readline()：返回一行。这个在上面那个例子提到，我们一行一行读取。file.readlines([size]) ：返回包含size行的列表, size 未指定则返回全部行。for line in f: print line ：通过迭代器访问。f.write(“hello\\n”)：如果要写入字符串以外的数据,先将他转换为字符串。f.tell()：返回一个整数,表示当前文件指针的位置(就是到文件头的比特数)。f.seek(偏移量,[起始位置])：用来移动文件指针。偏移量: 单位为比特，可正可负起始位置: 0 - 文件头, 默认值; 1 - 当前位置; 2 - 文件尾f.close() 关闭文件","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://yoursite.com/categories/PYTHON/"}],"tags":[]},{"title":"insightface中recognition训练过程","slug":"insightface中recognition训练过程","date":"2019-08-09T02:41:15.000Z","updated":"2019-11-22T14:47:56.279Z","comments":true,"path":"2019/08/09/insightface中recognition训练过程/","link":"","permalink":"http://yoursite.com/2019/08/09/insightface%E4%B8%ADrecognition%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/","excerpt":"insightface中recognition训练过程","text":"insightface中recognition训练过程 1 config文件到目录/recognition/下有sample_config.py文件，这是给了个样例配置文件，我们复制一下起名config.py然后编辑 config.py 12cp sample_config.py config.pyvim config.py config下做了一些配置 default默认配置config配置network网络模型参数loss损失函数选择dataset数据集配置generate_config生成配置文件，把network、loss、dataset配置都加载config上。 然后看train.py文件 2 main主函数比较简单，参数填进去就可以了。这里需要特别注意下参数这一块。 1234def main(): global args args = parse_args() train_net(args) 到最开始那里有参数，如下，我们注意先来三个参数dataset、network、loss然后有一个generate_config生成配置，吧三个参数组合到config里，这里这三个参数要和config.py里面每个参数起的名字一样哦例如dataset : emore retina 当然你也可以自己改配置文件自己加network :r100 r50 r100fc… 1234567891011121314151617181920212223def parse_args(): parser = argparse.ArgumentParser(description='Train face network') # general parser.add_argument('--dataset', default=default.dataset, help='dataset config') parser.add_argument('--network', default=default.network, help='network config') parser.add_argument('--loss', default=default.loss, help='loss config') args, rest = parser.parse_known_args() generate_config(args.network, args.dataset, args.loss) parser.add_argument('--models-root', default=default.models_root, help='root directory to save model.') parser.add_argument('--pretrained', default=default.pretrained, help='pretrained model to load') parser.add_argument('--pretrained-epoch', type=int, default=default.pretrained_epoch, help='pretrained epoch to load') parser.add_argument('--ckpt', type=int, default=default.ckpt, help='checkpoint saving option. 0: discard saving. 1: save when necessary. 2: always save') parser.add_argument('--verbose', type=int, default=default.verbose, help='do verification testing and model saving every verbose batches') parser.add_argument('--lr', type=float, default=default.lr, help='start learning rate') parser.add_argument('--lr-steps', type=str, default=default.lr_steps, help='steps of lr changing') parser.add_argument('--wd', type=float, default=default.wd, help='weight decay') parser.add_argument('--mom', type=float, default=default.mom, help='momentum') parser.add_argument('--frequent', type=int, default=default.frequent, help='') parser.add_argument('--per-batch-size', type=int, default=default.per_batch_size, help='batch size in each context') parser.add_argument('--kvstore', type=str, default=default.kvstore, help='kvstore setting') args = parser.parse_args() return args 然后主要看train.py文件 3 train前面会选择GPU，然后make 存模型的文件夹，然后读数据集bin文件 加载预训练模型，如果有预训练模型，就直接加载checkpoint模型参数到arg_params，aux_params，然后sym = get_symbol(args)走一个前向传播，如果没有模型则声明个none。参数在后面fit时有用。 1234567891011if len(args.pretrained)==0: arg_params = None aux_params = None sym = get_symbol(args) if config.net_name=='spherenet': data_shape_dict = &#123;'data' : (args.per_batch_size,)+data_shape&#125; spherenet.init_weights(sym, data_shape_dict, args.num_layers)else: print('loading', args.pretrained, args.pretrained_epoch) _, arg_params, aux_params = mx.model.load_checkpoint(args.pretrained, args.pretrained_epoch) sym = get_symbol(args) 这里有个get_symbol前面有定义，我们到前面能看到，写了一大堆，我们看一点这里先获取你的网络的模型net_name这一套在/recognition/symbol有resnet等等的get_symbol，这个特征提完后，加全连接层，分别有softmax等等。 12345678910111213def get_symbol(args): embedding = eval(config.net_name).get_symbol() all_label = mx.symbol.Variable(&apos;softmax_label&apos;) gt_label = all_label is_softmax = True if config.loss_name==&apos;softmax&apos;: #softmax _weight = mx.symbol.Variable(&quot;fc7_weight&quot;, shape=(config.num_classes, config.emb_size), lr_mult=config.fc7_lr_mult, wd_mult=config.fc7_wd_mult, init=mx.init.Normal(0.01)) if config.fc7_no_bias: fc7 = mx.sym.FullyConnected(data=embedding, weight = _weight, no_bias = True, num_hidden=config.num_classes, name=&apos;fc7&apos;) else: _bias = mx.symbol.Variable(&apos;fc7_bias&apos;, lr_mult=2.0, wd_mult=0.0) fc7 = mx.sym.FullyConnected(data=embedding, weight = _weight, bias = _bias, num_hidden=config.num_classes, name=&apos;fc7&apos;) 回到train上来然后计算算力，这部分时flops_counter.py在/common下，有兴趣可以看一下。 1234567#计算消耗算力if config.count_flops: all_layers = sym.get_internals() _sym = all_layers['fc1_output'] FLOPs = flops_counter.count_flops(_sym, data=(1,3,image_size[0],image_size[1])) _str = flops_counter.flops_str(FLOPs) print('Network FLOPs: %s'%_str) 后面创建模型contex默认选择CPU，给ctxGPU序号symbol给出计算的输出即可。symbol: the network definitioncontext: the device (or a list of devices) to use for executiondata_names : the list of input data variable nameslabel_names : the list of input label variable names 1234model = mx.mod.Module( context = ctx, symbol = sym,) 主要看model.fit 1234567891011121314model.fit(train_dataiter, begin_epoch = begin_epoch, num_epoch = 999999, eval_data = val_dataiter, eval_metric = eval_metrics, kvstore = args.kvstore, optimizer = opt, #optimizer_params = optimizer_params, initializer = initializer, arg_params = arg_params, aux_params = aux_params, allow_missing = True, batch_end_callback = _batch_callback, epoch_end_callback = epoch_cb ) 参数解释如下train_data (DataIter) – Train DataIter.eval_data (DataIter) – 验证集数据，每个epoch会验证一下，可以选择noneepoch_end_callback (function or list of functions) – 每一次epoch调用一次batch_end_callback (function or list of function) – 每个batch调用一次，可以存放一些打印函数或者我们更新学习率kvstore (str or KVStore) – Defaults to ‘local’.optimizer (str or Optimizer) – Defaults to ‘sgd’.优化器initializer (Initializer) – The initializer is called to initialize the module parameters when they are not already initialized.arg_params (dict) – 初始化参数，默认没有aux_params (dict) – aux参数同上allow_missing (bool) – 在有预训练参数时是否允许丢失，也就是如果允许，则上面的参数万一有丢的，我们就随机初始化。begin_epoch (int) – 开始的次数，从几开始计数num_epoch (int) – 训练epoch 然后看到model每个epoch调用了回调函数，做了点啥呢主要时当数据量达到一定就打印数据，当测试效果好的时候就保存模型。 4运行1CUDA_VISIBLE_DEVICES=&apos;0,1,2,3&apos; python -u train.py --network r50 --loss arcface --dataset emore","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"人脸识别","slug":"计算机视觉/人脸识别","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"insightface","slug":"insightface","permalink":"http://yoursite.com/tags/insightface/"}]},{"title":"linux服务器下一些操作","slug":"linux服务器下一些操作","date":"2019-08-09T02:41:15.000Z","updated":"2019-11-22T14:38:15.110Z","comments":true,"path":"2019/08/09/linux服务器下一些操作/","link":"","permalink":"http://yoursite.com/2019/08/09/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/","excerpt":"本文总结了最近使用linux服务器用到的一些基本操作","text":"本文总结了最近使用linux服务器用到的一些基本操作 1 链接服务器1ssh menglingjun@hpcgpu9.ai.lycc.qihoo.net 输入密码… 2 创建软连接12345readlink -f ./insightface/产生软连接/data/menglingjun/insightface复制一下，然后到Home下ln -s /data/menglingjun/insightface . 3 查内存情况1df -h 4 查看文件详情123ll /~ home目录pwd显示当前工作目录绝对路径 5 查看GPU情况1nvidia-smi 6 安装PIP包到个人用户下1pip3 install packget --user 71pip --default-timeout=100 install -U packget 8 查看本机IP12ifconfigls -lht查看当前文件夹下文件信息 9 windows把文件传到linux下1234567scp -rp menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/datasets/lfw2/Micky_Arison/Micky_Arison_0001.jpg d:/ascp /d:/1.txt menglingjun@10.160.167.27:/home/menglingjun/ascp -r /d:/a menglingjun@10.160.167.27:/home/menglingjun/a#拷贝整个目录scp /d:/im2rec.py menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/src/data/scp menglingjun@10.173.226.203 :/home/menglingjun/a/d:/1.txt /d:/ascp /d:/generate_image_pairs.py menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/src/data/ 10 删除文件夹1rm -rf //","categories":[{"name":"LINUX","slug":"LINUX","permalink":"http://yoursite.com/categories/LINUX/"}],"tags":[]},{"title":"insightface测试recognition验证集效果全过程","slug":"insightface测试recognition验证集效果全过程","date":"2019-08-08T03:41:15.000Z","updated":"2019-11-22T14:51:30.468Z","comments":true,"path":"2019/08/08/insightface测试recognition验证集效果全过程/","link":"","permalink":"http://yoursite.com/2019/08/08/insightface%E6%B5%8B%E8%AF%95recognition%E9%AA%8C%E8%AF%81%E9%9B%86%E6%95%88%E6%9E%9C%E5%85%A8%E8%BF%87%E7%A8%8B/","excerpt":"insightface测试recognition验证集效果全过程","text":"insightface测试recognition验证集效果全过程 本过程在insightface代码下作实验，源代码参考https://github.com/deepinsight/insightface#pretrained-models实验测试recognition下的eval验证 1数据准备数据准备参考博文：insightface数据制作全过程记录https://blog.csdn.net/CLOUD_J/article/details/98769515 2eval验证在/recognition/eval下verification.py文件 1python3 verification.py --data-dir ../../datasets/lfw --model ../../models/model-r50-am-lfw/model,0 --nfolds 10 运行时出现了点问题，主要在于我们之前准备数据集时没有打乱数据导致正样本挤在一起，会使这一部分数据负样本为0，我们计算正确率会用负样本识别正确比上总负样样本数，分母出现0，所以做了更改calculate_val_far函数 1234567891011121314151617def calculate_val_far(threshold, dist, actual_issame): predict_issame = np.less(dist, threshold) true_accept = np.sum(np.logical_and(predict_issame, actual_issame)) false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame))) n_same = np.sum(actual_issame) n_diff = np.sum(np.logical_not(actual_issame)) #print(true_accept, false_accept) #print(n_same, n_diff) if n_same == 0: val = 1 else: val = float(true_accept) / float(n_same) if n_diff == 0: far = 0 else: far = float(false_accept) / float(n_diff) return val, far 3代码解析3.1主函数主函数首先做了模型载入，数据载入bin文件，然后对载入的模型分别做测试，检测各个模型数据效果。核心在这里，遍历ver_list不同数据集，遍历nets不同模型 12345678910111213141516if args.mode==0: for i in range(len(ver_list)): results = [] for model in nets: acc1, std1, acc2, std2, xnorm, embeddings_list = test(ver_list[i], model, args.batch_size, args.nfolds) print('[%s]XNorm: %f' % (ver_name_list[i], xnorm)) print('[%s]Accuracy: %1.5f+-%1.5f' % (ver_name_list[i], acc1, std1)) print('[%s]Accuracy-Flip: %1.5f+-%1.5f' % (ver_name_list[i], acc2, std2)) results.append(acc2) print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results))) elif args.mode==1: model = nets[0] test_badcase(ver_list[0], model, args.batch_size, args.target) else: model = nets[0] dumpR(ver_list[0], model, args.batch_size, args.target) 然后再提示几点， 1--model', default='../../models/model-r50-am-lfw/model,50 该参数代表模型路径的名字加上训练的epoch，../../models/model-r50-am-lfw是路径，然后model是名字；后面的50是epoch就是你可能在训练时会把多个epoch的结果输出，你可能验证不同epoch当时模型参数的效果。 3.2 test函数测试函数首先前向传播得到输出特征，然后计算它的范数，之后计算他的准确率 前向传播主要在这！这里对你的多个数据集遍历，当然你要是只有一个数据集就一次楼。data数据集，ba和bb随便起的名字，然后就这样不断取batchsize进行前向传播model.forward(db, is_train=False)然后将输出存到embeddings这个东西里，最后将多个数据集都存到embeddings_list 1234567891011121314151617181920212223242526for i in range( len(data_list) ): data = data_list[i] embeddings = None ba = 0 while ba&lt;data.shape[0]: bb = min(ba+batch_size, data.shape[0]) count = bb-ba _data = nd.slice_axis(data, axis=0, begin=bb-batch_size, end=bb) #print(_data.shape, _label.shape) time0 = datetime.datetime.now() if data_extra is None: db = mx.io.DataBatch(data=(_data,), label=(_label,)) else: db = mx.io.DataBatch(data=(_data,_data_extra), label=(_label,)) model.forward(db, is_train=False) net_out = model.get_outputs()#获取输出 _embeddings = net_out[0].asnumpy() time_now = datetime.datetime.now() diff = time_now - time0 time_consumed+=diff.total_seconds() #print(_embeddings.shape) if embeddings is None:#第一次的话先创建一个列表 embeddings = np.zeros( (data.shape[0], _embeddings.shape[1]) ) embeddings[ba:bb,:] = _embeddings[(batch_size-count):,:]#补进去 ba = bb embeddings_list.append(embeddings) 第二步，做了个范数计算计算一下特征的总平均范数 12345678910_xnorm = 0.0_xnorm_cnt = 0for embed in embeddings_list: for i in range(embed.shape[0]): _em = embed[i] _norm=np.linalg.norm(_em) #print(_em.shape, _norm) _xnorm+=_norm _xnorm_cnt+=1_xnorm /= _xnorm_cnt 第三步 计算准确率这里传入特征列表和标签列表还有nrof_folds，啥意思，这个是做K折检测的，分K份检测。 12_, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=nfolds)acc2, std2 = np.mean(accuracy), np.std(accuracy) 这里有一点注意，文中有个 embeddings = embeddings_list[0] + embeddings_list[1]我理解把两个数据集组合验证。 3.3evaluate评估函数首先先将数据集分了两块，就是原来是这样A1 A2 A3 A4 B1 B2这样A1和A2对比同类1改成这样A1 A3 B1奇数放一起A2 A4 B2偶数放一起 python中a::b代表从a开始以b单位增长 这里还搞了个thresholds作为阈值，会在评估函数里遍历寻找最好的阈值。完事做了两个评估calculate_roccalculate_val 1234567891011def evaluate(embeddings, actual_issame, nrof_folds=10, pca = 0): # Calculate evaluation metrics thresholds = np.arange(0, 4, 0.01) embeddings1 = embeddings[0::2] embeddings2 = embeddings[1::2] tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), nrof_folds=nrof_folds, pca = pca) thresholds = np.arange(0, 4, 0.001) val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds) return tpr, fpr, accuracy, val, val_std, far 3.4 calculate_roc第一步 先生命一个K折数据类1、这里assert是断言的意思，就是说后面那句话不对就直接报错；2、LFold在前面有声明类，就是调用kfold这个包3、 12345678910assert(embeddings1.shape[0] == embeddings2.shape[0]) assert(embeddings1.shape[1] == embeddings2.shape[1]) nrof_pairs = min(len(actual_issame), embeddings1.shape[0]) nrof_thresholds = len(thresholds) k_fold = LFold(n_splits=nrof_folds, shuffle=False) tprs = np.zeros((nrof_folds,nrof_thresholds)) fprs = np.zeros((nrof_folds,nrof_thresholds)) accuracy = np.zeros((nrof_folds)) indices = np.arange(nrof_pairs) 第二步，求了下范数距离欧式距离 123if pca==0: diff = np.subtract(embeddings1, embeddings2)#做减法 dist = np.sum(np.square(diff),1)#求平方和 第三步 遍历thresholds寻找最好的阈值。k_fold.split(indices)是分数据函数，用训练集取找好的阈值，用测试机打分。tprs暂时没用，关注accuracy准确率 12345678910111213141516171819202122232425262728for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)): #print('train_set', train_set) #print('test_set', test_set) if pca&gt;0: print('doing pca on', fold_idx) embed1_train = embeddings1[train_set] embed2_train = embeddings2[train_set] _embed_train = np.concatenate( (embed1_train, embed2_train), axis=0 ) #print(_embed_train.shape) pca_model = PCA(n_components=pca) pca_model.fit(_embed_train) embed1 = pca_model.transform(embeddings1) embed2 = pca_model.transform(embeddings2) embed1 = sklearn.preprocessing.normalize(embed1) embed2 = sklearn.preprocessing.normalize(embed2) #print(embed1.shape, embed2.shape) diff = np.subtract(embed1, embed2) dist = np.sum(np.square(diff),1) # Find the best threshold for the fold acc_train = np.zeros((nrof_thresholds)) for threshold_idx, threshold in enumerate(thresholds):#遍历找最好阈值 _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set]) best_threshold_index = np.argmax(acc_train) #print('threshold', thresholds[best_threshold_index]) for threshold_idx, threshold in enumerate(thresholds): tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set]) _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set]) 3.5 准确率计算核心函数在这，我们的改动也在这里。 说几个注意 np.less求最小值，求每个值与阈值相比，如果比阈值小则真。np.logical_and代表逻辑与的意思，就是两个numpy进行与，把预测和真实进行与一下得到tp，就是预测正确的正样本truepositive。np.logical_not(actual_issame)代表取反，给真是样本取反，fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))这样代表预测和真实样本取反的与就是错误预测的正样本。tn，fn一样。 1234567891011def calculate_accuracy(threshold, dist, actual_issame): predict_issame = np.less(dist, threshold) tp = np.sum(np.logical_and(predict_issame, actual_issame)) fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame))) tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame))) fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame)) tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn) fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn) acc = float(tp+tn)/dist.size return tpr, fpr, acc 最后结果如下Accuracy没有，我们只有一个数据集，这里我理解的是Acuuracy是单个数据集准确率，Accuracy-Flip和其他数据集混在一起，这里就看0.99675即可。 前面阈值打印出来是1.39 4结果 LFW CFP-FP renet-r50 99.63%(99.80%) 92.66%(92.74%) renet-r100 99.81%(99.77%) 95.94%(98.27%) 注意：括号内为github作者的结果，括号前为我的结果。结果取batchsize=16 目前对于差别有些疑问，还等待发现，如有大神能指点，还请指导。 5问题1、内存问题考虑减小batchsize 1python3 verification.py --data-dir ../../datasets/lfw2/ --model ../../models/model-r100-ii/model,0 --nfolds 10 --batch-size 16","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"人脸识别","slug":"计算机视觉/人脸识别","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"insightface","slug":"insightface","permalink":"http://yoursite.com/tags/insightface/"}]},{"title":"insightface数据制作全过程记录","slug":"insightface数据制作全过程记录","date":"2019-08-07T02:41:15.000Z","updated":"2019-11-22T14:51:35.453Z","comments":true,"path":"2019/08/07/insightface数据制作全过程记录/","link":"","permalink":"http://yoursite.com/2019/08/07/insightface%E6%95%B0%E6%8D%AE%E5%88%B6%E4%BD%9C%E5%85%A8%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/","excerpt":"insightface数据制作全过程记录","text":"insightface数据制作全过程记录 测试insightface时发现需要调整数据，insigtface/datasets属于存放数据的目录，insightface/src/data中存放了数据处理的一些代码，包括rec文件生成。 1数据对齐我们用lfw数据做实验，你也可以自己找数据。 lfw数据http://vis-www.cs.umass.edu/lfw/我下载的是这个原图像https://drive.google.com/file/d/1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp/view?usp=sharing datasets/lfwdata下存放数据lfwdata下分目录存放每一类的数据，每个人一个文件夹，里面存图。在src/align里提供了人脸对齐的代码，检测加对齐 1python3 align_lfw.py --input-dir ../../datasets/lfwdata --output-dir ../../datasets/lfw2 注意：这里cfp-fp数据集的对齐和lfw不一样，原因在于目录层级不一样，cfp-fp数据目录下每个person还分正脸侧脸，所以在遍历时不太一致，我们又写了align_cfp.py文件。这里我们复制一份align_lfw.py到align_cfp.py。然后改main函数里1、数据路径遍历方式变为ytf，这个你去看face_image有定义 12dataset = face_image.get_dataset('ytf', args.input_dir)print('dataset size', 'lfw', len(dataset)) 2、113行改一下生成新的路径存储图片这里 123img = img[:,:,0:3]_paths = fimage.image_path.split('/')a,b,c = _paths[-3], _paths[-2],_paths[-1] 2生成list文件这里insightface提供的face2rec2不能生成list，所以我们找了个程序稍后上传，用im2rec.py，这个程序也是生成rec的，我们加上参数–list 就生成lst文件 –recursive代表遍历文件下所有目录。../../datasets/lfw/lfw输出目录最后lfw代表list的名字，不用加后缀。../../datasets/lfw2图片目录 1python3 im2rec.py --list --recursive ../../datasets/lfw/train ../../datasets/lfw2 3生成rec &amp;idx 文件（依托于list）生成rec文件，把–list去掉../../datasets/lfw/train.lst代表lst的目录../../datasets/lfw2原图存在的目录 1python3 im2rec.py ../../datasets/lfw/train.lst ../../datasets/lfw2 4创建property配置文件直接创建一个名为property的文件，没有后缀写1000,112,112代表ID数量,尺寸,尺寸 目前datasets/lfw/目录下存在lfw.lst lfw.rec lfw.idx 5 创建pair文件为了做测试我们需要生成验证集用的bin文件，bin文件生成前需要做pair文件，就是一对一对的数据，每一行分别是图A的目录 空格 图B的目录 空格 标志0/1（代表两张图类别一致否）利用generate_image_pairs.py（源文件有问题，已修改）稍后上传，附录有../../datasets/lfw2对齐图像目录../../datasets/lfw/train.txt存放txt100要多少个数据 1python3 generate_image_pairs.py --data-dir ../../datasets/lfw2 --outputtxt ../../datasets/lfw/train.txt --num-samepairs 100 注意：这里生成pairs的方法不太好，数据集给了一些标准的pairs文件，我们可以写一个脚本取解读，具体如下： lfw在insightface里面有pair.txtcfp没有，只有一组FP对，需要我们自己写个脚本，这里我写好了上传来。我们依然放到src/data下cfp_make_bin.pycfp给了这个目录有一组对 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import mxnet as mxfrom mxnet import ndarray as ndimport argparseimport pickleimport sysimport osparser = argparse.ArgumentParser(description=&apos;Package LFW images&apos;)# generalparser.add_argument(&apos;--data-dir&apos;, default=&apos;&apos;, help=&apos;&apos;)parser.add_argument(&apos;--image-size&apos;, type=str, default=&apos;112,112&apos;, help=&apos;&apos;)parser.add_argument(&apos;--output&apos;, default=&apos;&apos;, help=&apos;path to save.&apos;)parser.add_argument(&apos;--num-samepairs&apos;,default=100)args = parser.parse_args()data_dir = args.data_dirimage_size = [int(x) for x in args.image_size.split(&apos;,&apos;)]pairs_end = []def get_paths(): pairs = [] prefix = os.path.join(data_dir,&apos;Protocol/&apos;) #prefix = &quot;/Split/&quot; prefix_F = os.path.join(prefix, &quot;Pair_list_F.txt&quot;) pairs_F = [] prefix_P = os.path.join(prefix,&quot;Pair_list_P.txt&quot;) pairs_P = [] pairs_end = [] issame_list = [] #读pairlist文件 with open(prefix_F, &apos;r&apos;) as f: for line in f.readlines()[0:]: pair = line.strip().split() pairs_F.append(pair[1]) print(len(pairs_F)) with open(prefix_P, &apos;r&apos;) as f: for line in f.readlines()[0:]: pair = line.strip().split() pairs_P.append(pair[1]) print(len(pairs_P)) #读pair文件 prefix = os.path.join(data_dir,&quot;Protocol/Split/FP&quot;) folders_1 = os.listdir(prefix) for folder in folders_1: sublist = [] same_list = [] pairtxt = os.listdir(os.path.join(prefix, folder)) for pair in pairtxt: img_root_path = os.path.join(prefix, folder, pair) with open(img_root_path, &apos;r&apos;) as f: for line in f.readlines()[0:]: #print(line) pair1 = line.strip().split(&apos;,&apos;) #print(pair) pairs_end += (os.path.join(data_dir,&apos;Protocol/&apos;,pairs_F[int(pair1[0])-1]),os.path.join(data_dir,&apos;Protocol/&apos;,pairs_P[int(pair1[1])-1])) #print(pair) if pair == &apos;same.txt&apos;: #print(&apos;ok!&apos;) issame_list.append(True) else: issame_list.append(False) return pairs_end,issame_list 6 生成验证集bin文件完事后利用/src/data/下的 lfw2pack.py生成bin文件但是有点问题，需要修改下，参考这篇博客https://blog.csdn.net/hanjiangxue_wei/article/details/86566348修改lfw2pack.py中19行，打#的为更改的，改为两个参数，一个是txt读出来的列表，另一个是总数量。注意：cfp跳过就可以了 1234567891011121314151617181920212223242526272829303132333435363738import mxnet as mxfrom mxnet import ndarray as ndimport argparseimport pickleimport sysimport ossys.path.append(os.path.join(os.path.dirname(__file__), &apos;..&apos;, &apos;eval&apos;))import lfwparser = argparse.ArgumentParser(description=&apos;Package LFW images&apos;)# generalparser.add_argument(&apos;--data-dir&apos;, default=&apos;&apos;, help=&apos;&apos;)parser.add_argument(&apos;--image-size&apos;, type=str, default=&apos;112,112&apos;, help=&apos;&apos;)parser.add_argument(&apos;--output&apos;, default=&apos;&apos;, help=&apos;path to save.&apos;)parser.add_argument(&apos;--num-samepairs&apos;,default=100)args = parser.parse_args()lfw_dir = args.data_dirimage_size = [int(x) for x in args.image_size.split(&apos;,&apos;)]lfw_pairs = lfw.read_pairs(os.path.join(lfw_dir, &apos;train.txt&apos;))print(lfw_pairs)lfw_paths, issame_list = lfw.get_paths(lfw_pairs,int(args.num_samepairs)+1)#, &apos;jpg&apos;)lfw_bins = []#lfw_data = nd.empty((len(lfw_paths), 3, image_size[0], image_size[1]))print(len(issame_list))i = 0for path in lfw_paths: with open(path, &apos;rb&apos;) as fin: _bin = fin.read() lfw_bins.append(_bin) #img = mx.image.imdecode(_bin) #img = nd.transpose(img, axes=(2, 0, 1)) #lfw_data[i][:] = img i+=1 if i%1000==0: print(&apos;loading lfw&apos;, i)with open(args.output, &apos;wb&apos;) as f: pickle.dump((lfw_bins, issame_list), f, protocol=pickle.HIGHEST_PROTOCOL) 对应的get_paths这个文件存在src\\eval\\lfw.py下，把他也改了 1234567891011121314151617181920212223def get_paths(pairs, same_pairs): nrof_skipped_pairs = 0 path_list = [] issame_list = [] cnt = 1 for pair in pairs: path0 = pair[0] path1 = pair[1] if cnt &lt; same_pairs: issame = True else: issame = False if os.path.exists(path0) and os.path.exists(path1): # Only add the pair if both paths exist path_list += (path0,path1) issame_list.append(issame) else: print(&apos;not exists&apos;, path0, path1) nrof_skipped_pairs += 1 cnt += 1 if nrof_skipped_pairs&gt;0: print(&apos;Skipped %d image pairs&apos; % nrof_skipped_pairs) return path_list, issame_list 之后再运行 1python3 lfw2pack.py --data-dir ../../datasets/lfw --output ../../datasets/lfw/lfw.bin --num-samepairs 300 附录generate_image_pairs.py、 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# coding:utf-8import sysimport osimport randomimport timeimport itertoolsimport pdbimport argparse#src = '../../datasets/lfw2'#dst = open('../../datasets/lfw/train.txt', 'a')parser = argparse.ArgumentParser(description='generate image pairs')# generalparser.add_argument('--data-dir', default='', help='')parser.add_argument('--outputtxt', default='', help='path to save.')parser.add_argument('--num-samepairs',default=100)args = parser.parse_args()cnt = 0same_list = []diff_list = []list1 = []list2 = []folders_1 = os.listdir(args.data_dir)dst = open(args.outputtxt, 'a')count = 0dst.writelines('\\n')# 产生相同的图像对for folder in folders_1: sublist = [] same_list = [] imgs = os.listdir(os.path.join(args.data_dir, folder)) for img in imgs: img_root_path = os.path.join(args.data_dir, folder, img) sublist.append(img_root_path) list1.append(img_root_path) for item in itertools.combinations(sublist, 2): for name in item: same_list.append(name) if len(same_list) &gt; 10 and len(same_list) &lt; 13: for j in range(0, len(same_list), 2): if count &lt; int(args.num_samepairs):#数量可以修改 dst.writelines(same_list[j] + ' ' + same_list[j+1]+ ' ' + '1' + '\\n') count += 1 if count &gt;= int(args.num_samepairs): breaklist2 = list1.copy()# 产生不同的图像对diff = 0print(count)# 如果不同的图像对远远小于相同的图像对，则继续重复产生，直到两者相差很小while True: random.seed(time.time() * 100000 % 10000) random.shuffle(list2) for p in range(0, len(list2) - 1, 2): if list2[p] != list2[p + 1]: dst.writelines(list2[p] + ' ' + list2[p + 1] + ' ' + '0'+ '\\n') diff += 1 if diff &gt;= count: break #print(diff) if diff &lt; count: #print('--') continue else: break 有问题请留言，最近一两周在做这个可以更新帖子及问题 Next：insightface测试验证集效果全过程https://blog.csdn.net/CLOUD_J/article/details/98882718","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"人脸识别","slug":"计算机视觉/人脸识别","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"insightface","slug":"insightface","permalink":"http://yoursite.com/tags/insightface/"}]},{"title":"图像分割FCN全卷积神经网络","slug":"图像分割FCN全卷积神经网络","date":"2019-05-29T08:37:41.000Z","updated":"2019-11-22T16:13:23.029Z","comments":true,"path":"2019/05/29/图像分割FCN全卷积神经网络/","link":"","permalink":"http://yoursite.com/2019/05/29/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2FCN%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"FCN全卷积神经网络是图像分割的基础网络","text":"FCN全卷积神经网络是图像分割的基础网络 FCN实现了端到端的网络，输入mxm图像，输出为mxmxc的图像，其中c代表种类。 1思想概述FCN叫做全卷积神经网络，顾名思义所有曾都是卷积层！也就是抛掉了全连接层，这是第一个改变；再者卷积神经网络卷到最后特征图尺寸越来越少，分辨率比较小，不适合我们做图像分割，好，这里引入一个上采样的做法，卷积完之后再上采样到大尺寸图；网络又考虑层数不断叠加后原图的信息丢失的比较多，那么我们这里引入一个跳层结构，把前面的第四层、第三层特征引过来叠加一下。有种resnet思想。 2FCN网络图我们再看一下下面这个图网络的主题，从左到右卷积、池化一顿操作猛如虎（蓝色卷积，绿色池化），到了最右边16*16*4096，全连接层丢了，先给他弄到21通道再上采样到34*34*21然后合并featuremap4后再上采样，最后经过一系列得到最后500*500*21的特征图。之后怎么做？很多人没有说好，我们平时做分类是得到全连接的1*21这样对他进行softmax，21类，每类有个分数存在1*21的向量中，好我们这500*500*21是像素级别的分类，总共有500*500个像素，每个像素属于21个类别的分别得分是啥，这样用softmax计算损失。所以我们看那些标注图是每个像素有个颜色代表它所属于的类别。端到端！伟大！ 端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。 那么接下来就要解决细节问题，上采样怎么实现？怎么合并数据？ 3上采样对于低分辨率的特征图，我们常常采用上采样的方式将它还原高分辨率，这里陈述三种方法。 3.1双线性插值上采样首先线形插值是什么？线形插值就是知道两个点的值，连一条直线，来确定中间的点的值，具体怎么做，我们找(x1,y1)、(x2,y2)连成一条直线，[x1,x2]中的点就可以用线上的点表示。双线性插值是一个三维的坐标系，我们找到四个点来确定中心点坐标，如下图为网上找的一个例子。 这种方法计算简单，无需训练。 3.2反卷积上采样3.2.1 怎样上采样普通的卷积操作，会使得分辨率降低，如下图3*3的卷积核去卷积4*4得到2*2的输出。上采样的过程也是卷积，那么怎么会得到分辨率提高呢？之前我们看卷积时有个保持输出与输入同分辨率的方法就是周围补0，嗯嗯。那么你是否灵机一动，那要是让分辨率提高呢？是不是再多补一些0，对的看看图2。 图1 常规卷积操作图2 上采样操作（四周补0） 其实呢作者在这又换了个方法，你想一下，只在四周补0会导致最边上的信息不太好，那我们把这个信息平均下，在每个像素与像素之间补0呢，看图3就是这么操作的。 图3 反卷积插零 3.2.2 反卷积补多少零？然后我们看一下参数的计算，先看一下四周补0的情况。下图给出了发卷积的参数，我们要保持转置卷积的输出核卷积的输入相同也就是i=o’，转置卷积就是反卷积。这里主要算出p为多少就是补多少个0，看我图5给出了计算。图4 四周补零反卷积图5 四周补0计算 再看一下反卷积间隙补0的计算如图6所示。其中插零输入是先往里面插入i‘-1个0。再计算p’ 图6 间隙补0反卷积图7 间隙补零反卷积计算 3.3反池化上采样反池化可以用下图来理解，再池化时需要记录下池化的位置，反池化时把池化的位置直接还原，其他位置填0。 3.4 小结三种方法各有优缺，双线性插值方法实现简单，无需训练；反卷积上采样需要训练，但能更好的还原特征图； 4跳层结构Skip-Layer跳层结构是啥呢，就是把第四层的featuremap考虑进来和最后一层得到的featuremap上采样合并信息再进行上采样。这个结构叫做跳层。那么实际你可以再结合第三层featuremap这样上次样倍数就需要提高，考虑的源信息就越多。 5再看FCN网络之后我们再看FCN这个网络，如果你只考虑最后一层信息，进行上采样，得到下图，这是在ALEXnet上做了修改，把最后的全连接层去掉改为卷积操作，最后卷出来16*16*21的featuremap进行了一个步长为32的双线性插值上采样得到500*500*21的图，可想而知这家伙肯定差太多了，中间这么多点都是插值插出来的！继续，改一下，这里把那个上采样改为2倍的反卷积采样，上采样到34*34*21，然后考虑第四层featuremap对他进行1*1*21卷积得到34*34*21的图，之后再合并两个信息，直接对应元素相加，得到34*34*21然后进行步长16的上采样。在这之后考虑还不行就在考虑第三个特征层加进去！ 6实验结果实验分别对FCN32、16、8的结构做了实验，可以看出考虑浅层的信息越多，实验效果越好！ 7小结FCN给我们提供了一个上采样的思路来解决图像分辨率低的问题，以及跳层结构来考虑浅层网络的特征来考虑多图像信息。 参考文献[1]FCN中反卷积、上采样、双线性插值之间的关系https://blog.csdn.net/u011771047/article/details/72872742/","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"语义分割","slug":"计算机视觉/语义分割","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"}],"tags":[{"name":"FCN","slug":"FCN","permalink":"http://yoursite.com/tags/FCN/"}]},{"title":"目标检测CNN经典网络（四）R-FCN","slug":"目标检测CNN经典网络（四）R-FCN","date":"2019-05-09T04:41:15.000Z","updated":"2019-11-22T14:45:36.350Z","comments":true,"path":"2019/05/09/目标检测CNN经典网络（四）R-FCN/","link":"","permalink":"http://yoursite.com/2019/05/09/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E5%9B%9B%EF%BC%89R-FCN/","excerpt":"前面所说的RCNN作为最早的目标检测网络，具有一些问题，其对每一个ROI都做一遍卷积（费时）；对于感兴趣区域的筛选也费时；随后FastR-CNN、Faster-CNN给出了解决方案。之后学者又去思考后面的全连接层能不能去掉，或者不让每一个感兴趣区域都走一遍全连接，这也很费时间啊！（我们了解在图像分类中，人们都很讨厌全连接层，参数量太大），为此R-FCN出现了！","text":"前面所说的RCNN作为最早的目标检测网络，具有一些问题，其对每一个ROI都做一遍卷积（费时）；对于感兴趣区域的筛选也费时；随后FastR-CNN、Faster-CNN给出了解决方案。之后学者又去思考后面的全连接层能不能去掉，或者不让每一个感兴趣区域都走一遍全连接，这也很费时间啊！（我们了解在图像分类中，人们都很讨厌全连接层，参数量太大），为此R-FCN出现了！ 1回顾回顾R-CNN系列的方案都是在解决一个计算量计算时间的问题，R-CNN存在一些问题，SPP-net提出金字塔池化层，将卷积放在前面，实现了一张图卷积一次就行，降低了大量时间！Fast R-CNN提出了ROI池化，引入并行的多任务损失函数，也提高了效率！到了Faster R-CNN认为区域建议网络也可以环程卷积网络，提出了RPN网络~~。本篇文章介绍下一个迭代版本R-FCN将后面的全连接层分类层也换成卷积网络！ 2位置敏感性的理解写着一部分看了好多文章都没看懂，查了好多资料总结到如下，之后有新的理解会加上，如果朋友们有好的文章解释可以推荐给我。首先，对于图片分类问题，我们用比较深的卷积网络比较好，当网络比较深的时候，分类问题对于位置的敏感性就较差，因为一张图片通过一个深的网络到了后面再到全连接层，位置的变动对于后面特征的影响比较小了，所以它的识别还是会准确，这就是位置不敏感，平移不变性。然而对于定位问题，我们需要得到它的定位，网络过于深的化就会造成我们定位不准确，平移不变！所以构造一个浅的网络可能对于分类来说比较好。 再来看个Faster R-CNN + ResNet-101结构的例子。这个是参考文献3给出的，如果在Faster R-CNN中没有ROI层，直接对整个feature map进行分类和位置的回归，由于ResNet的结构较深，平移可变性较差，检测出来的坐标会极度不准确。具体网络可以自己查一下！ 如果在ResNet中间（conv4与conv5间）加个ROI层结果就不一样了，此时到了第四卷积层才给框选区域，坐标还是有的，此时分类器离得比较近了，分类器对于位置的敏感程度比较低了！也就是平移可变性！ROI层提取出的proposal中，有的对应前景label，有的对应背景label，proposal位置的偏移就有可能造成分类器（前景和背景分类）的不同。偏移后原来的前景很有可能变成了背景，原来的背景很有可能变成了前景，换句话说分类loss对proposal的位置是敏感的，这种情况ROI层给深层网络带来了平移可变性。 如果把ROI加到ResNet的最后一层（图1 conv5后）结果又是怎样呢？conv5的第一个卷积stride是2，造成conv5输出的feature map更小，这时proposal的一个小偏移在conv5输出上很有可能都感知不到，即proposal对应的label没有改变，此时识别出来的位置准确度会很差。 作者在考虑前人的实验后，想着把ROI向后靠一下是不是就能把全连接层也共享了！实则会导致位置的准确度降低的！所以考虑这些因素，他提出了位置敏感卷积网络。 3位置敏感卷积层网络下图为R-FCN网络（这里说明一下，下面应该是还有一个回归！），其中红色部分是修改后的分类器，我们可以看出前面卷积之后得到featuremap之后有一部分向上去做RPN网络了，这个和FasterR-CNN一样。另一路去做分类了这里你会看到它写的是conv卷积，对没错，我们用一个卷积得到一个具有深度为K*K*(C+1)的特征。C+1代表类别，C个类别加一个背景。K是啥意思？这个KK我们表示它属于某个类别上左、上中、上右……的可能性。下图中每个颜色块代表一个方位，一共3\\3=9个方位，每个方位其实是包含C+1个的。再通俗点黄色层其实不是一层有C+1个层，分别代表它属于每个类别左上角的概率！然后它不是有很多ROI，它用ROI套住一个区域，对于第一类，他需要从这9个大层中对应这个类的小层得到一个平均结果，然后把这9个数组合投票看他是不是这个类。看下面这个图，每个大层有C+1个，然后对ROI抽取后得到饼图，有C+1个类所以是C+1层，再求一个得分得到C+1的向量。那么这时需要解决两个问题：First 对于一个ROI如何抽成一个点Second 投票怎么投？好，接下来说这个问题。First 对于一个ROI如何抽成一个点这里它直接采取平均值的方法，所有像素值的平均值来代表一个值。 Second 投票怎么投？对于投票，他也是计算的这K*K个值的平均值。哈哈！ 4位置敏感回归前面分类有了，那么回归位置呢？思路是一样的！前面分类是卷积为K*K*(C+1),回归的话就是四个参数啊，左上角右上角坐标！，那就是K*K*4,其他的就和上面一样了。即在ResNet的共享卷积层的最后一层上面连接一个与position-sensitive score map并行的score maps，该score maps用来进行regression操作，我们将其命名为regression score map，而该regression score map的维度应当是 4KK ，然后经过Position-sensitive RoI pooling操作后，每一个RoI就能得到4个值作为该RoI的x,y,w,h的偏移量了，其思路和分类完全相同。 5损失的定义然后就是怎么去定义这个损失，只有定义好损失，网络才能按照作者的设想去跑啊！，看下面这个图（图都是从小象课堂的PPT上截取的，感谢！）。先主要说一下分类损失，得到上面的投票，C+1的向量对这个值softmax归一化，其实就是都去指数幂次，然后把正确的做分子，这样看正确类别分类的得分是否够大，不大的话，呵呵损失就大，那么网络训练就回去矫正，那么为了让损失小，就得让这个类别的得分高，那么就得让前面的位置敏感卷积层的得分高~这样从理论上应该是能实现的。 6可视化结果论文中给出了其可视化的结果，如下图所示，可以看出，当这个ROI选对了，那么对于人这个类别，它得出的9个方位图在中间，我们可以看出左上角其实不太属于人，看中间那个图高亮的区域少；中间部位其实肯定属于人了，我们看出第五个格子几乎全部高亮，以此类推可以分析其他格子，最后计算每个格子的平均得到右图，可以看出其值比较高，属于人类。图2哪个区域选歪了之后就不一样了得分低一点，也很好分析！ 7实验效果作者首先做了一个实验去测试一下不同的位置敏感池化单元尺寸对结果的影响，可以看出1*1的尺寸，也就是对于一个ROI不分块，失败！3*3也就是分成左上左中左下九份效果还可以，7*7的效果最好。之后又做了对比试验，与之前的网络做了对比。可以看出效果得到了现主的提升，在保证效果的同时，时间更短！ 8参考文献[1]小象学堂课程PPT[2]R-FCN详解https://blog.csdn.net/WZZ18191171661/article/details/79481135[3]解答关于R-FCN的所有疑惑（原创）https://www.jianshu.com/p/409fd61db9db","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[]},{"title":"目标检测CNN经典网络（三）Fast R-CNN ，Faster R-CNN","slug":"目标检测CNN经典网络（三）Fast R-CNN ，Faster R-CNN","date":"2019-05-08T04:41:15.000Z","updated":"2019-11-22T14:45:26.166Z","comments":true,"path":"2019/05/08/目标检测CNN经典网络（三）Fast R-CNN ，Faster R-CNN/","link":"","permalink":"http://yoursite.com/2019/05/08/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89Fast%20R-CNN%20%EF%BC%8CFaster%20R-CNN/","excerpt":"继R-CNN与SPP-net之后又出现了Fast R-CNN与Faster R-CNN,比之前的网络有了更快的速度，其准确率也没有降低。并且把端到端的概念引入到目标检测中。","text":"继R-CNN与SPP-net之后又出现了Fast R-CNN与Faster R-CNN,比之前的网络有了更快的速度，其准确率也没有降低。并且把端到端的概念引入到目标检测中。 Fast R-CNNFastR-CNN借鉴SPP-net思想加入ROI感兴趣池化，并将两个任务回归和分类同时进行，进行多任务训练。 ROI池化ROI感兴趣池化可以看作SPPpool的简化，下图可以看出，ROI只对感兴趣的区域做一次池化即可。考虑到感兴趣区域（RoI）尺寸不一，但是输入图中后面FC层的大小是一个统一的固定值，因为ROI池化层的作用类似于SPP-net中的SPP层，即将不同尺寸的RoI feature map池化成一个固定大小的feature map。具体操作：假设经过RoI池化后的固定大小为是一个超参数，因为输入的RoI feature map大小不一样，假设为，需要对这个feature map进行池化来减小尺寸，那么可以计算出池化窗口的尺寸为：h/H,w/W，即用这个计算出的窗口对RoI feature map做max pooling，Pooling对每一个feature map通道都是独立的。 多任务损失函数Fast R-CNN将两个任务并行运行到一起，其损失定义如下所示，考虑损失为分类损失和回归损失。 最终论文给出其实现的性能可以看下表，其将训练时间与测试时间大大缩短，尤其是单图的测试时间大大缩短，这功劳归结于一方面采用了先卷积后 Faster R-CNNFast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。（想较少测试的时间那就把任务交给深度学习，学习出一套参数可以以后一直用。）给这么一个方法起了个名字叫做Region Proposal Network(RPN)。 网络总图先看下面这张图，它是最终的测试网络。通过上面这张图可以看出Faster R-CNN由四个部分组成：1）卷积层(conv layers)，用于提取图片的特征，输入为整张图片，输出为提取出的特征称为feature maps2）RPN网络(Region Proposal Network)，用于推荐候选区域，这个网络是用来代替之前的search selective的。输入为图片为featrue maps，输出为多个候选区域，这里的细节会在后面详细介绍。3）RoI pooling，和Fast R-CNN一样，将不同大小的输入转换为固定长度的输出，输入输出和Fast R-CNN中RoI pooling一样。4）分类和回归，这一层的输出是最终目的，输出候选区域所属的类，和候选区域在图像中的精确位置。 那么其实你可以看出来关键的部分在于候选区域的改动，那么我们主要说下候选区域的方法。 RPN网络首先作者自己设想原图上或者说featuremap上每个位置可能会产生k中可能的区域，下图所示，作者给出的方案具体来说就是每个位置可能会出现三种面积可能，三种长宽表示样式。三种面积分别是128×128，256×256,512×512，每种面积又分成3种长宽比，分别是2:1,1:2,1:1 。呵呵一算是九种！所以它的k是9！然后RPN网络是这么干的，先将一幅图片经过CNN网络得到第五个卷积层，然后将第五个卷积层用3*3卷积核卷积得到一个256通道的特征图，之后再分为两路，一路用1*1卷积降维到原尺寸下2*k个通道（这是干啥，这2*k个分别对应是/否为对的候选区域，总共有k个框，所以是2*k个），另一路用1*1的卷积降维到原尺寸下4*k个通道（这个就是左上角坐标，右下角坐标，k种）ok！注意：这里尺寸没有变哎，因为每个位置都要输出这么个得分情况哎，我们最后确定比较好的中心位置哎还有它的尺寸。 那么如果RPN一套神经网络，FastR-CNN一套网络好像也不行哦，训练费事哦。那么RPN网络和FastR-CNN是如何组合的是如何权值共享的呢？ Faster R-CNN训练总结一下Faster R-CNN的训练过程如下所示：（1）先再imagenet上预训练一个CNN模型，得到一个初始的RPN网络（2）再另外训练一个imagenet模型CNN2，然后把在步骤1得到的Regionproposals用来训练FastR-CNN模型。（3）有了一个比较好的FastR-CNN模型也就是步骤2的模型，我们把这个CNN部分固定再去矫正RPN网络，此时CNN部分就用CNN2（把CNN1直接丢掉了！）然后去调参RPN后面那一部分，调好了这时的RPN模型基本可以了。（4）完事候选区域又变了，回去在调FastR-CNN模型，此时CNN部分不动了，这两个网络已经共享了！我们去调后面的全连接啥的。ok调好了就大功告成。可以看出训练的时候费事，但是选练好了后都是神经网络，测试时间就大大减少。 Faster R-CNN的主要贡献是设计了提取候选区域的网络RPN，代替了费时的选择性搜索，使得检测速度大幅提高。 参考文献[1]第三十一节，目标检测算法之 Faster R-CNN算法详解https://www.cnblogs.com/zyly/p/9247863.html[2]Faster R-CNN文章详细解读https://blog.csdn.net/liuxiaoheng1992/article/details/81843363[3]【目标检测】Fast R-CNN论文详解（Fast R-CNN）https://www.jianshu.com/p/fbbb21e1e390","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[]},{"title":"目标检测CNN经典网络（二）SPPnet","slug":"目标检测CNN经典网络（二）SPPnet","date":"2019-05-07T04:41:15.000Z","updated":"2019-11-22T14:45:30.220Z","comments":true,"path":"2019/05/07/目标检测CNN经典网络（二）SPPnet/","link":"","permalink":"http://yoursite.com/2019/05/07/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89SPPnet/","excerpt":"在上文介绍了R-CNN网络，可以看出其存在很多问题：卷积网络的重复计算造成计算量大耗时多，之后出现了SPPnet提出先卷积再进行框选目标区域的方法。","text":"在上文介绍了R-CNN网络，可以看出其存在很多问题：卷积网络的重复计算造成计算量大耗时多，之后出现了SPPnet提出先卷积再进行框选目标区域的方法。 总体思路首先看下面这幅图，R-CNN中我们在卷积之前需要先选定预选框，然后可能大小尺寸不一，需要对其进行crop或者warp一下同意尺寸再进行卷积网络；SPPnet提出我们可以先对原图进行卷积神经网络，再利用预选框去找出对应的第五层卷积网络所对应的特征图（因为我们发现其实再训练卷积网络时，原图和每层的卷积图实际位置关系还是不变的）全连接层我们单独放在后面，在进入全连接层之前我们做了一个金字塔池化SSP（后面讲！）到统一尺寸。 空间金字塔+最大池化在图像处理中有个图像金字塔的思想可以采样图像到任意大小，这里借鉴这个思想，我们把卷积网络的输出得到的13*13*256特征提取三个不同尺寸，分别是1*1（也就是原图），2*2（原图分为四份），4*4（原图分为16份），然后对每一份选区最大的一个值代表该区域也就是最大池化（一份包含很多像素点啊）。最后得到16+4+1=21 *256 的特征！这样不管上一层输出的尺寸多大，我们都能做这个变换！ 如果像上图那样将reponse map分成4x4 2x2 1x1三张子图，做max pooling后，出来的特征就是固定长度的(16+4+1)x256那么多的维度了。如果原图的输入不是224x224，出来的特征依然是(16+4+1)x256 更加通用的认识可以认为：输入尺寸在[180,224]之间，假设最后一个卷积层的输出大小为a×a，若给定金字塔层有n×n 个bins，进行滑动窗池化，窗口尺寸为win=⌈a/n⌉，步长为str=⌊a/n⌋，使用一个网络完成一个完整epoch的训练，之后切换到另外一个网络。只是在训练的时候用到多尺寸，测试时直接将SPPNet应用于任意尺寸的图像。这样我们得到了统一尺寸的特征在进行后面的全连接层训练即可。 空间金字塔在定位实验中的应用在定位实验中，需要我们将原图上的ROI映射到featuremap中，具体可以参考我的另一篇文章探讨卷积的感受视野以及sPPnet中ROI映射到featuremap我详细介绍了ROI映射的方法。对此，我们得到映射后的图片就可以在featuremap中找到对应的区域，然后再进行空间金字塔最大池化得到特征。金字塔用了{6x6 3x3 2x2 1x1}，共50个bin，分类器也是用了SVM。最后得到了很大的提升，主要在于时间！ 总结R-CNN提取特征比较耗时，需要对每个warp的区域进行学习，而SPPNet只对图像进行一次卷积，之后使用SPPNet在特征图上提取特征。结合EdgeBoxes提取的proposal，系统处理一幅图像需要0.5s。 参考文献[1]探讨卷积的感受视野以及sPPnet中ROI映射到featuremaphttps://blog.csdn.net/CLOUD_J/article/details/89917950[2]【目标检测】SPPNet算法详解https://blog.csdn.net/bryant_meng/article/details/78615353","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[]},{"title":"探讨卷积的感受视野以及sPPnet中ROI映射到featuremap","slug":"探讨卷积的感受视野以及sPPnet中ROI映射到featuremap","date":"2019-05-07T02:41:15.000Z","updated":"2019-11-22T14:46:16.527Z","comments":true,"path":"2019/05/07/探讨卷积的感受视野以及sPPnet中ROI映射到featuremap/","link":"","permalink":"http://yoursite.com/2019/05/07/%E6%8E%A2%E8%AE%A8%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%84%9F%E5%8F%97%E8%A7%86%E9%87%8E%E4%BB%A5%E5%8F%8AsPPnet%E4%B8%ADROI%E6%98%A0%E5%B0%84%E5%88%B0featuremap/","excerpt":"最近在学习目标检测sPPnet时看到ROI映射到featuremap中的方法，文中对此叙述较少，所以就此问题差了一些资料，在知乎上发现一片理解较好的文章(文末给出了参考文献)，于此做一总结。","text":"最近在学习目标检测sPPnet时看到ROI映射到featuremap中的方法，文中对此叙述较少，所以就此问题差了一些资料，在知乎上发现一片理解较好的文章(文末给出了参考文献)，于此做一总结。 先谈一下感受视野，在学习卷积的时候，我们比较熟悉的是上一层图映射到下一层之后的尺寸，很少谈及感受野，这其实是一个重要的概念。 卷积中的感受视野首先，我们知道上一层图的尺寸，去推测下一层的尺寸，如下公式即可，这个大家应该很熟悉output field size = ( input field size - kernel size + 2*padding ) / stride + 1(output field size 是卷积层的输出，input field size 是卷积层的输入，stride步长，padding填充像素，kernelsize卷积核尺寸)随后，我们想如果知道后一层的尺寸，是不是就可以知道前一层的尺寸答案必然是： input field size = （output field size - 1）* stride - 2*padding + kernel size那么此时如果我们把最后一层的尺寸定为1，向前推是不是可以知道最后一层的一个小单位格子对应到原图的尺寸，也就是感受野了。 卷积神经网络CNN中，某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field。感受野的大小是由kernel size，stride，padding , outputsize 一起决定的。 我们可以看下面这张图更好的理解下虽然原图很大，但我们考虑原图中的11*11的尺寸向后映射到map2变为77的尺寸，再向后映射到11.7=(11-5)/1+1；1=(7-7)/1+1；是不是~~之后我们反过来证明一下最后一层向map2映射7 = (1-1)*1+7map2向map1映射11 = (7-1)*1+5 了解了感受野，我们再看一下坐标变换 坐标变换在感受野上面做坐标变换，实则是对中心位置做变换，如下图map3的一个点（位置为p3）它向上映射到map2的范围内中心位置是多少呢?我们可以用下面这个公式来理解对于 Convolution/Pooling layer: $p_i = s_i \\cdot p_{i+1} +( (k_i -1)/2 - padding)$（P代表位置，k表示卷积核大小，s代表步长）对于Neuronlayer(ReLU/Sigmoid/..) : $p_i = p_{i+1}$ 在此画了一个图方便理解，下面一层的绿色点反映一个1*1的点中对应到上一层绿色点，他其实是在最左侧的基本格子向右平移p(p就是下面那层的坐标)个stride（本层的步长）所以这部分坐标就是$s_i \\cdot p_{i+1}$然后还差一个基本格子的半个坐标就用$( (k_i -1)/2 - padding)$来决定哈哈！完美！ 我们最后再看一下下面这个图给出的计算！理解一下它的计算！ 上面是感受野的映射公式，到了ROI对于上面的公式，何凯明在SPP-net中采用做了一个简化。其实就是巧妙的化简一下公式$pi = s_i \\cdot p_{i+1} +( (k_i -1)/2 - padding)$令每一层的padding都为$padding = \\lfloor k_i /2 \\rfloor\\Rightarrow$$pi = s_i \\cdot p_{i+1} +( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor)$当$k_i$ 为奇数 $( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor) = 0$ 所以 $p_i = s_i \\cdot p_{i+1}$当k_i 为偶数 $( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor) = -0.5$ 所以 $p_i = s_i \\cdot p_{i+1} -0.5$而 $p_i$ 是坐标值，不可能取小数 所以基本上可以认为$p_i = s_i \\cdot p_{i+1}$。公式得到了化简：感受野中心点的坐标$p_i$只跟前一层 $p_{i+1}$ 有关。 ROI映射变换ROI映射的时候是将左上角和右小角映射到featuremap然后确定最后的区域。考虑原图上的点不可能每一个点在featuremap中都有对应，所以我们就找最近的点。例如图中左上角的点（x,y）映射到 feature map上的(x’,y’) ： 使得 (x’,y’) 在原始图上感受野（上图绿色框）的中心点 与（x,y）尽可能接近。在上面每层都填充padding/2 得到的简化公式 ： $p_i = s_i \\cdot p_{i+1}$我们考虑从featuremap传到原图得到$p_0 = S \\cdot p_{i+1}$ 其中 $(S = \\prod_{0}^{i} s_i)$(所有步长量乘积~~)呵呵简单了不少。把式子反过来就变为：$x’ = \\lfloor x/S \\rfloor ,;y’ = \\lfloor y/S \\rfloor$。这样就实现了ROI坐标变换，sPPnet中改了一下：$x’ = \\lfloor x/S \\rfloor +1 ,;y’ = \\lfloor y/S \\rfloor +1$,加了一个1，有什莫用呢？笔者认为相当于把得到的坐标向里面缩一下保证准确性。 参考文献[1]原始图片中的ROI如何映射到到feature map?https://zhuanlan.zhihu.com/p/24780433[2]目标检测CNN经典网络（二）SPPnethttps://blog.csdn.net/CLOUD_J/article/details/89893140","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"SPPnet","slug":"SPPnet","permalink":"http://yoursite.com/tags/SPPnet/"},{"name":"感受野","slug":"感受野","permalink":"http://yoursite.com/tags/%E6%84%9F%E5%8F%97%E9%87%8E/"}]},{"title":"目标检测CNN经典网络（一）R-CNN","slug":"目标检测CNN经典网络（一）R-CNN","date":"2019-05-06T02:41:15.000Z","updated":"2019-11-22T14:45:26.141Z","comments":true,"path":"2019/05/06/目标检测CNN经典网络（一）R-CNN/","link":"","permalink":"http://yoursite.com/2019/05/06/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89R-CNN/","excerpt":"在早期计算机视觉研究时，人们很好的将CNN应用在目标分类的问题中，但是一直不能很好的解决目标检测的问题，目标检测不仅要分析出其是什么东西，而且还要回归出其位置。这是其在CNN训练的痛点。为此出现了R-CNN网络。","text":"在早期计算机视觉研究时，人们很好的将CNN应用在目标分类的问题中，但是一直不能很好的解决目标检测的问题，目标检测不仅要分析出其是什么东西，而且还要回归出其位置。这是其在CNN训练的痛点。为此出现了R-CNN网络。 R-CNN思路R-CNN简单理解就是先训练一个CNN分类器，然后再对每幅图片挑选2000个预选区域，让这个预选区域经过R-CNN前向传播得到特征，特征先保留到硬盘中，此时与我们的标签进行对比，重叠率高的为正样本，重叠率低的为负样本，随后单独进行SVM训练。R-CNN的整体思路如下：（1）给定一张输入图片，从图片中提取 2000 个类别独立的候选区域。（selective search）（2）对于每个区域利用CNN网络抽取一个固定长度的特征向量。（这个CNN是提前训练好的一个网络）（3）再对每个区域所对应的特征向量利用 SVM 进行目标分类。 候选区域寻找selective search关于候选区域的选择有多种方法，最简单的方法为滑窗法，就是以一定间距不断横纵遍历得到所有的窗口，但是所有区域都遍历，而且有时候物体不一样大？这就太浪费时间了。论文章所采用的方法是选择搜索法。算法具体如下图所示step0：生成区域集R。 step1：计算区域集R里每个相邻区域的相似度S={s1,s2,…}//循环以下步骤step2：找出相似度最高的两个区域，将其合并为新集，添加进Rstep3：从S中移除所有与step2中有关的子集step4：计算新集与所有子集的相似度step5：S若为空则跳出循环，否则跳至step2 相似度计算那么关键点在于相似度怎么判断？图像之间的特征有很多，颜色？纹理特征？尺寸等？对于相似度的判断，选择性搜索考虑了颜色、纹理、尺寸和空间交叠这4个参数。1、颜色特征将色彩空间转为HSV，每个通道下以bins=25计算直方图，这样每个区域的颜色直方图有25*3=75个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：上式如何理解？简单说一下，我们归一化后每个通道的数据后，一维特征向量的和为1.0，比较两个区域的特征向量每一位，把每一位的最小值加起来。如果全都一样那么结果肯定是1啊！如果不一样，每一位都取最小值，那就肯定比1小啊！哈哈就是这么理解。 2、纹理相似度（texture similarity） 论文这里的纹理采用SIFT-Like特征，采用方差为1的高斯微分在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8310=240（使用RGB色彩空间）。计算公式如下所示： 3、尺度相似度尺度相似度其实是实现优先合并小的区域，也就是减少大区域合并小区域。4、交叠相似度（shape compatibility measure）上述完成后，要看一下两个区域是否爱挨着，如果距离十万八千里，那就没有合并的必要。如何计算？先找一个外接矩形把它框起来，如果两个矩形相距较远，则外接矩形就很大，所以我们设计这样的指标，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形BBij）减去两个矩形的尺寸，得到相对差距。其计算方式：候选区域排序相似度计算后还有一个问题，如何对候选区域排序呢，通过上述的步骤我们能够得到很多很多的区域，但是显然不是每个区域作为目标的可能性都是相同的，因此我们需要衡量这个可能性，这样就可以根据我们的需要筛选区域建议个数啦。 这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给他们乘以一个随机数，毕竟3分看运气嘛，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。这样我就得到了所有区域的目标分数，也就可以根据自己的需要选择需要多少个区域了。 CNN网络训练上述过程我们得到了大量候选区域（每个候选区域都提供了矩形框的位置情况），怎么训练呢？第一步、数据标注：我们对于每张图片标注一下boundbox四维向量和所属类别编号。第二步、预训练：论文在大型辅助训练集ILSVRC2012分类数据集（分类数据集没有约束框数据，每幅图片对应一个类别标签）上预训练了CNN。预训练采用了Caffe的CNN库。第三步、微调CNN：特定领域的参数调优第一步训练了一个分类器，但是我们要做的是目标检测，而且我们换了另一套数据集，所以为了让我们的CNN适应新的任务（即检测任务）和新的领域（变形后的候选窗口）。我们只使用变形后的候选区域对CNN参数进行SGD训练。1、网络修改：ImageNet是用了专用的1000-way分类层，这里我们换成21-way分类层，（其中20是VOC的类别数，1代表背景），卷积部分都没有改变。2、数据集选择性搜索算法会提取一些候选区域，如何区分正例，负例？这里我们考虑如果其和真实标注的框的IoU&gt;= 0.5就认为是正例，否则就是负例。OK这样前面选择性搜索的框到了后面就利用IOU自动标注，然后去softmax训练这样就训练了一个比较适合我们的候选框的分类器。完美！但是好像不好耶，重合度0.5就是正例，那不是好多一半框选目标就被认为合格了，一张图片会有好多的框合格对！那么你会想我们去把阈值调高点，不久解决了！这里作者考虑如果IOU太高那样正例就太少了，负例太多，譬如对于一张图片的2000候选框，1张合格，1999张不合格；那么训练会过拟合的！所以考虑这个，这里作者降低了IOU阈值（后面也有说）那么作者咋解决的？第四步第四步、多目标分类器这里作者就又搞了20个SVM分类器，上面第三步训练好了CNN了，我们把所有的图片走一遍CNN把最后的特征留下，不训练，把它保留当成特征，去训练SVM。这里把IOU&lt;0.3的认为是负样本。 另外样本数据很大，每幅图片都对应着很多的候选区域特征emmm这里论文采用了hard negative mining method训练时有时模型会把重叠率低的也当成正样本，这就属于顽固负样本，需要继续投进去训练。 【难负例挖掘算法，用途就是正负例数量不均衡，而负例分散代表性又不够的问题，hard negative就是每次把那些顽固的棘手的错误,再送回去继续练,练到你的成绩不再提升为止.这一个过程就叫做’hard negative mining‘】 这里你会想到为啥不直接CNN时直接搞一个分类器，还用啥SVM？对于这个问题我也有所疑惑，然后查了一下，大概这么解释还可以理清。 因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm。一旦CNN f7层特征被提取出来，那么我们将为每个物体累训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到20004096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包好了4096个W)，就可以得到结果了。 Bounding-box回归上面完成了分类，但其实回归效果不是很好，作者又在第五层卷积层采用了Bounding-box回归对候选区域做了修正，提高了边框的回归效果。 下图，绿色的框表示Ground Truth, 红色的框为Selective Search提取的Region Proposal。那么很明显框定位不准(IoU&lt;0.5)， 如果我们能对红色的框进行微调， 使得经过微调后的窗口跟Ground Truth 更接近， 这样岂不是定位会更准确。 确实，Bounding-box regression 就是用来微调这个窗口的。那么怎么变换呢?我们可以简单的看出可以通过平移、伸缩两种操作来实现。如下图所示，我们需要目标框的四维向量和合格的预选区域的向量，然后要训练一个线形回归模型，下图左边那个公式，这里计算损失公式如右下角所示。训练好后就可以用来预测了。具体哪个BoundingBox模型可以参考参考文献来学习。 测试环节非极大值抑制(Non-Maximum Suppression, NMS)训练好了怎么测试？测试阶段，在测试图像上使用selective search抽取2000个推荐区域（实验中，我们使用了选择性搜索的快速模式）。然后变形每一个推荐区域，再通过CNN前向传播计算出特征。然后我们使用对每个类别训练出的SVM给整个特征向量中的每个类别单独打分。然后给出一张图像中所有的打分区域，然后使用NMS（每个类别是独立进行的），拒绝掉一些和高分区域的IOU大于阈值的候选框。 非极大值抑制给出一张图片和上面许多物体检测的候选框（即每个框可能都代表某种物体），但是这些框很可能有互相重叠的部分，我们要做的就是只保留最优的框。假设有N个框，每个框被分类器计算得到的分数为Si, 1&lt;=i&lt;=N。0、建造一个存放待处理候选框的集合H，初始化为包含全部N个框；建造一个存放最优框的集合M，初始化为空集。1、将所有集合 H 中的框进行排序，选出分数最高的框 m，从集合 H 移到集合 M；2、遍历集合 H 中的框，分别与框 m 计算交并比（Interection-over-union，IoU），如果高于某个阈值（一般为0~0.5），则认为此框与 m 重叠，将此框从集合 H 中去除。3、回到第1步进行迭代，直到集合 H 为空。集合 M 中的框为我们所需。 用下面这张图可以看懂测试的流程。 总结本篇论文虽然已经算是早起的目标检测的论文，但依然被新手所研究，算是目标检测的开篇之作把！他给出了目标检测的一个最简单的思路，但是费时费力。 参考文献[1]【深度学习】R-CNN 论文解读及个人理解https://blog.csdn.net/briblue/article/details/82012575[2]R-CNN论文详解（论文翻译）https://blog.csdn.net/v1_vivian/article/details/78599229[3]选择性搜索（selective search）https://blog.csdn.net/guoyunfei20/article/details/78723646[4]第三十三节，目标检测之选择性搜索-Selective Searchhttps://www.cnblogs.com/zyly/p/9259392.html[5]边框回归：BoundingBox-Regression(BBR)https://blog.csdn.net/v1_vivian/article/details/80292569","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[]},{"title":"候选区域寻找选择性寻找selective search","slug":"候选区域寻找选择性寻找selective search","date":"2019-05-04T02:41:15.000Z","updated":"2019-11-22T14:47:22.348Z","comments":true,"path":"2019/05/04/候选区域寻找选择性寻找selective search/","link":"","permalink":"http://yoursite.com/2019/05/04/%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%E5%AF%BB%E6%89%BE%E9%80%89%E6%8B%A9%E6%80%A7%E5%AF%BB%E6%89%BEselective%20search/","excerpt":"目标检测时需要筛选候选区域，常见的方法有滑窗法，选择搜索法。本文简单介绍一下候选区域寻找的方法。","text":"目标检测时需要筛选候选区域，常见的方法有滑窗法，选择搜索法。本文简单介绍一下候选区域寻找的方法。 候选区域寻找selective search关于候选区域的选择有多种方法，最简单的方法为滑窗法，就是以一定间距不断横纵遍历得到所有的窗口，但是所有区域都遍历，而且有时候物体不一样大？这就太浪费时间了。论文章所采用的方法是选择搜索法。算法具体如下图所示step0：生成区域集R。 step1：计算区域集R里每个相邻区域的相似度S={s1,s2,…}//循环以下步骤step2：找出相似度最高的两个区域，将其合并为新集，添加进Rstep3：从S中移除所有与step2中有关的子集step4：计算新集与所有子集的相似度step5：S若为空则跳出循环，否则跳至step2 相似度计算那么关键点在于相似度怎么判断？图像之间的特征有很多，颜色？纹理特征？尺寸等？对于相似度的判断，选择性搜索考虑了颜色、纹理、尺寸和空间交叠这4个参数。1、颜色特征将色彩空间转为HSV，每个通道下以bins=25计算直方图，这样每个区域的颜色直方图有25*3=75个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：上式如何理解？简单说一下，我们归一化后每个通道的数据后，一维特征向量的和为1.0，比较两个区域的特征向量每一位，把每一位的最小值加起来。如果全都一样那么结果肯定是1啊！如果不一样，每一位都取最小值，那就肯定比1小啊！哈哈就是这么理解。 2、纹理相似度（texture similarity） 论文这里的纹理采用SIFT-Like特征，采用方差为1的高斯微分在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8310=240（使用RGB色彩空间）。计算公式如下所示： 3、尺度相似度尺度相似度其实是实现优先合并小的区域，也就是减少大区域合并小区域。4、交叠相似度（shape compatibility measure）上述完成后，要看一下两个区域是否爱挨着，如果距离十万八千里，那就没有合并的必要。如何计算？先找一个外接矩形把它框起来，如果两个矩形相距较远，则外接矩形就很大，所以我们设计这样的指标，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形BBij）减去两个矩形的尺寸，得到相对差距。其计算方式：候选区域排序相似度计算后还有一个问题，如何对候选区域排序呢，通过上述的步骤我们能够得到很多很多的区域，但是显然不是每个区域作为目标的可能性都是相同的，因此我们需要衡量这个可能性，这样就可以根据我们的需要筛选区域建议个数啦。 这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给他们乘以一个随机数，毕竟3分看运气嘛，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。这样我就得到了所有区域的目标分数，也就可以根据自己的需要选择需要多少个区域了。 参考文献[1]选择性搜索（selective search）https://blog.csdn.net/guoyunfei20/article/details/78723646[2]第三十三节，目标检测之选择性搜索-Selective Searchhttps://www.cnblogs.com/zyly/p/9259392.html","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"目标检测","slug":"计算机视觉/目标检测","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"选择性搜索","slug":"选择性搜索","permalink":"http://yoursite.com/tags/%E9%80%89%E6%8B%A9%E6%80%A7%E6%90%9C%E7%B4%A2/"}]},{"title":"图像分类CNN经典网络（四）CNN总结","slug":"图像分类CCNN经典网络（四）CNN总结","date":"2019-04-26T02:41:30.000Z","updated":"2019-11-22T14:45:07.630Z","comments":true,"path":"2019/04/26/图像分类CCNN经典网络（四）CNN总结/","link":"","permalink":"http://yoursite.com/2019/04/26/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E5%9B%9B%EF%BC%89CNN%E6%80%BB%E7%BB%93/","excerpt":"前面我们分析了CNN的经典网络模型，在此对CNN设计的一些技巧做一些总结。","text":"前面我们分析了CNN的经典网络模型，在此对CNN设计的一些技巧做一些总结。 1、避免遇到瓶颈HxW逐渐变小C逐渐变大缓慢！2、计算量控制（卷积核控制）减小计算量我们可以改变的因素有输入特征数量（但是一般不好控制）输出的特征（与分类有关）卷积核（我们需要去改变的）A考虑小卷积表达大卷积等方法。B考虑11降维再33C并联3、感受视野要足够大这样才能捕捉更大尺寸的内容，为此考虑可以用小卷积和来表达大卷积核。这样既能降低参数，提高视野，而且还用了更多的非线性激活。 4、通道拆分并联 关键词：卷积的视野计算量、参数量网络的表现能力","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"图像分类","slug":"计算机视觉/图像分类","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[]},{"title":"图像分类CCNN经典网络（三）ResNet","slug":"图像分类CCNN经典网络（三）ResNet","date":"2019-04-25T08:37:41.000Z","updated":"2019-11-22T14:44:59.154Z","comments":true,"path":"2019/04/25/图像分类CCNN经典网络（三）ResNet/","link":"","permalink":"http://yoursite.com/2019/04/25/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89ResNet/","excerpt":"众所周知，网络的层数越低，网络的性能会越来越好。恰面我们看到了经典的四个网络架构，层数最多的也就22层。是不是可以造出更深的网络来呢？为此很多人继续去做实验，人们发现网络性能没有提高反而降低了，考虑其原因可能是梯度爆炸或者梯度消失等，为此有人提出了残差网络的思想。","text":"众所周知，网络的层数越低，网络的性能会越来越好。恰面我们看到了经典的四个网络架构，层数最多的也就22层。是不是可以造出更深的网络来呢？为此很多人继续去做实验，人们发现网络性能没有提高反而降低了，考虑其原因可能是梯度爆炸或者梯度消失等，为此有人提出了残差网络的思想。 在前面两篇文章总结了经典的CNN四个模型，其网络层次如下所示：众所周知，网络的层数越低，网络的性能会越来越好。恰面我们看到了经典的四个网络架构，层数最多的也就22层。是不是可以造出更深的网络来呢？为此很多人继续去做实验，人们发现网络性能没有提高反而降低了，考虑其原因可能是梯度爆炸或者梯度消失等，为此有人提出了残差网络的思想。 ResNet残差网络那么我们作这样一个假设：假设现有一个比较浅的网络（Shallow Net）已达到了饱和的准确率，这时在它后面再加上几个恒等映射层（Identity mapping，也即y=x，输出等于输入），这样就增加了网络的深度，并且起码误差不会增加，也即更深的网络不应该带来训练集上误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想，便是著名深度残差网络ResNet的灵感来源。 ResNet的主要思想在于残差的思想，如下图所示：我总结了两方面理解这个思想，emmm不知道对不对，欢迎指正。（1）我们先看这个结构，如果说浅层网络已经训练的比较好了，深层网络要做的就是保持这个效果，保持效果即使要用一个恒等映射，也就是我们的残差网络H=F+X，这个等式如果我们让F=0，则H=X就能恒等映射浅层网络的东西了，所以残差网络训练到后期就会训练F=0这个过程。 （2）那么对于一个数据集，我们不太清楚训练多少层比较好，但是我们知道一个浅层的网络达到比较好的笑过后，再往后加网络层数不知道效果如何，但是如果加一个恒等映射的话应该是保持浅层的效果的！get到。（3）好，那么我们就构造一个深层次的带残差的网络当训练到一定时间后，浅层的网络会优化的很好，深层的网络会逐渐地调节使F趋于0.（4）残差对于收敛速度的影响X是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。 好了，之后又对残差网络做了优化，两层的残差网络参数量太大，下图右侧为三层的残差网络，其先降维，然后再卷积，再升维。左图参数量3x3x256x64x2（？？对不对）右图1x1x256x64+3x3x64x64+1x1x64x256=69632的参数量。参数量大大减小！ ResNeXtResNeXt是再残差网络上做了一定的修改，它是借鉴了Lenet的思想，将卷积网络按照通道拆分开，如下图所示。左图为ResNet，右图为RexNext。它考虑左边是直接串联完成了所有的步骤先降维得到64通道再用33卷积核卷积再升维度。右边是分为并联的几个小块，分别降维到4个通道，然后卷积33，这时参量才334432个相比于3364*64小多了emmm，案后再升维度，再组合到一起。也是一个很好的思想。 如下展示了对ResNet与ResNeXt的对比，其在没有提高参数量的同时提高了预测的正确率。 参考文献[1]大话深度残差网络（DRN）ResNet网络原https://blog.csdn.net/rogerchen1983/article/details/79353972[2]浅析深度ResNet有效的原理https://blog.csdn.net/u014296502/article/details/80438616","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"图像分类","slug":"计算机视觉/图像分类","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[]},{"title":"图像分类CNN经典网络（二）GoogLeNet","slug":"图像分类CCNN经典网络（二）GoogLeNet","date":"2019-04-25T02:41:30.000Z","updated":"2019-11-22T14:45:03.811Z","comments":true,"path":"2019/04/25/图像分类CCNN经典网络（二）GoogLeNet/","link":"","permalink":"http://yoursite.com/2019/04/25/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89GoogLeNet/","excerpt":"在GoogLeNet之前，人们一直想着去创造更深层的网络去改善CNN模型，但是这样会导致参数量不断增大也不一定有一个很好的效果；随着网络层数增加可能会导致梯度消失现象。","text":"在GoogLeNet之前，人们一直想着去创造更深层的网络去改善CNN模型，但是这样会导致参数量不断增大也不一定有一个很好的效果；随着网络层数增加可能会导致梯度消失现象。 GoogLeNet在GoogLeNet之前，人们一直想着去创造更深层的网络去改善CNN模型，但是这样会导致参数量不断增大也不一定有一个很好的效果；随着网络层数增加可能会导致梯度消失现象。 *小贴士：梯度消失是什么样？在我们卷积网络中层数不断增加时可能会导致梯度消失的情况，比如如果我们选区sigmoid激活函数，其导数的最大值为0.25，如果层数增大，可能会导致随着后向传播，浅层的网络梯度不断减小，对于参数的更改也就变动十分小，甚至没有。当然产生梯度消失的原因不只这一个，这个是由于激活函数的硬伤导致的，可以换用relu、leakrelu、elu等激活函数。也可以加入Batchnorm。* V1版本1、Inception结构GoogLeNet提出了一种Inception结构来解决这个问题。下图为google团队提出的早起的inception基础结构，该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积池化后的尺寸是一样的，然后把其通道叠在一起即可），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。(不需要我们去实验选择用什么卷积核，让网络自己去优化抉择哪一个卷积层好。) 然而这个Inception原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，这也就形成了Inception v1的网络结构，如下图所示： *小贴士：1x1的卷积层作用是什么？1x1的卷积层有降低维度的作用，比如上一层网络的输出为100x100x100，如果我们直接用5x5x50去卷积，参数量为100x5x5x50=125000。如果我们先用1x1x50的卷积降维得到100x100x50再进行5x5x50卷积，参数量为100x1x1x50+50x5x5x50=67500，减少了两倍。所以这样加入一个1x1的卷积层能够在降低维度的情况下又能使用5x5的卷积层。* 思考：为什么VGG通常作为最基础的网络架构，Inception这么好不能作为基础？答：主要在于Inception训练好它会更加的适用于某一个场景数据，当换了场景可能需要修改许多参数；而VGG做出的网络更具有普遍性。 2、GAP全局平均池化第二个问题是如何处理全连接层权重过大，这里googLeNet提出了全局平均池化的方法，就是把得到的最后特征，每一层求平均后用一个点代替原来一层的特征，这样就减少了大量的全连接层参数。比如特征为771024此时要得到14096的特征，那就得用7710244096参数，如果先全局平均池化的话即先得到111024的特征，再用111024*4096即可。 3、辅助分类器当网络层数过长时会存在 梯度消失的现象，googLeNet引入了两个辅助分类器，它感觉可能在某一层会出现梯度消失时，在这里加了一个分类器，最后的分类结果会有一个权重考虑，最终分类器全重大0.5，两个辅助分类器03。这样也类似于一个模型融合提高了系统的泛化能力。也给系统增加了额外的梯度。最终V1版本的googLeNet模型结构图GoogLeNet网络结构明细表解析如下：0、输入原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。1、第一层（卷积层）使用7x7的卷积核（滑动步长2，padding为3），64通道，输出为112x112x64，卷积后进行ReLU操作经过3x3的max pooling（步长为2），输出为((112 - 3+1)/2)+1=56，即56x56x64，再进行ReLU操作2、第二层（卷积层）使用3x3的卷积核（滑动步长为1，padding为1），192通道，输出为56x56x192，卷积后进行ReLU操作经过3x3的max pooling（步长为2），输出为((56 - 3+1)/2)+1=28，即28x28x192，再进行ReLU操作3a、第三层（Inception 3a层）分为四个分支，采用不同尺度的卷积核来进行处理（1）64个1x1的卷积核，然后RuLU，输出28x28x64（2）96个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x96，然后进行ReLU计算，再进行128个3x3的卷积（padding为1），输出28x28x128（3）16个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x16，进行ReLU计算后，再进行32个5x5的卷积（padding为2），输出28x28x32（4）pool层，使用3x3的核（padding为1），输出28x28x192，然后进行32个1x1的卷积，输出28x28x32。将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x2563b、第三层（Inception 3b层）（1）128个1x1的卷积核，然后RuLU，输出28x28x128（2）128个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x128，进行ReLU，再进行192个3x3的卷积（padding为1），输出28x28x192（3）32个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x32，进行ReLU计算后，再进行96个5x5的卷积（padding为2），输出28x28x96（4）pool层，使用3x3的核（padding为1），输出28x28x256，然后进行64个1x1的卷积，输出28x28x64。将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480 第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与3a、3b类似，在此就不再重复。 V2版本V2版本做了一些小的改动BatchNormalization 以往的神经网络，我们会对数据的输入做归一化处理，降低数据之间的差异性，来提高网络的收敛速度。BatchNormalization提出普通的网络的隐含层的输出也可以加入标准化，这样也能提高网络的收敛速度，我参考了文献中的几篇博客对BN说明如下： 在我们训练时，隐含层可能训练到后期，输出的数据可能偏向于某一侧，导致sigmoid激活函数梯度减小甚至消失，导致其收敛速度大大减小，这时我们用一个均值方差归一化做处理是数据集中到正态分布中间来，便能提高收敛的速度。但是这样是不是有了一个问题？？所有数据都归一到中间，中间部位其实是趋于一个线形函数的，那么就导致非线性的操作消失了，模型的表达能力变差。这是我们不想要的！BN的一个精髓就出来了，他引入一个偏移，让数据便宜一下到非线性区域。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。 整理后BN算法大致思想如下图所示，求取上一级输出的平均值与方差，对所有数据做归一化处理，但是这样搞完后，数据都偏移了，所以这里还要加入一个尺度变换纠正偏移。 V3版本V3版本相比之前增加了卷积拆解的思想。具体怎么说呢？ 大卷积拆解为小卷积在实际我们设计网络时，用大的卷积核会有比较好的视野，每一个点所综合考虑的因素多一些，但是这样的话，参数量就会提升。为此V2提出了一种将nn的卷积核拆解为1n后再n1卷积的方法，实验证明这个方法在卷积核复杂的时候比较好用（12&lt;n&lt;18）（下图为参考文献中的一个图，之前VGG中提出过用两个33代替5*5的网络。） 参考文献[1]大话CNN经典模型：GoogLeNet（从Inception v1到v4的演进）https://my.oschina.net/u/876354/blog/1637819[2]神经网络中的梯度消失https://www.cnblogs.com/mengnan/p/9480804.html[3]【深度学习】深入理解Batch Normalization批标准https://www.cnblogs.com/guoyaohua/p/8724433.html[4]【深度学习】批归一化（Batch Normalization）https://www.cnblogs.com/skyfsm/p/8453498.html[5]","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"图像分类","slug":"计算机视觉/图像分类","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[]},{"title":"图像分类CNN经典网络（一）AlexNet与VGG","slug":"图像分类CCNN经典网络（一）AlexNet与VGG","date":"2019-04-25T02:41:15.000Z","updated":"2019-11-22T14:52:14.946Z","comments":true,"path":"2019/04/25/图像分类CCNN经典网络（一）AlexNet与VGG/","link":"","permalink":"http://yoursite.com/2019/04/25/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89AlexNet%E4%B8%8EVGG/","excerpt":"本篇文章总结了CNN领域用于目标检测的两个网络AlexNet与VGGNet。","text":"本篇文章总结了CNN领域用于目标检测的两个网络AlexNet与VGGNet。 在目标检测领域，CNN有了很好的应用，下图展示了2012年之后的一些经典的网络架构。本篇文章总结了CNN领域用于目标检测的两个网络AlexNet与VGGNet。 AlexNetAlexNet是打开卷积神经网络大门的第一个作品，它重新将卷积神经网络带入计算机视觉的科研中。他给出了卷积神经网络在目标检测中的基本思想。注意要特点又1、ReLU、双GPU运算：提高训练速度。（应用于所有卷积层和全连接层）2、重叠pool池化层：提高精度，不容易产生过度拟合。（应用在第一层，第二层，第五层后面）3、局部响应归一化层(LRN)：提高精度。（应用在第一层和第二层后面）4、Dropout：减少过度拟合。（应用在前两个全连接层） VGGNetVGGNet由牛津大学计算机视觉组合和Google DeepMind公司研究员一起研发的深度卷积神经网络。它探索了卷积神经网络的深度和其性能之间的关系，通过反复的堆叠33的小型卷积核和22的最大池化层，成功的构建了16~19层深的卷积神经网络。VGGNet获得了ILSVRC 2014年比赛的亚军和定位项目的冠军，在top5上的错误率为7.5%。 VGGNet的网络结构如下图所示。VGGNet包含很多级别的网络，深度从11层到19层不等，比较常用的是VGGNet-16和VGGNet-19。VGGNet把网络分成了5段，每段都把多个3*3的卷积网络串联在一起，每段卷积后面接一个最大池化层，最后面是3个全连接层和一个softmax层。 我们以VGG16做一个分析下图给出了VGG16的一个内存占用和参数量的分析。 VGG特点总结如下： 1、至今，VGGNet仍然是现如今搭建基本的CNN网络的基本网络，很多网络构建初期都是在VGG的基础上搭建。2、网络在创新上提出了用小的卷积核代替大卷积的思想。用两个3*3的卷积核去替代5*5的卷积核，用三个3*3的卷积核代替7*7的卷积核。如何理解？可以看下图原来一个55的区域要用一个核卷积，如今先用个33的核来卷积得到第二层的图，图纸考虑是步长为1的情况，此时最底层移动三次可以把5*5的区域遍历到，然后得到第二层3*3的图。然后再用33的核对第二层卷积得到顶层一个。这样就达到用用两个3\\3的卷积核去替代5*5的卷积核。很巧妙！！使用尺寸小的卷积的好处？（1）更少的参数量；（2）更多的非线性变换，使得CNN对特征的学习能力更强表达能力增强；（3）隐式的正则化效果（收敛速度要快）。 参考文献[1]AlexNet详细解读https://blog.csdn.net/qq_24695385/article/details/80368618[2]VGGNet介绍https://blog.csdn.net/u013181595/article/details/80974210[3]卷积神经网络的网络结构——VGGNethttps://www.imooc.com/article/34700[4]大话CNN经典模型：https://my.oschina.net/u/876354/blog/1634322","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"图像分类","slug":"计算机视觉/图像分类","permalink":"http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"AlexNet","slug":"AlexNet","permalink":"http://yoursite.com/tags/AlexNet/"},{"name":"VGG","slug":"VGG","permalink":"http://yoursite.com/tags/VGG/"}]},{"title":"Python基础学习（三）面向对象","slug":"Python基础学习（三）面向对象","date":"2019-04-16T09:33:52.000Z","updated":"2019-04-16T09:34:34.397Z","comments":true,"path":"2019/04/16/Python基础学习（三）面向对象/","link":"","permalink":"http://yoursite.com/2019/04/16/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","excerpt":"本篇文章介绍python的类，面向对象的思想。","text":"本篇文章介绍python的类，面向对象的思想。 类和其它编程语言相比，Python 在尽可能不增加新的语法和语义的情况下加入了类机制。 Python中的类提供了面向对象编程的所有基本功能：类的继承机制允许多个基类，派生类可以覆盖基类中的任何方法，方法中可以调用基类中的同名方法。 对象可以包含任意数量和类型的数据。 python类相比其他的不同有以下几个特点：1、self参数作为对象参数，class里面每个函数都会有一个参数self（后面详细介绍）2、init初始化函数，类似于构造函数。3、多了一些内置的方法，我们都可以给他重构。 1234567891011121314#!/usr/bin/python3 class MyClass: \"\"\"一个简单的类实例\"\"\" i = 12345 def f(self): return 'hello world' # 实例化类x = MyClass() # 访问类的属性和方法print(\"MyClass 类的属性 i 为：\", x.i)print(\"MyClass 类的方法 f 输出为：\", x.f()) 类的属性方法属性就类似于C++中类的成员变量，有私有有共有。python也是一样。私有属性私有属性变量前面有两个下划线。例如__count代表一个私有成员。公有属性没有前置下划线。例如count、a。 123class People: __age = 12#私有成员 name = \"Jack\"#公有成员 方法类似于C++中的函数。python的方法需要传入一个self代表某个类，调用的时候A.function1(b,c)其中A就是哪个self无需在括号里传值。如下例子定义了两个函数一个时getname一个时setname，getname设置参数为self，调用时直接A.get_name()就能获取到它的年龄，函数体里面改变成员变量用self.name即可。 123456789class People: __age = 12#私有成员 name = \"Jack\"#公有成员 def get_name(self): return self.name def set_name(self,str): self.name = strPeople AA.name() 私有方法private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。self.private_methods。 专用方法在类中有一系列专用方法，一般的类都会有的方法，也是类特有的方法，一般用到比较多的时构造函数init，在c++中也有，做初始化处理。再有就是打印函数用来打印结果repr。这些函数我们都可以重新构造。 __init__ : 构造函数，在生成对象时调用__del__ : 析构函数，释放对象时使用__repr__ : 打印，转换__setitem__ : 按照索引赋值__getitem__: 按照索引获取值__len__: 获得长度__cmp__: 比较运算__call__: 函数调用__add__: 加运算__sub__: 减运算__mul__: 乘运算__truediv__: 除运算__mod__: 求余运算__pow__: 乘方 继承继承也是类中的一个重要概念，在开发程序的过程中，如果我们定义了一个类A，然后又想新建立另外一个类B，但是类B的大部分内容与类A的相同时我们不可能从头开始写一个类B，这就用到了类的继承的概念。通过继承的方式新建类B，让B继承A，B会‘遗传’A的所有属性(数据属性和函数属性)，实现代码重用python也有继承这个规则 123456class DerivedClassName(BaseClassName1): &lt;statement-1&gt; . . . &lt;statement-N&gt; 在子类中可以重写方法，则调用字类时运行字类的新方法。 封装封装与扩展性封装在于明确区分内外，使得类实现者可以修改封装内的东西而不影响外部调用者的代码；而外部使用用者只知道一个接口(函数)，只要接口（函数）名、参数不变，使用者的代码永远无需改变。这就提供一个良好的合作基础——或者说，只要接口这个基础约定不变，则代码改变不足为虑。 多态多态指的是一类事物有多种形态 动物有多种形态：人，狗，猪 （附录）面向对象常用术语抽象/实现 抽象指对现实世界问题和实体的本质表现,行为和特征建模,建立一个相关的子集,可以用于 绘程序结构,从而实现这种模型。抽象不仅包括这种模型的数据属性,还定义了这些数据的接口。 对某种抽象的实现就是对此数据及与之相关接口的现实化(realization)。现实化这个过程对于客户 程序应当是透明而且无关的。 封装/接口 封装描述了对数据/信息进行隐藏的观念,它对数据属性提供接口和访问函数。通过任何客户端直接对数据的访问,无视接口,与封装性都是背道而驰的,除非程序员允许这些操作。作为实现的 一部分,客户端根本就不需要知道在封装之后,数据属性是如何组织的。在Python中,所有的类属性都是公开的,但名字可能被“混淆”了,以阻止未经授权的访问,但仅此而已,再没有其他预防措施了。这就需要在设计时,对数据提供相应的接口,以免客户程序通过不规范的操作来存取封装的数据属性。 注意：封装绝不是等于“把不想让别人看到、以后可能修改的东西用private隐藏起来” 真正的封装是，经过深入的思考，做出良好的抽象，给出“完整且最小”的接口，并使得内部细节可以对外透明 （注意：对外透明的意思是，外部调用者可以顺利的得到自己想要的任何功能，完全意识不到内部细节的存在） 合成 合成扩充了对类的 述,使得多个不同的类合成为一个大的类,来解决现实问题。合成 述了 一个异常复杂的系统,比如一个类由其它类组成,更小的组件也可能是其它的类,数据属性及行为, 所有这些合在一起,彼此是“有一个”的关系。 派生/继承/继承结构 派生描述了子类衍生出新的特性,新类保留已存类类型中所有需要的数据和行为,但允许修改或者其它的自定义操作,都不会修改原类的定义。继承描述了子类属性从祖先类继承这样一种方式继承结构表示多“代”派生,可以述成一个“族谱”,连续的子类,与祖先类都有关系。 泛化/特化 基于继承泛化表示所有子类与其父类及祖先类有一样的特点。特化描述所有子类的自定义,也就是,什么属性让它与其祖先类不同。 多态与多态性 多态指的是同一种事物的多种状态：水这种事物有多种不同的状态：冰，水蒸气 多态性的概念指出了对象如何通过他们共同的属性和动作来操作及访问,而不需考虑他们具体的类。 冰，水蒸气，都继承于水，它们都有一个同名的方法就是变成云，但是冰.变云(),与水蒸气.变云()是截然不同的过程，虽然调用的方法都一样 自省/反射 自省也称作反射，这个性质展示了某对象是如何在运行期取得自身信息的。如果传一个对象给你,你可以查出它有什么能力,这是一项强大的特性。如果Python不支持某种形式的自省功能,dir和type内建函数,将很难正常工作。还有那些特殊属性,像dict,name及doc 参考文献[1]菜鸟教程面向对象http://www.runoob.com/python3/python3-class.html[2]面向对象介绍https://www.cnblogs.com/wangmo/p/7751199.html","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[]},{"title":"Python基础学习（二）错误与异常","slug":"Python基础学习（二）错误与异常","date":"2019-04-16T03:27:58.000Z","updated":"2019-04-16T03:28:54.089Z","comments":true,"path":"2019/04/16/Python基础学习（二）错误与异常/","link":"","permalink":"http://yoursite.com/2019/04/16/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E9%94%99%E8%AF%AF%E4%B8%8E%E5%BC%82%E5%B8%B8/","excerpt":"本节介绍以下python中的错误与异常，以及学会如何去自定义一个异常，这样有助于我们之后的调试。","text":"本节介绍以下python中的错误与异常，以及学会如何去自定义一个异常，这样有助于我们之后的调试。 错误与异常在编写程序时可能会出现一些语法错误，这时系统会提示语法错误，比如类型错误，无效参数等，如下给出一个例子： 12345&gt;&gt;&gt;while True print('Hello world') File \"&lt;stdin&gt;\", line 1, in ? while True print('Hello world') ^SyntaxError: invalid syntax 在运行时有时会发生一些异常，比如分母作为0了，以下给出一个抛出异常的例子： 1234&gt;&gt;&gt;10 * (1/0)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in ?ZeroDivisionError: division by zero 常见的错误及异常关键词如下： KeyboardInterrupt 用户中断执行(通常是输入^C)Exception 常规错误的基类StopIteration 迭代器没有更多的值GeneratorExit 生成器(generator)发生异常来通知退出StandardError 所有的内建标准异常的基类FloatingPointError 浮点计算错误OverflowError 数值运算超出最大限制ZeroDivisionError 除(或取模)零 (所有数据类型)SyntaxError Python 语法错误TypeError 对类型无效的操作ValueError 传入无效的参数 异常处理有时，对于异常我们可以加以限制，比如需要用户输入一个数，但是用户输入错误，抛出一个异常，系统可能中断，但这里我们可以采用一个异常处理，当抛出异常时重新让用户输入信息并给与提示。 这里就用到try语句，他可以尝试执行一句话如果抛出异常，则根据其异常的类别不同执行不同的语句。如下所示 12345678try:&lt;语句&gt; #运行代码except &lt;异常名字1&gt;：&lt;语句&gt; #如果在try代码引发了'异常名字1'异常，则执行这条语句except &lt;异常名字2&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'异常名字2'异常，并且获得附加的数据else:&lt;语句&gt; #如果没有异常发生 另外这里也可以不带异常名字，直接发生异常后执行一段代码。 下面给出一个让用户输入一个数字的代码，这里有个强制转换，如果转换失败应该会报出valueerror则打印输入错误。 123456while True: try: x = int(input(\"Please enter a number: \")) break except ValueError: print(\"Oops! That was no valid number. Try again \") 抛出异常上述为检测一个异常的做法，在调试过程中，我们也可以自主的抛出异常，比如执行到某段程序，我们去测一下是否如我们所想像，如果不是我们想要的就让他抛出一个异常。 1234&gt;&gt;&gt;raise NameError('HiThere')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in ?NameError: HiThere raise 唯一的一个参数指定了要被抛出的异常。它必须是一个异常的实例或者是异常的类（也就是 Exception 的子类）。 如果你只想知道这是否抛出了一个异常，并不想去处理它，那么一个简单的 raise 语句就可以再次把它抛出。 12345678910&gt;&gt;&gt;try: raise NameError('HiThere') except NameError: print('An exception flew by!') raise An exception flew by!Traceback (most recent call last): File \"&lt;stdin&gt;\", line 2, in ?NameError: HiThere 自定义异常类我们也可以定义一个自己的异常类，其作为Exception的类的继承，可以设置参数。 12345class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value) 参考文献[1]菜鸟教程python3错误与异常http://www.runoob.com/python3/python3-errors-execptions.html","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[]},{"title":"Python基础学习（一）基本语法","slug":"Python基础学习（一）基本语法","date":"2019-04-12T03:09:12.000Z","updated":"2019-04-12T03:13:28.047Z","comments":true,"path":"2019/04/12/Python基础学习（一）基本语法/","link":"","permalink":"http://yoursite.com/2019/04/12/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/","excerpt":"概述学习一门语言都要有一个helloworld，都要了解最基础的输入输出、注释、变量类型等基本的语法，对于其他的语法都是paper tiger。话不多说，我们一起来瞅瞅~~","text":"概述学习一门语言都要有一个helloworld，都要了解最基础的输入输出、注释、变量类型等基本的语法，对于其他的语法都是paper tiger。话不多说，我们一起来瞅瞅~~ Python基础学习（一）基本语法学习一门语言都要有一个helloworld，都要了解最基础的输入输出、注释、变量类型等基本的语法，对于其他的语法都是paper tiger。话不多说，我们一起来瞅瞅~~ 特点1、编程规范性python语言与之前的C语言、CPP语言最直观的表现在于其没有分号，它以缩进，对齐来实现其结构，所以python对编程的规范要求比较高，如下我们展示python的一段编程。for，if等以冒号代替花括号； 123for it in range(9): print(it)print(\"end\") 2、没有主函数如上所示，写下的代码都可以运行，无需掉主函数。 3、语句太长，换行语句太长可以用 \\ 反斜杠换行，如下： 1234567for (x,y,w,h) in faces: img3 = cv.rectangle(img1,(x,y),(x+w,y+w),(255,0,0),2) img4 = cv.putText(img3,'prepare %s picture'\\ #这里换行 %str(count+1),(20,80),\\#这里换行 cv.FONT_HERSHEY_PLAIN,2,(255,0,0)) f = cv.resize(img2[y:y+h,x:x+w],(200,200)) cv.imshow('13',img4) 打印函数print函数是最基本的应用，python3中有括号，python2中没有括号。 1print(&quot;hello,world!&quot;) 注释python的注释用#表示，如果对多行注释的话，可以用’’’在两个’’’之间的被注释掉。另外也可以改为”””三个双引号。 123456789'''本案例用来说明注释的作用。'''for it in range(9):#循环九次 print(it)#打印itprint(\"end\")\"\"\"程序结束\"\"\" 数字类型python中数字有四种类型：整数、布尔型、浮点数和复数。int (整数), 如 1bool (布尔), 如 True。float (浮点数), 如 1.23、3E-2complex (复数), 如 1 + 2j、 1.1 + 2.2j 输入输出读取键盘输入读取键盘输入可以用input语句 12str1 = input(\"请输入字符串\")print(str1) 字符串格式化输出字符串格式化输出类似于C语言的格式化输出。用print打印一个字符串，可以在后面跟上%，后面存入前面%对应的变量。 12345a = 3.1415926b = 1c = 1.21print('常量 PI 的值近似为：%5.3f。' %a)print('b = %d,c = %.1f' %(b,c)) 注意：其中格式化输出符号如下表所示，另外.1代表保留一位小数。5.3代表至少占5个字符3位小数。新版格式化输出新版格式化输出类似，采用{}表示一个字段，用.format()的参数替代.举例： 123456&gt;&gt;&gt; print('&#123;&#125;网址： \"&#123;&#125;!\"'.format('菜鸟教程', 'www.runoob.com'))菜鸟教程网址： \"www.runoob.com!\"&gt;&gt;&gt; print('&#123;0&#125; 和 &#123;1&#125;'.format('Google', 'Runoob'))Google 和 Runoob&gt;&gt;&gt; print('&#123;1&#125; 和 &#123;0&#125;'.format('Google', 'Runoob'))Runoob 和 Google 也可以在{}中加入关键字 123&gt;&gt;&gt; print('站点列表 &#123;0&#125;, &#123;1&#125;, 和 &#123;other&#125;。'.format('Google',\\&gt;'Runoob',other='Taobao'))站点列表 Google, Runoob, 和 Taobao。 可选项 ‘:’ 和格式标识符可以跟着字段名。 这就允许对值进行更好的格式化。 下面的例子将 Pi 保留到小数点后三位： 12345a = 3.1415926b = 1c = 1.21print('常量 PI 的值近似为：&#123;0:.3f&#125;'.format(a))print('b = %d,c = %.1f' %(b,c)) 如果你有一个很长的格式化字符串, 而你不想将它们分开, 那么在格式化时通过变量名而非位置会是很好的事情。 最简单的就是传入一个字典, 然后使用方括号 ‘[]’ 来访问键值 : 123&gt;&gt;&gt; table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;&gt;&gt;&gt; print('Runoob: &#123;0[Runoob]:d&#125;; Google: &#123;0[Google]:d&#125;; Taobao: &#123;0[Taobao]:d&#125;'.format(table))Runoob: 2; Google: 1; Taobao: 3 导入库python可以导入自带库或者自己写的库，用到import语句。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数或者某个类，格式为： from somemodule import somefunction从某个模块中导入多个函数或者多个类，格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import * 12345import cv2 as cv #导入CV2库并给他换了个名字import numpy as np import os,sysfrom CS231N_Data import load_CIFAR10#从我的CS231N_Data文件中导入load_CIFAR10类from neural_net import TwoLayerNet","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[]},{"title":"玉渊潭樱花之旅","slug":"玉渊潭樱花之旅","date":"2019-03-18T03:25:09.000Z","updated":"2019-11-22T14:48:53.764Z","comments":true,"path":"2019/03/18/玉渊潭樱花之旅/","link":"","permalink":"http://yoursite.com/2019/03/18/%E7%8E%89%E6%B8%8A%E6%BD%AD%E6%A8%B1%E8%8A%B1%E4%B9%8B%E6%97%85/","excerpt":"三月初春在北京待的第二个春天，想起去年我还在考研复试，如今已经在北京待了一年了，今年春天来得比较早，玉渊潭的樱花又开了，老司机当然要去看看了，赏花是每年不能错过的节目。","text":"三月初春在北京待的第二个春天，想起去年我还在考研复试，如今已经在北京待了一年了，今年春天来得比较早，玉渊潭的樱花又开了，老司机当然要去看看了，赏花是每年不能错过的节目。 主演：🧑&amp;👩地点：玉渊潭公园🌸时间：2019.3.17 线路地铁：清早乘坐2号线转1号线可以到木樨地站，下车步行800米可以到达东门，或者做地铁4号线转9号线可以到达白堆子站，下车步行600米可以到达北门。 公交：每个地方不一样哈，自己找路吧 皂片发来国人旅游就是牌照，嘿嘿，咱也不例外，我们俩说是来观景，拍照倒是消耗了许久，对这一片竹子林照了好久~~ 桃花&amp;樱花篇首先，来几张花照，今天还没有到花的旺盛季节，人比较少，还比较好拍~还有些含苞未放的花骨朵。在樱花下少不了一张合照来记录~~ 相机摆拍篇一直看网上有相机摆拍的照片，今天让我们也来玩耍一下，拍的不好，嘿嘿第一次，以后会拍好的。让我聚焦你认真的时刻嘿嘿，我也假装拍照一下~~ 蓝天篇北京的蓝天是真的美，也许是在一段时间雾霾后出现蓝天映衬的，反正是好看。 结束工作之余多出去走走~~爱上生活","categories":[{"name":"生活随笔","slug":"生活随笔","permalink":"http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"玉渊潭","slug":"玉渊潭","permalink":"http://yoursite.com/tags/%E7%8E%89%E6%B8%8A%E6%BD%AD/"}]},{"title":"Tensorflow学习总结（二）用Tensorflow实现一个全卷积神经网络","slug":"Tensorflow学习总结（二）用Tensorflow实现一个全卷积神经网络 - 副本","date":"2019-03-17T08:22:21.000Z","updated":"2019-03-18T15:40:40.149Z","comments":true,"path":"2019/03/17/Tensorflow学习总结（二）用Tensorflow实现一个全卷积神经网络 - 副本/","link":"","permalink":"http://yoursite.com/2019/03/17/Tensorflow%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89%E7%94%A8Tensorflow%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20-%20%E5%89%AF%E6%9C%AC/","excerpt":"学习完CS231N一系列课程后，我们都书写了自己的深层卷积网络，代码量不多。这里我们使用google的tensorflow框架去写一个卷积神经网络，并去把卷积网路中的优化一步一步复现出来。","text":"学习完CS231N一系列课程后，我们都书写了自己的深层卷积网络，代码量不多。这里我们使用google的tensorflow框架去写一个卷积神经网络，并去把卷积网路中的优化一步一步复现出来。 搭建一个卷积神经网络，需要包括以下几个部分： 数据输入 前向传播 损失计算+正则化优化 网络优化 测试 一、数据输入数据输入，定义变量我们定义了两个输入数据的占位，方便之后运行时传入即可。然后定义了隐含层的参数输出层的参数。 1234567891011#输入数据 X = tf.placeholder(tf.float32,[None,INPUT_NODE],name = \"x-input\") y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name = \"y-output\") #隐藏层参数 weight1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev = 0.1)) b1 = tf.Variable(tf.constant(0.1,shape = [LAYER1_NODE])) #输出层参数 weight2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev = 0.1)) b2 = tf.Variable(tf.constant(0.1,shape = [OUTPUT_NODE])) #定义step global_step = tf.Variable(0,trainable=False) 二、前向传播前向传播比价简单1、矩阵相乘matmul 1a = tf.matmul(X,weight1) 2、激活函数激活函数有很多种类RELU激活函数tf.nn.relu() 1layer1 = tf.nn.relu(tf.matmul(X,weight1) + b1) 3、一个两层的神经网络代码如下： 1234#前向传播 #计算得分 layer1 = tf.nn.relu(tf.matmul(X,weight1) + b1) y = tf.matmul(layer1,weight2) + b2 三、损失计算+正则化优化经过前向传播我们得到了传播后的分数，之后我们需要计算它的损失，一部分是得分的损失，一部分是权重的正则损失。 softmax回归+交叉熵损失在计算损失之前，我们先对结果进行分类回归，一般对于多分类问题，我们采用softmax分类器进行回归，softmax可以将分数转变为概率的形式，就可以反应每个数据归属于每个类别的概率。然后我们采用交叉熵（cross_entropy）来计算损失。tensorflow中提供了tf.nn.sparse_softmax_cross_entropy_with_logits函数来计算损失，其输入有两个参数logits是回归结果，label是标签。logits填入待取log的概率化数据。格式为[N,C]，其中N代表样本数量，C代表最后的划分类别数。label填入标签数据，格式为[N,1]，其中N代表样本数量，每一行只有一个结果，取值0-C。这里我们用的数据集样本标签和logits格式是一样的，所以我们引用tf.argmax选取每一行的最大值所代表的编号，得到[N,1]类型结果。计算完后我们将所有损失加起来取平均。如下所示： 123#计算损失 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels=tf.argmax(y_,1)) cross_entropy_mean = tf.reduce_mean(cross_entropy) 正则化有时候我们的数据量少，参数多的时候可能会出现过拟合现象，我们一般采取正则化的方式来优化，过拟合的原因在于权重参数可能在调解过程中变得过于复杂去适应我们的训练数据，就好比我们有五个不等式方程，我们去改善它的5*5个系数，使他对于我们的数据都能正确，如果一次取样10个数据，我们可能获得等式的25个唯一参数，很好的去适应这10个数据，但换一组数据它就不是那么好，在控制中就是鲁棒性不是太好。所以我们引入一个刻画权重参数复杂程度的指标J(θ)，然后用一个系数乘以它，来调节它对整个损失的影响比例。这就是正则化一般我们采用L1、L2损失函数，在tensorflow中为： 1regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE)(weight) 把L2改为L1就代表L1正则化。可以看出后面有两个括号，第一个括号参数为正则化系数，第二个为权重。也可以改写为下面的代码，先声明一个具有某个正则化系数的函数regular，然后在计算W1与W2时可以直接用regular(W1)。 123#正则化损失regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE)regularization = regular(weight1) + regular(weight2) PS：两个正则化的对比：L1正则化会让参数变得稀疏（指会有更多参数变为0），而L2正则化则不会（因为参数的平方后会让小的参数变得更小，大的参数变得更大，同样起到了特征选取的功能，而不会让参数变为0）。其次是L1正则化计算不可导，而L2的正则化损失函数可导。 下面给出整体的损失计算代码 12345678#计算损失 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels=tf.argmax(y_,1)) cross_entropy_mean = tf.reduce_mean(cross_entropy) #正则化损失 regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE) regularization = regular(weight1) + regular(weight2) #总损失 loss = cross_entropy_mean + regularization 四、网络优化Optimizer计算完损失之后需要进行网络优化，网络优化也就是通过不停改变参数把损失值降低，一般我们多采用随机梯度下降法， Optimizer优化1、梯度下降法一般我们常用的优化算法为梯度下降法，也就是每次计算出梯度后对每一个参数进行线形调节X = X + learning_rate * Gradient 1tf.train.GradientDescentOptimizer(learning_rate)#梯度下降法类 这种优化算法存在一些问题，可能会出现有好几个波谷，但是梯度下降一直在一个波谷里面来回动荡。这会导致梯度下降法有时不能找到最优解，只能找到一个极值点。再者梯度下降法对所有数据进行运算，耗时太长。每一个优化方法类下都有一个minimize函数，对某个值进行最小化优化。 1train_ = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) 2、随机梯度下降法（SGD）考虑梯度下降法优化速度慢的问题，后来有人提出随机梯度下降法，我们每次训练可以不训练所有的数据，可以只取一个小的batch数据进行训练，这样每次训练的速度就会加快，然后我们每次取样的batch不一样，这样就保证稳定性了。代码可以和上面一样，考虑每次训练取样batch即可。 更多的优化算法在我另一篇博客中介绍。 在这之后我们在考虑一个学习率的问题，学习率在一开始可能需要大一点比价好，加快收敛速度，但是在后期，就需要慢下来，因为幅度大的话可能在两边来回晃，很难收敛。 学习率优化问题这里给出一个指数衰减法的方法。让学习率以指数方式慢慢衰减。这里我抓取一张常用的图，tensorflow提供两个衰减情况，一个是连续衰减，一个是梯度衰减，如下图所示。tensorflow给出以下函数expoential_decay，参数有四个，基础学习率，全局步数，衰减步数，衰减率。其公式为：learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY^(global_step/decay_step)含义很明白，当全局步数达到衰减步数时学习率变为基础学习率的基础上乘以衰减率，如果是连续衰减时，在这过程中则一直变化。 123#学习率更新 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,\\ decay_step,LEARNING_RATE_DECAY) 五、测试结果在进行一段时间训练后我们希望输出以测试集的正确性，利用tf.equal得到两组数据是否i相等，然后求结果的平均值，如下所示。tf.cast可以将其转换类型，因为equal返回的时bool型。故我们将其转换为float32再求平均数。 12correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1)) show_result = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) 六、运行首先开启一个会话，然后初始化所有变量，然后准备一个字典型数据，mnist数据集给了一个取样函数next_batch直接取样即可，然后运行_train把数据送进去。当运行100次时打印数据。 12345678910111213with tf.Session() as sess: #初始化数据 tf.global_variables_initializer().run() #验证集数据 validate_feed = &#123;X:mnist.validation.images,y_:mnist.validation.labels&#125; for it in range(TRAINING_STEPS): XS,YS = mnist.train.next_batch(BATCH_SIZE)#取样数据 sess.run(train_,feed_dict=&#123;X:XS,y_:YS&#125;) if it % 100 == 0: #打印数据 result = sess.run(show_result,feed_dict=validate_feed) print(\"After %d training step(s),validation accuracy is %g\"%(it,result)) 主函数我们编写如下代码： 12345678mnist = input_data.read_data_sets('./', one_hot=True)#mnist = input_data.read_data_sets(\"/\",one_hot = True)print(mnist.train.num_examples)print(mnist.train.images[0])train(mnist) 注意：这里我们是在本地存了一些数据，如有需要可以从我的github获取用Tensorflow搭建两层神经网络训练MNIST数据集 最后结果如下所示","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/categories/TensorFlow/"}],"tags":[]}]}