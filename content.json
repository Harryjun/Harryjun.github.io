{"pages":[{"title":"关于","text":"我是谁 昵称：意犹未尽学校：北京理工大学研究方向：计算机视觉专业：控制工程 我怎样？性格平常喜欢电脑面前敲敲敲 但也不是宅男 又喜欢组织大家一起粗去 比如之前 宿舍户外烧烤 鸟巢看好声音 去天津看爱情保卫战 （PS:我是观众！） 与人相处比较友善，喜欢互相交流，互相学习。 一开始腼腆，混熟了嘴就不停了。😀 爱好哈哈 no 爱好 平是就是瞎鼓捣 比如写写博客，弄弄公众号 真没有爱好吗？ 也不能说没爱好吧，喜欢创作 比如做饭？烘焙？自己装饰自己的家？ （原来我该学设计的！） 有时也喜欢搞点花样~~比如想着做个北理工的一天快影录制 （舍友没人陪我玩，就泡汤了） （再说说，我这人喜欢跟大家一起互动玩一下） （宿舍偶尔打打篮球，PS：我不会，纯属觉得身体不行了该锻炼了！！！） 履历 本人刚25出头，没啥履历,写一些本科和研究生做的项目。本科主要做嵌入式软件方向，研究生改做计算机视觉。嘿嘿，如果你觉得下面无聊了，可以戳这里跳过哦，哈哈。 2019.08-2019.11 360人工智能研究院 CV算法实习生 Prj1：全网审核之公众人物审核 需求：对全网视频、图片进行违规审核，包括涉黄、涉政、暴恐、敏感话题等内容的审核。 要点：根据需求不同上线了政治人物审核、公众人物审核两个版本服务。算法主体包括利用MTCNN网络进行人脸检测与对齐，再利用人脸特征提取网络（ArcFace+triplet）提取人脸特征与预先提取好的特征库进行比对，计算人脸相似度，给出结果。 Prj2：视频理解之关键帧提取与视频摘要生成 需求：对全网视频进行视频理解，主要包括视频场景分割、场景分类、视频摘要生成、视频封面制作等。 要点：目前做了一些视频理解方面的调研工作，以及一些视频摘要生成的baseline实验，包括用强化学习对视频摘要做代表性奖励与关键帧不相似度奖励的DR-DSN网络实验等。 2019.06至今 基于GAN的滴虫性阴道炎图像生成技术研究 需求：课题主要研究滴虫性阴道炎的疾病诊断问题，当前问题主要在于数据集的稀缺，我们需要利用现有医院提供的部分数据集去生成更加完备的数据集。 要点： 相比于其他数据集，医学数据集更为稀缺，所以首先需要用传统数据增强方法对数据进行增强； 在WGAN基础上将人脸识别的类间距应用到此增加不同形态滴虫生成图的类间距；（实验中） 在损失函数上增加输出图像间距与输入噪声之比的正则化项提高图像多样性。 2017.03-2017.06 BMS智能电池管理系统 本项目主要实现对电池组充电过程的性能监控及健康管理。主要涉及无线通信协议的搭建以及上位机界面（用C#编写）的设计。 环境：VisualStudio（C#上位机）；Keil（嵌入式编程C++&amp;C）。 要点：上位机通信协议设计； Github:https://github.com/Harryjun/BMS 2016.07-2016.07 第十一届NXP杯智能车竞赛华北赛区裁判志愿者 2016年全国大学生智能车竞赛在燕山大学举办，我有幸担任裁判组长一员参与。 2015.05-2016.05 基于太阳能发电的电动汽车铅酸蓄电池充放电及智能化能量管理 项目为国家级大学生创新计划项目，主要研究三方面：1）三段式充电与最大功率充电组合策略；2）太阳能的最大功率追踪算法实现3）上位机观测电池健康程度及充电策略调试。有效代码量：2100行。 环境：AltiumDesigner(绘制PCB)；Keil（嵌入式编程C++&amp;C）；VisualStudio（C#上位机）。 要点：光伏充电与传统三段式充电结合的算法；无线远程控制；主控电路板设计； GitHub:https://github.com/Harryjun/MPPTPrj 2014.07-2015.07 飞思卡尔智能车竞赛 本项目为本科创新实验室比赛项目，主要利用摄像头传感器采集图像实现智能车对线路的跟踪。其中主要涉及图像滤波、边缘线检测、障碍物检测、曲率估计、PID转速调节等。有效代码量：3100行。 环境：飞思卡尔编程软件（C++&amp;C） 要点：各类传感器（摄像头、电机、编码器、LCD）驱动程序编写；图像处理算法编写；电机PID调速程序编写； GitHub:https://github.com/Harryjun/Freescalecar 为啥写这个博客 一则，本人就爱玩这个，搞点新鲜玩意；二呢，也是总结下知识，记录点东西，也想给别人分享些东西；三点，记录点生活琐碎，等我50出头在回忆，哈哈； 为啥博客名叫这个？哈哈，以前我所有的名字叫做意犹未尽，后来做了cv就变为cloudcver。 cv上云，云上的cver。 ![](https://tuchuangblog-1257632417.cos.ap-chengdu.myqcloud.com/9904fc68ce07064b4e1df814b5333222.jpg) if 你想和我说点什么: then,可以留个言哈哈，也可以致信我的邮箱cloudcver@163.com","link":"/about/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"所有分类","text":"","link":"/categories/index.html"},{"title":"推荐博客&公众号","text":"","link":"/friends/index.html"},{"title":"Links","text":"申请友链须知 原则上只和技术、情感类博客互换友链，且具有原创高质量文章优先。 不与含有色情、暴力、政治敏感或其他法律法规禁止的网站互换。 申请链接前请先添加本博链接，通过本友链页面留言或邮件告知。 请提供网站名称、网站链接、网站描述（12字以内）、网站图标或头像。 友情链接 一见倾心 你要很酷，世界才不会残酷 水寒的博客 安卓、前端、物联网大佬 Jitwxs Java 后端开发大佬","link":"/links/index.html"},{"title":"WIKI&PLAN","text":"3月KR 1、能力：Docker、SQL、模型部署Tensort； 2、研究：密集商品检测、变换检测（分割）； 3、代码：分类网络demo整理； 2020/3/2-2020/3/8 本周目标： 1、目标检测方案以及arcface部署； 2、SQL学习 3、商品检测调研工作 4、分类代码整理","link":"/plan/index.html"},{"title":"我的资料","text":"请输入密码 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","link":"/privatefile/index.html"},{"title":"project","text":"Project政治人物审核DemoV2.1 对全网视频进行视频审核，对视频是否涉及政治人物、敏感人物进行审核。 展示版本：视频关键帧提取+ MTCNN人脸检测 + ArcFace人脸识别 + 274类任务 准确度实现开集测试99.6%，自建闭集测试98.7% 图片测试接口： 可以用socket对服务器进行TCP通信测试，IP：39.97.178.93。 端口91为人脸检测+识别接口（目前协议：向端口发送图片URL，会返回json文件）：[Client Code] 端口90为人脸识别端口（目前协议：向端口发送图片URL，会返回json文件）[Cilent Code] Json 格式12345678910111213141516171819{\"object\": [ {\"grade\": 0.5884853601455688, \"class\": \"xijinping\", \"pos\": { \"y\": 138, \"x\": 474, \"w\": 89, \"h\": 89 }}, {\"grade\": 0.5833901166915894, \"class\": \"hujintao\", \"pos\": { \"y\": 167, \"x\": 246, \"w\": 85, \"h\": 85}} ], \"img_url\": \"http://news.cnr.cn/dj/20160914/W020160914388600058399.jpg\"} 视频审核Demo：我们做了个简单的视频审核客户端展示，可以从百度云下载，简单示例如下： 未来：更多探索。 瓜果蔬菜分类DemoV1.2 应用于学校食堂、无人餐厅等场所，实现对餐盘的视频进行自动检测。 Demo:开源Resnet、MobileNet版本 目前实现4类蔬菜的分类（AI研习社比赛） 准确度： 测试API接口： 移动客户端（MobileNet）展示： 效果展示： 未来： 商品变换检测 应用于线下零售店的缺货检测，可以自动检测商品变换，商品是否缺货。 方案：分割思想来做（backbone：FCN） 准确度： 服务器云端加速下载V1.0 通过客户端TCP通信服务器端远程下载外网文件或者下载速度慢的文件。 开放端口号定时更新，需要端口号可联系我） 远程服务器长期监听。 客户端用PYQT制作。 更多可以参考说明 视频摘要生成V1.5 视频摘要主要应用于长视频、短视频的推荐中，利用深度学习挖掘视频里的用户感兴趣、笑点多、重要的关键帧形成简短的视频展示给大家。 目前完成测试demo版本 暂时不提供API及图片展示。","link":"/project/index.html"},{"title":"共享资料","text":"推荐系统论文推荐系统论文","link":"/publicfile/index.html"},{"title":"所有标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"CVPR2019 GAN论文分享","text":"更新于2019-9-292019-9-29增加DCGAN总结2019-9-27增加《Geometry-Consistent Generative Adversarial Networks for》2019-9-26增加《Sphere Generative Adversarial Network Based on Geometric Moment Matching》2019-9-25增加《Mixture Density Generative Adversarial Networks》2019-9-23创建文本 概要2019CVPR共有10篇论文上榜，其中包含6篇改进GAN网络的文章，包括四篇改良GAN的模型崩塌问题，有一篇改良文字转图像模型的，有一篇提出切片W距离；除了改进GAN的论文还有有4篇文章属于领域创新型文章，有应用在模型压缩、美妆生成、交互式图像生成、视频生成等领域。 《Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis》——-MSGAN这篇文章主要针对cGAN的模型崩塌问题提出了一种正则化的思想，在原目标函数基础上加入一个输出图像之间的距离与噪声输入之间距离的比值。源码：https://github.com/HelenMao/MSGAN IDEA先摆一张论文中的图，这是他idea的来源，讲的什么意思呢，左边为真实数据分布，应该在模型空间里会有五个模型，模型崩塌时会像第二个图一样，他映射好只有中间那两个波峰模型，而利用它的ModeSeeking会达到第三张图状态，怎么达到呢，看右侧虚线框，里面解释在Z到ImageSpaceI的映射中，可以看出如果，这个比值小的话，会使的只在中间晃荡，当比值大时就活到外边映射到更多的模型去。所以作者给出一个正则化思想就是最大化一个输出图像之间的距离与噪声输入之间距离的比值。 Work加入的正则项如下，他主要想最大化这个比值，以实现输入端一个差值很小的输入却能引起输出端较大的模型差异。（笔者感觉这是一个方向，大家都在想让输出模型尽可能差别大一些，或者增大距离，或者映射到一个区别都更大的空间，这样模型才能输出多样性）一般人们引入一个正则项后要加入一个权重系数来平衡，所以最终的目标函数如下： Experiments作者对于图像约束GAN（pairedimage and unpaired images）、文字约束GAN分别做了大量实验来验证它的正则化项好！这里举一例paired image cGAN作者用pix2pix做的baseline，数据集用的facades（建筑风格）与maps（地图映射）最终结果如下，这大家都说自己好的。 《Mixture Density Generative Adversarial Networks》——-MDGAN这篇文章也是用来解决GAN的模型崩塌问题，传统的GAN中D只做了个打分操作，这里作者想到让D输出K维向量，然后在这个K维度空间去聚类不同的高斯模型。这不同的高斯模型就代表真实图像可能存在的模型（感觉做这方面改进的想法都差不多，我们能改进吗？）源码：https://github.com/eghbalz/mdgan（作者还没写……只写了个readme） IDEA这里还是拿论文中一张图简要说一下作者思路，作者想构造更多的模型，他就像最后映射到一个高纬度空间，我们去聚类多个高斯模型这样就能形成更多的模型。 Work作者主要的工作就是这个公司，定义了一个lk函数，lk函数如下，他把D输出的结果当成k维embedding来处理，将k维度向量放到C个高斯分布中去计算lk值，这个值越高就说明他服从这个高斯分布，然后这里C个模型相当于他想去映射k个不同的模型来提高图像的多样性（我不太懂得是他这C个模型参数是提前给定参数还是训练自动调？？？）注意：这里的权值是1/d+1，可不可以改进一下，引入个过拟合项，哈哈。 然后实际损失函数如下，整体还是和GAN一样。 Experiments作者分别对MNIST、Fashion-MNIST、CIFAR-10、CelebA数据集做了实验，取VanillaGAN以及WGAN、DRAGAN做对比实验，这里展示MNIST数据集实验结果如下： 《Sphere Generative Adversarial Network Based on Geometric Moment Matching》本篇文章也是旨在改良GAN网络训练时容易出现模型崩塌和不稳定的问题。原始的GAN网络损失函数计算特征embedding的一阶矩，GAN在训练的过程中会出现不稳定和模型崩塌，WGAN/WGAN-gp虽然引用了Wasserstein距离，一定程度上解决了这个问题，但是这些模型同样存在问题：对于模型的参数要求还是比较高，因此SphereGAN尝试不引入新的参数的情况下将模型整体进行变换，达到稳定模型的效果。源码：https://github.com/Dotori-HJ/SphereGAN-Pytorch-implementation IDEA目前我还没全看完这篇文章，我理解的意思时，普通的一阶矩可能会突然距离很远，导致一些不稳定的现像，而我们把embedding弄到超球面，计算超球面距离就会被局限到超球面，以实现训练比较稳定的效果，可以看下图理解。另外，作者说利用一个高维的特征以及超球面的特征来保证充分评测数据，以有一个好的结果。两者兼备了！优秀。 Work下图展示了整个网络的示意图，前面和GAN网络一致修改了后面损失计算的部分。 损失函数如下： Experiments作者在CIFAR上做了实验，下图为实验结果，效果还算提高了很多。 《Geometry-Consistent Generative Adversarial Networks for One-Sided Unsupervised Domain Mapping》本文提出一种几何一致的无监督Image2ImageGAN，相比于CycleGAN、distanceGAN有一个更好的效果，好像文中没有去解释与别人相比是否好，文中说是is compatible with other well-studied unsupervised constraints.但是实验结果他当然证明比别人好！ IDEA起初image2image的生成对抗网络需要大量的pair图才能有一个好的效果，为此研究者提出好了cycleGAN构造Gxy和Gyx来把y作为中间隐含的变量；distanceGAN则是利用y距离来映射x之间距离，这里作者给出一种几何一致的方法，下图右侧所示，也是在不知道y的情况，用银含量去拟合。 Experiments作者采用了SVHM与MNIST数据集作为x、y，对比了DistanceGAN、CycleGAN等实验方法，结果如下： 《Towards Optimal Structured CNN Pruning via Generative Adversarial Learning》本文算是比较新颖的一篇文章，作者将GAN网络应用到模型剪枝方面，提出了GAL网络。源码：https://github.com/anonymouscvpr1983/GAL IDEA这篇文章引用一个softmask（软掩码）与稀疏限制的思想去试图删掉原网络的一部分层去看它的效果，来训练得到一个和baseline相似效果的网络。 Work网络框图如下所示，Generator完成对模型的压缩，Discrimantor完成对原始BaseLine模型和剪枝后的模型的评价，Discriminator优化使两个模型差距更大，而Generator使的模型效果和Baseline一致。Generator分三方面去改动： 1）BlockSelection：这个方法适合resnet等带残差的网络，将网络分成许多块，每个块在传播时乘以参数m，m的值决定该block需要的程度，如果最后发现m变为0则该层就可以删掉。2）BranchSelection：卷积核选择，对于1*1、3*3等卷积核分别乘以一定比例，决定卷积核对网络的影响多少；这个在googlenet等一些用多种卷积核的网络中有好处。 3）ChanelSelection：通道选择，这种优化方法对于每个卷积网络都适用。他是对于每一层卷积网络，在每它的每一个权重的基础上乘以参数m来限制该权重对整个模型的影响。最后清除m为0的权重。 Experiments作者做了大量实验证明GAL在保证模型准确率的情况下大大压缩了模型的参数量。如下图为MNIST数据集，LeNet与VGG模型的实验结果，作者用SSL和NISP方法对比， GAL 获得了最好的分类错误率与参数压缩量。除此之外，作者也对Resnet、GoogLenet、Densenet做了GAL优化，给出了优化结果。","link":"/posts/CVPR2019%20GAN%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"},{"title":"Docker基础知识学习（来自阿里云）","text":"之前实习一直听上线的老大说docker，但是一直没让我用过，我就自己学了下，docker是用来做镜像隔离的工具。那么什么是容器与镜像？如何构建容器与镜像？ 1、容器与镜像1.1 什么是容器一般系统下运行的进程是资源共享的，这样会存在一些问题： 1）进程攻击：因为这些进程能够相互看到并且进行通信，高级权限的进程可以攻击其他进程；2）共享文件：因为它们使用的是同一个文件系统，因此会带来两个问题：这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行；此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力；3）资源竞争**：因为这些进程使用的是同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量 CPU 和内存资源的时候，就可能会破坏其他应用的运行，导致其他应用无法正常地提供服务。 所以就有了容器这个东西，让一组进程有一个自己的集合，也就是容器。 容器就是一个视图隔离、资源可限制、独立文件系统的进程集合。所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等；控制资源使用率则是可以对于内存大小以及 CPU 使用个数等进行限制。容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图。 1.2 什么是镜像容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或者工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来。 我们将这些容器运行时所需要的所有的文件集合称之为容器镜像。 1）环境依赖冲突，比如做深度学习有时候你的代码对于tf的版本可能有特定要求，版本过高过低都会出问题，这样不同的程序可能对于系统都有着不同的要求。2）文件 2、如何构建镜像2.1 编写DockerFile文件一般我们新建一个文件夹，在里面存放必要文件和Dockerfile（名字不能改）文件。1vim Dockerfile例子如下：123456789101112# Base Images## 从天池基础镜像构建FROM registry.cn-shanghai.aliyuncs.com/tcc-public/python:3## 把当前文件夹里的文件构建到镜像的根目录下ADD . /## 指定默认工作目录为根目录（需要把run.sh和生成的结果文件都放在该文件夹下，提交后才能运行）WORKDIR /## 镜像启动后统一执行 sh run.shCMD [&quot;sh&quot;, &quot;run.sh&quot;]1、RUN命令：1234## 执行指令，可以安装，可以解压文件RUN yum install wgetRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-5.0.3.tar.gz&quot;RUN tar -xvf redis.tar.gz 2、CMD指令类似于 RUN 指令，用于运行程序，但二者运行的时间点不同:CMD 在docker run 时运行。RUN 是在 docker build。作用：为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。注意：如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。12CMD [&quot;sh&quot;, &quot;run.sh&quot;]CMD [&quot;python&quot;, &quot;helloworld.py&quot;]3、COPY复制指令，从上下文目录中复制文件或者目录到容器里指定路径。 4、ENTRYPOINT这个类似于CMD，但是这个指令不会被docker run的参数覆盖，一定会被执行。而且，docker run传递的参数将会作为ENTRYPOINT执行命令的参数。1ENTRYPOINT [&quot;/bin/bash&quot;,&quot;run.sh&quot;] 2.2 build and push一般我们编写一个镜像文件后，就可以通过 docker build 命令构建出所需要的应用。构建出的结果存储在本地。1docker build -t registry.cn-shenzhen.aliyuncs.com/test_for_tianchi/test_for_tianchi_submit:1.0 .其中registry.cn-shenzhen.aliyuncs.com/test_for_tianchi/test_for_tianchi_submit是云端地址，我们现在放在阿里云上的公共仓库，（你也可以自己注册一个），随后1.0代表版本号，用来区别你生成的不同版本。最后还有个.代表要操作的目录当前目录。 在实际生产环境中，我们需要将它传到云端仓库上。后期测试或使用是将它再拉下来。1docker push registry.cn-shenzhen.aliyuncs.com/test_for_tianchi/test_for_tianchi_submit:1.0 2.3 pull and run下拉镜像文件1docker pull box:1.12通过以下命令查看现存镜像文件1docker images运行镜像123docker run images_name:version (command)docker run registry.cn-shanghai.aliyuncs.com/cloudcver_test/test1:1.0 sh run.shdocker run registry.cn-shanghai.aliyuncs.com/cloudcver_test/test1:1.0 2.4一些常见指令1、启动一个镜像我们可以启动一个通用的镜像环境，比如ubuntu，再比如到了别人的电脑上可以快速布局一个python3环境，启动一个镜像即可，这时候加一个参数-i就是可以交互式操作，执行指令/bin/bash就可以进入命令行下1docker run -it ubuntu /bin/bash-i: 交互式操作。-t: 终端。/bin/bash执行的指令，也可以单独就执行一个指令sh run.sh。输入exit退出1exit注意：其实也可以后台启动一个镜像，加一个参数d，随后就docker exec id进入镜像即可。12docker run -itd --name ubuntu-test ubuntu /bin/bashdocker exec -it 243c32535da7 /bin/bash2、查看所有容器1docker ps -a如下展示我们运行的容器直接执行结束了，所以状态是退出的状态。3、启动一个容器或者重启12docker start 0f09f77f021edocker restart 0f09f77f021e4、停止一个容器1docker stop sss5、删除容器1docker rm -f 1e560fca3906下面的命令可以清理掉所有处于终止状态的容器。1docker container prune 6、删除镜像1docker rmi xxxCPU镜像：docker run your_image sh run.shGPU镜像：nvidia-docker run your_image sh run.sh 3 容器的生命周期容器是一组具有隔离特性的进程集合，在使用 docker run 的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为 initial 进程，这个 initial 进程启动的时候，容器也会随之启动，当 initial 进程退出的时候，容器也会随之退出。 因此，可以认为容器的生命周期和 initial 进程的生命周期是一致的。当然，因为容器内不只有这样的一个 initial 进程，initial 进程本身也可以产生其他的子进程或者通过 docker exec 产生出来的运维操作，也属于 initial 进程管理的范围内。当 initial 进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏。 但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷。 数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失。 通常情况下，数据卷管理主要有两种方式： 第一种是通过 bind 的方式，直接将宿主机的目录直接挂载到容器内；这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理。第二种是将目录管理交给运行引擎。 4 练习任务：阿里云天池大赛docker练习场https://tianchi.aliyun.com/competition/entrance/231759/introduction 答案备用：answer.py123456789101112131415161718# step 1print(&quot;hello world!&quot;)# step 2import csvcsv_file=csv.reader(open(&apos;/tcdata/num_list.csv&apos;,&apos;r&apos;))content=[] #用来存储整个文件的数据，存成一个列表，列表的每一个元素又是一个列表，表示的是文件的某一行for line in csv_file: content.append(int(line[0]))sum_num = sum(content)content.sort()content.reverse()# step 3import jsonprint(content[:10])test_dict = {&quot;Q1&quot;:&quot;Hello world&quot;,&quot;Q2&quot;:sum_num,&quot;Q3&quot;:content[-10:] }with open(&quot;./result.json&quot;,&quot;w&quot;) as f: json.dump(test_dict,f)run.sh1python answer.pyDockerfile12345678910111213# Base Images## 从天池基础镜像构建FROM registry.cn-shanghai.aliyuncs.com/tcc-public/python:3## 把当前文件夹里的文件构建到镜像的根目录下ADD . /## 指定默认工作目录为根目录（需要把run.sh和生成的结果文件都放在该文件夹下，提交后才能运行）WORKDIR /## 镜像启动后统一执行 sh run.shCMD [&quot;sh&quot;, &quot;run.sh&quot;]","link":"/posts/Docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%A5%E8%87%AA%E9%98%BF%E9%87%8C%E4%BA%91%EF%BC%89/"},{"title":"Faiss源码安装","text":"本篇文章提供了了Faiss安装的教程（CPU版安好了，GPU版有点问题，后补） Step 1 下载FAISS1git clone https://github.com/facebookresearch/faiss Step 2 生成配置12cd faiss./configure --with-cuda=/usr/local/cuda-9.0 --prefix=/home/cver/software/faiss/ --with-python=python3 configure生成makfile文件，这里需要配置cuda目录，安装目录prefix Step 3 安装faiss我运行时存在一些问题不能安装GPU版本，这里我把GPU部分屏蔽了，以后再解决。1234567891011vim makefile.inc-------------------------------NVCC = /usr/local/cuda-9.0/bin/nvccCUDA_ROOT = /usr/local/cuda-9.0//CUDA_ARCH = -gencode=arch=compute_35,code=compute_35 \\//-gencode=arch=compute_52,code=compute_52 \\//-gencode=arch=compute_60,code=compute_60 \\//-gencode=arch=compute_61,code=compute_61 \\//-gencode=arch=compute_70,code=compute_70 \\//-gencode=arch=compute_75,code=compute_75修改之后，编译，安装 12makemake install Step 4 安装swing3swing3是C++转python的桥梁…具体可以查一下。首先要下载swing312wget https://sourceforge.net/projects/swig/files/swig/swig-3.0.12/swig-3.0.12.tar.gztar -zxvf swig-3.0.12.tar.gz然后配置config123bash ./configure --prefix=/home/cver/software/swing3 --without-pcremakemake install配置环境变量12345vim ~/.bashrc-----------------------添加以下两句export SWIG_PATH=/home/cver/software/swing3/binexport PATH=$SWIG_PATH:$PATH Step 5 生成python包回到faiss目录下1make py仍然需要修改配置文件12345cd pythonvim Makefile---------------------我没有sudo权限，所以我安装到自己目录下$(PYTHON) setup.py install --prefix=/home/cver/.local配置好后编译，安装1make install","link":"/posts/Faiss%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85/"},{"title":"GoogleClab使用注意事项  !cd指令失效","text":"GoogleClab cd指令失效问题 注意：cd命令，用!cd 往往达不到自己的目的，需要用%cd 才能切换目录。","link":"/posts/GoogleClab%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%20%20!cd%E6%8C%87%E4%BB%A4%E5%A4%B1%E6%95%88/"},{"title":"Pycharm更新代码到服务器端","text":"到设置-Deploment部署里，点加号，添加一个。选择SFTP 然后，随便起个名字喽。再设置Connection选项。如下图之后需要设置Mappings，设置本地代码地址以及服务器代码地址，服务器代码地址点击右侧打开后去找就行。 设置好后点击ok随后再代码部分右击会有deployment，分别有上传，下载，还有sync这样会同步所有不一样的，可以设置。sync下是这样界面，点击那个箭头可以改变更新方向，默认是按照时间更新。","link":"/posts/Pycharm%E6%9B%B4%E6%96%B0%E4%BB%A3%E7%A0%81%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF/"},{"title":"Python基础学习（一）基本语法","text":"概述学习一门语言都要有一个helloworld，都要了解最基础的输入输出、注释、变量类型等基本的语法，对于其他的语法都是paper tiger。话不多说，我们一起来瞅瞅~~ Python基础学习（一）基本语法学习一门语言都要有一个helloworld，都要了解最基础的输入输出、注释、变量类型等基本的语法，对于其他的语法都是paper tiger。话不多说，我们一起来瞅瞅~~ 特点1、编程规范性python语言与之前的C语言、CPP语言最直观的表现在于其没有分号，它以缩进，对齐来实现其结构，所以python对编程的规范要求比较高，如下我们展示python的一段编程。for，if等以冒号代替花括号；123for it in range(9): print(it)print(\"end\")2、没有主函数如上所示，写下的代码都可以运行，无需掉主函数。 3、语句太长，换行语句太长可以用 \\ 反斜杠换行，如下： 1234567for (x,y,w,h) in faces: img3 = cv.rectangle(img1,(x,y),(x+w,y+w),(255,0,0),2) img4 = cv.putText(img3,'prepare %s picture'\\ #这里换行 %str(count+1),(20,80),\\#这里换行 cv.FONT_HERSHEY_PLAIN,2,(255,0,0)) f = cv.resize(img2[y:y+h,x:x+w],(200,200)) cv.imshow('13',img4) 打印函数print函数是最基本的应用，python3中有括号，python2中没有括号。 1print(&quot;hello,world!&quot;) 注释python的注释用#表示，如果对多行注释的话，可以用’’’在两个’’’之间的被注释掉。另外也可以改为”””三个双引号。 123456789'''本案例用来说明注释的作用。'''for it in range(9):#循环九次 print(it)#打印itprint(\"end\")\"\"\"程序结束\"\"\" 数字类型python中数字有四种类型：整数、布尔型、浮点数和复数。int (整数), 如 1bool (布尔), 如 True。float (浮点数), 如 1.23、3E-2complex (复数), 如 1 + 2j、 1.1 + 2.2j 输入输出读取键盘输入读取键盘输入可以用input语句 12str1 = input(\"请输入字符串\")print(str1) 字符串格式化输出字符串格式化输出类似于C语言的格式化输出。用print打印一个字符串，可以在后面跟上%，后面存入前面%对应的变量。12345a = 3.1415926b = 1c = 1.21print('常量 PI 的值近似为：%5.3f。' %a)print('b = %d,c = %.1f' %(b,c))注意：其中格式化输出符号如下表所示，另外.1代表保留一位小数。5.3代表至少占5个字符3位小数。新版格式化输出新版格式化输出类似，采用{}表示一个字段，用.format()的参数替代.举例： 123456&gt;&gt;&gt; print('{}网址： \"{}!\"'.format('菜鸟教程', 'www.runoob.com'))菜鸟教程网址： \"www.runoob.com!\"&gt;&gt;&gt; print('{0} 和 {1}'.format('Google', 'Runoob'))Google 和 Runoob&gt;&gt;&gt; print('{1} 和 {0}'.format('Google', 'Runoob'))Runoob 和 Google 也可以在{}中加入关键字 123&gt;&gt;&gt; print('站点列表 {0}, {1}, 和 {other}。'.format('Google',\\&gt;'Runoob',other='Taobao'))站点列表 Google, Runoob, 和 Taobao。 可选项 ‘:’ 和格式标识符可以跟着字段名。 这就允许对值进行更好的格式化。 下面的例子将 Pi 保留到小数点后三位：12345a = 3.1415926b = 1c = 1.21print('常量 PI 的值近似为：{0:.3f}'.format(a))print('b = %d,c = %.1f' %(b,c))如果你有一个很长的格式化字符串, 而你不想将它们分开, 那么在格式化时通过变量名而非位置会是很好的事情。 最简单的就是传入一个字典, 然后使用方括号 ‘[]’ 来访问键值 :123&gt;&gt;&gt; table = {'Google': 1, 'Runoob': 2, 'Taobao': 3}&gt;&gt;&gt; print('Runoob: {0[Runoob]:d}; Google: {0[Google]:d}; Taobao: {0[Taobao]:d}'.format(table))Runoob: 2; Google: 1; Taobao: 3 导入库python可以导入自带库或者自己写的库，用到import语句。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数或者某个类，格式为： from somemodule import somefunction从某个模块中导入多个函数或者多个类，格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import * 12345import cv2 as cv #导入CV2库并给他换了个名字import numpy as np import os,sysfrom CS231N_Data import load_CIFAR10#从我的CS231N_Data文件中导入load_CIFAR10类from neural_net import TwoLayerNet","link":"/posts/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"},{"title":"Python基础学习（三）面向对象","text":"本篇文章介绍python的类，面向对象的思想。 类和其它编程语言相比，Python 在尽可能不增加新的语法和语义的情况下加入了类机制。 Python中的类提供了面向对象编程的所有基本功能：类的继承机制允许多个基类，派生类可以覆盖基类中的任何方法，方法中可以调用基类中的同名方法。 对象可以包含任意数量和类型的数据。 python类相比其他的不同有以下几个特点：1、self参数作为对象参数，class里面每个函数都会有一个参数self（后面详细介绍）2、init初始化函数，类似于构造函数。3、多了一些内置的方法，我们都可以给他重构。 1234567891011121314#!/usr/bin/python3 class MyClass: \"\"\"一个简单的类实例\"\"\" i = 12345 def f(self): return 'hello world' # 实例化类x = MyClass() # 访问类的属性和方法print(\"MyClass 类的属性 i 为：\", x.i)print(\"MyClass 类的方法 f 输出为：\", x.f()) 类的属性方法属性就类似于C++中类的成员变量，有私有有共有。python也是一样。私有属性私有属性变量前面有两个下划线。例如__count代表一个私有成员。公有属性没有前置下划线。例如count、a。123class People: __age = 12#私有成员 name = \"Jack\"#公有成员 方法类似于C++中的函数。python的方法需要传入一个self代表某个类，调用的时候A.function1(b,c)其中A就是哪个self无需在括号里传值。如下例子定义了两个函数一个时getname一个时setname，getname设置参数为self，调用时直接A.get_name()就能获取到它的年龄，函数体里面改变成员变量用self.name即可。 123456789class People: __age = 12#私有成员 name = \"Jack\"#公有成员 def get_name(self): return self.name def set_name(self,str): self.name = strPeople AA.name() 私有方法private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。self.private_methods。 专用方法在类中有一系列专用方法，一般的类都会有的方法，也是类特有的方法，一般用到比较多的时构造函数init，在c++中也有，做初始化处理。再有就是打印函数用来打印结果repr。这些函数我们都可以重新构造。 __init : 构造函数，在生成对象时调用__del : 析构函数，释放对象时使用__repr : 打印，转换__setitem : 按照索引赋值__getitem: 按照索引获取值__len: 获得长度__cmp: 比较运算__call: 函数调用__add: 加运算__sub: 减运算__mul: 乘运算__truediv: 除运算__mod: 求余运算__pow: 乘方 继承继承也是类中的一个重要概念，在开发程序的过程中，如果我们定义了一个类A，然后又想新建立另外一个类B，但是类B的大部分内容与类A的相同时我们不可能从头开始写一个类B，这就用到了类的继承的概念。通过继承的方式新建类B，让B继承A，B会‘遗传’A的所有属性(数据属性和函数属性)，实现代码重用python也有继承这个规则 123456class DerivedClassName(BaseClassName1): &lt;statement-1&gt; . . . &lt;statement-N&gt; 在子类中可以重写方法，则调用字类时运行字类的新方法。 封装封装与扩展性封装在于明确区分内外，使得类实现者可以修改封装内的东西而不影响外部调用者的代码；而外部使用用者只知道一个接口(函数)，只要接口（函数）名、参数不变，使用者的代码永远无需改变。这就提供一个良好的合作基础——或者说，只要接口这个基础约定不变，则代码改变不足为虑。 多态多态指的是一类事物有多种形态 动物有多种形态：人，狗，猪 （附录）面向对象常用术语抽象/实现 抽象指对现实世界问题和实体的本质表现,行为和特征建模,建立一个相关的子集,可以用于 绘程序结构,从而实现这种模型。抽象不仅包括这种模型的数据属性,还定义了这些数据的接口。 对某种抽象的实现就是对此数据及与之相关接口的现实化(realization)。现实化这个过程对于客户 程序应当是透明而且无关的。 封装/接口 封装描述了对数据/信息进行隐藏的观念,它对数据属性提供接口和访问函数。通过任何客户端直接对数据的访问,无视接口,与封装性都是背道而驰的,除非程序员允许这些操作。作为实现的 一部分,客户端根本就不需要知道在封装之后,数据属性是如何组织的。在Python中,所有的类属性都是公开的,但名字可能被“混淆”了,以阻止未经授权的访问,但仅此而已,再没有其他预防措施了。这就需要在设计时,对数据提供相应的接口,以免客户程序通过不规范的操作来存取封装的数据属性。 注意：封装绝不是等于“把不想让别人看到、以后可能修改的东西用private隐藏起来” 真正的封装是，经过深入的思考，做出良好的抽象，给出“完整且最小”的接口，并使得内部细节可以对外透明 （注意：对外透明的意思是，外部调用者可以顺利的得到自己想要的任何功能，完全意识不到内部细节的存在） 合成 合成扩充了对类的 述,使得多个不同的类合成为一个大的类,来解决现实问题。合成 述了 一个异常复杂的系统,比如一个类由其它类组成,更小的组件也可能是其它的类,数据属性及行为, 所有这些合在一起,彼此是“有一个”的关系。 派生/继承/继承结构 派生描述了子类衍生出新的特性,新类保留已存类类型中所有需要的数据和行为,但允许修改或者其它的自定义操作,都不会修改原类的定义。继承描述了子类属性从祖先类继承这样一种方式继承结构表示多“代”派生,可以述成一个“族谱”,连续的子类,与祖先类都有关系。 泛化/特化 基于继承泛化表示所有子类与其父类及祖先类有一样的特点。特化描述所有子类的自定义,也就是,什么属性让它与其祖先类不同。 多态与多态性 多态指的是同一种事物的多种状态：水这种事物有多种不同的状态：冰，水蒸气 多态性的概念指出了对象如何通过他们共同的属性和动作来操作及访问,而不需考虑他们具体的类。 冰，水蒸气，都继承于水，它们都有一个同名的方法就是变成云，但是冰.变云(),与水蒸气.变云()是截然不同的过程，虽然调用的方法都一样 自省/反射 自省也称作反射，这个性质展示了某对象是如何在运行期取得自身信息的。如果传一个对象给你,你可以查出它有什么能力,这是一项强大的特性。如果Python不支持某种形式的自省功能,dir和type内建函数,将很难正常工作。还有那些特殊属性,像dict,name及doc 参考文献[1]菜鸟教程面向对象http://www.runoob.com/python3/python3-class.html[2]面向对象介绍https://www.cnblogs.com/wangmo/p/7751199.html","link":"/posts/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"title":"Python基础学习（二）错误与异常","text":"本节介绍以下python中的错误与异常，以及学会如何去自定义一个异常，这样有助于我们之后的调试。 错误与异常在编写程序时可能会出现一些语法错误，这时系统会提示语法错误，比如类型错误，无效参数等，如下给出一个例子： 12345&gt;&gt;&gt;while True print('Hello world') File \"&lt;stdin&gt;\", line 1, in ? while True print('Hello world') ^SyntaxError: invalid syntax 在运行时有时会发生一些异常，比如分母作为0了，以下给出一个抛出异常的例子： 1234&gt;&gt;&gt;10 * (1/0)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in ?ZeroDivisionError: division by zero 常见的错误及异常关键词如下： KeyboardInterrupt 用户中断执行(通常是输入^C)Exception 常规错误的基类StopIteration 迭代器没有更多的值GeneratorExit 生成器(generator)发生异常来通知退出StandardError 所有的内建标准异常的基类FloatingPointError 浮点计算错误OverflowError 数值运算超出最大限制ZeroDivisionError 除(或取模)零 (所有数据类型)SyntaxError Python 语法错误TypeError 对类型无效的操作ValueError 传入无效的参数 异常处理有时，对于异常我们可以加以限制，比如需要用户输入一个数，但是用户输入错误，抛出一个异常，系统可能中断，但这里我们可以采用一个异常处理，当抛出异常时重新让用户输入信息并给与提示。 这里就用到try语句，他可以尝试执行一句话如果抛出异常，则根据其异常的类别不同执行不同的语句。如下所示 12345678try:&lt;语句&gt; #运行代码except &lt;异常名字1&gt;：&lt;语句&gt; #如果在try代码引发了'异常名字1'异常，则执行这条语句except &lt;异常名字2&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'异常名字2'异常，并且获得附加的数据else:&lt;语句&gt; #如果没有异常发生 另外这里也可以不带异常名字，直接发生异常后执行一段代码。 下面给出一个让用户输入一个数字的代码，这里有个强制转换，如果转换失败应该会报出valueerror则打印输入错误。123456while True: try: x = int(input(\"Please enter a number: \")) break except ValueError: print(\"Oops! That was no valid number. Try again \") 抛出异常上述为检测一个异常的做法，在调试过程中，我们也可以自主的抛出异常，比如执行到某段程序，我们去测一下是否如我们所想像，如果不是我们想要的就让他抛出一个异常。 1234&gt;&gt;&gt;raise NameError('HiThere')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in ?NameError: HiThere raise 唯一的一个参数指定了要被抛出的异常。它必须是一个异常的实例或者是异常的类（也就是 Exception 的子类）。 如果你只想知道这是否抛出了一个异常，并不想去处理它，那么一个简单的 raise 语句就可以再次把它抛出。 12345678910&gt;&gt;&gt;try: raise NameError('HiThere') except NameError: print('An exception flew by!') raise An exception flew by!Traceback (most recent call last): File \"&lt;stdin&gt;\", line 2, in ?NameError: HiThere 自定义异常类我们也可以定义一个自己的异常类，其作为Exception的类的继承，可以设置参数。 12345class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value) 参考文献[1]菜鸟教程python3错误与异常http://www.runoob.com/python3/python3-errors-execptions.html","link":"/posts/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E9%94%99%E8%AF%AF%E4%B8%8E%E5%BC%82%E5%B8%B8/"},{"title":"Tensorflow学习总结（二）用Tensorflow实现一个全卷积神经网络","text":"学习完CS231N一系列课程后，我们都书写了自己的深层卷积网络，代码量不多。这里我们使用google的tensorflow框架去写一个卷积神经网络，并去把卷积网路中的优化一步一步复现出来。 搭建一个卷积神经网络，需要包括以下几个部分： 数据输入 前向传播 损失计算+正则化优化 网络优化 测试 一、数据输入数据输入，定义变量我们定义了两个输入数据的占位，方便之后运行时传入即可。然后定义了隐含层的参数输出层的参数。1234567891011#输入数据 X = tf.placeholder(tf.float32,[None,INPUT_NODE],name = \"x-input\") y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name = \"y-output\") #隐藏层参数 weight1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev = 0.1)) b1 = tf.Variable(tf.constant(0.1,shape = [LAYER1_NODE])) #输出层参数 weight2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev = 0.1)) b2 = tf.Variable(tf.constant(0.1,shape = [OUTPUT_NODE])) #定义step global_step = tf.Variable(0,trainable=False) 二、前向传播前向传播比价简单1、矩阵相乘matmul1a = tf.matmul(X,weight1)2、激活函数激活函数有很多种类RELU激活函数tf.nn.relu()1layer1 = tf.nn.relu(tf.matmul(X,weight1) + b1)3、一个两层的神经网络代码如下：1234#前向传播 #计算得分 layer1 = tf.nn.relu(tf.matmul(X,weight1) + b1) y = tf.matmul(layer1,weight2) + b2 三、损失计算+正则化优化经过前向传播我们得到了传播后的分数，之后我们需要计算它的损失，一部分是得分的损失，一部分是权重的正则损失。 softmax回归+交叉熵损失在计算损失之前，我们先对结果进行分类回归，一般对于多分类问题，我们采用softmax分类器进行回归，softmax可以将分数转变为概率的形式，就可以反应每个数据归属于每个类别的概率。然后我们采用交叉熵（cross_entropy）来计算损失。tensorflow中提供了tf.nn.sparse_softmax_cross_entropy_with_logits函数来计算损失，其输入有两个参数logits是回归结果，label是标签。logits填入待取log的概率化数据。格式为[N,C]，其中N代表样本数量，C代表最后的划分类别数。label填入标签数据，格式为[N,1]，其中N代表样本数量，每一行只有一个结果，取值0-C。这里我们用的数据集样本标签和logits格式是一样的，所以我们引用tf.argmax选取每一行的最大值所代表的编号，得到[N,1]类型结果。计算完后我们将所有损失加起来取平均。如下所示：123#计算损失 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels=tf.argmax(y_,1)) cross_entropy_mean = tf.reduce_mean(cross_entropy) 正则化有时候我们的数据量少，参数多的时候可能会出现过拟合现象，我们一般采取正则化的方式来优化，过拟合的原因在于权重参数可能在调解过程中变得过于复杂去适应我们的训练数据，就好比我们有五个不等式方程，我们去改善它的5*5个系数，使他对于我们的数据都能正确，如果一次取样10个数据，我们可能获得等式的25个唯一参数，很好的去适应这10个数据，但换一组数据它就不是那么好，在控制中就是鲁棒性不是太好。所以我们引入一个刻画权重参数复杂程度的指标J(θ)，然后用一个系数乘以它，来调节它对整个损失的影响比例。这就是正则化一般我们采用L1、L2损失函数，在tensorflow中为：1regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE)(weight)把L2改为L1就代表L1正则化。可以看出后面有两个括号，第一个括号参数为正则化系数，第二个为权重。也可以改写为下面的代码，先声明一个具有某个正则化系数的函数regular，然后在计算W1与W2时可以直接用regular(W1)。123#正则化损失regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE)regularization = regular(weight1) + regular(weight2)PS：两个正则化的对比：L1正则化会让参数变得稀疏（指会有更多参数变为0），而L2正则化则不会（因为参数的平方后会让小的参数变得更小，大的参数变得更大，同样起到了特征选取的功能，而不会让参数变为0）。其次是L1正则化计算不可导，而L2的正则化损失函数可导。 下面给出整体的损失计算代码12345678#计算损失 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels=tf.argmax(y_,1)) cross_entropy_mean = tf.reduce_mean(cross_entropy) #正则化损失 regular = tf.contrib.layers.l2_regularizer(REGULARZATION_RATE) regularization = regular(weight1) + regular(weight2) #总损失 loss = cross_entropy_mean + regularization 四、网络优化Optimizer计算完损失之后需要进行网络优化，网络优化也就是通过不停改变参数把损失值降低，一般我们多采用随机梯度下降法， Optimizer优化1、梯度下降法一般我们常用的优化算法为梯度下降法，也就是每次计算出梯度后对每一个参数进行线形调节X = X + learning_rate * Gradient1tf.train.GradientDescentOptimizer(learning_rate)#梯度下降法类这种优化算法存在一些问题，可能会出现有好几个波谷，但是梯度下降一直在一个波谷里面来回动荡。这会导致梯度下降法有时不能找到最优解，只能找到一个极值点。再者梯度下降法对所有数据进行运算，耗时太长。每一个优化方法类下都有一个minimize函数，对某个值进行最小化优化。1train_ = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) 2、随机梯度下降法（SGD）考虑梯度下降法优化速度慢的问题，后来有人提出随机梯度下降法，我们每次训练可以不训练所有的数据，可以只取一个小的batch数据进行训练，这样每次训练的速度就会加快，然后我们每次取样的batch不一样，这样就保证稳定性了。代码可以和上面一样，考虑每次训练取样batch即可。 更多的优化算法在我另一篇博客中介绍。 在这之后我们在考虑一个学习率的问题，学习率在一开始可能需要大一点比价好，加快收敛速度，但是在后期，就需要慢下来，因为幅度大的话可能在两边来回晃，很难收敛。 学习率优化问题这里给出一个指数衰减法的方法。让学习率以指数方式慢慢衰减。这里我抓取一张常用的图，tensorflow提供两个衰减情况，一个是连续衰减，一个是梯度衰减，如下图所示。tensorflow给出以下函数expoential_decay，参数有四个，基础学习率，全局步数，衰减步数，衰减率。其公式为：learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY^(global_step/decay_step)含义很明白，当全局步数达到衰减步数时学习率变为基础学习率的基础上乘以衰减率，如果是连续衰减时，在这过程中则一直变化。123#学习率更新 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,\\ decay_step,LEARNING_RATE_DECAY) 五、测试结果在进行一段时间训练后我们希望输出以测试集的正确性，利用tf.equal得到两组数据是否i相等，然后求结果的平均值，如下所示。tf.cast可以将其转换类型，因为equal返回的时bool型。故我们将其转换为float32再求平均数。 12correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1)) show_result = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) 六、运行首先开启一个会话，然后初始化所有变量，然后准备一个字典型数据，mnist数据集给了一个取样函数next_batch直接取样即可，然后运行_train把数据送进去。当运行100次时打印数据。12345678910111213with tf.Session() as sess: #初始化数据 tf.global_variables_initializer().run() #验证集数据 validate_feed = {X:mnist.validation.images,y_:mnist.validation.labels} for it in range(TRAINING_STEPS): XS,YS = mnist.train.next_batch(BATCH_SIZE)#取样数据 sess.run(train_,feed_dict={X:XS,y_:YS}) if it % 100 == 0: #打印数据 result = sess.run(show_result,feed_dict=validate_feed) print(\"After %d training step(s),validation accuracy is %g\"%(it,result))主函数我们编写如下代码：12345678mnist = input_data.read_data_sets('./', one_hot=True)#mnist = input_data.read_data_sets(\"/\",one_hot = True)print(mnist.train.num_examples)print(mnist.train.images[0])train(mnist)注意：这里我们是在本地存了一些数据，如有需要可以从我的github获取用Tensorflow搭建两层神经网络训练MNIST数据集 最后结果如下所示","link":"/posts/Tensorflow%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89%E7%94%A8Tensorflow%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20-%20%E5%89%AF%E6%9C%AC/"},{"title":"VIM最近使用常见指令总结","text":"本文总结了VIM编辑器的一些常见指令 1 基本功 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 1.1 命令模式：用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。 i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 1.2 输入模式在命令模式下按下i就进入了输入模式。输入状态就直接打字就可以，记住以下几个指令 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 记住写完了按下ESC就回到命令模式1.1中 1.3 底线命令模式在命令模式下按下:（英文冒号）就进入了底线命令模式。底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 一般我们写完了，按ESC回退到命令模式，再按:qw回车保存并退出。 2 命令模式其他功能命令模式下页面变换 nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行 gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n n 为数字。光标向下移动 n 行(常用) [Ctrl] +[f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于[Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 搜索替换功能 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！(常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :1,$s/word1/word2/g 或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 参考文献[1]菜鸟教程 https://www.runoob.com/linux/linux-vim.html","link":"/posts/VIM%E6%9C%80%E8%BF%91%E4%BD%BF%E7%94%A8%E5%B8%B8%E8%A7%81%E6%8C%87%E4%BB%A4%E6%80%BB%E7%BB%93/"},{"title":"WGAN代码解读及实验总结","text":"GAN作为图像的另一个新领域，本成为21世纪最好的idea。嘿嘿，最近小试牛刀，下载了个WGAN的代码，这里简单分析下，给大家一个参考。 【提示】本文预计阅读时间5分钟，带灰色底纹的和加粗的为重要部分哦！ （一）WGAN初识 （二）代码分析2.1 main struct打开代码后，它的主要结构如下图所示。我们先看一下wgan_conv主函数，打开之后首先直接到最底main的位置，如下1234567891011121314151617if __name__ == '__main__': os.environ['CUDA_VISIBLE_DEVICES'] = '0' # the dir of pic generated sample_folder = 'Samples/mnist_wgan_conv' if not os.path.exists(sample_folder): os.makedirs(sample_folder) # net param generator = G_conv_mnist() discriminator = D_conv_mnist() # data param data = mnist() # run wgan = WGAN(generator, discriminator, data) wgan.train(sample_folder)这里做几点阐述 1、首先创建了一个目录用来存储你的生成图像，程序会每隔一段时间输出一个图像。2、搞了三个类，一个generater生成器网络，一个是discriminator判别器类，然后是数据类。3、又声明一个对象WGAN网络，然后调用它的train函数 OK至此，主函数结构阐述清楚。那此时你会想generater咋定义？discriminator咋定义？好一个一个看。 2.2 generatorgenerator是生成器网络，其实就是搭了一个上采样的网络，先将噪声输入一维向量，通过全连接到更多的数据，然后把它展开成二维的图像，这里我们先用的灰度，你也可以改成彩色。然后再上采样，随意搞得，反正最后你要上采样到和你的正样本图像维度一致。如下所示：123456789101112131415161718192021class G_conv_mnist(object): def __init__(self): self.name = 'G_conv_mnist' def __call__(self, z): with tf.variable_scope(self.name) as scope: #step 1 全连接层，把z白噪声变为8*15*128图 g = tcl.fully_connected(z, 8*15*128, activation_fn = tf.nn.relu, normalizer_fn=tcl.batch_norm, weights_initializer=tf.random_normal_initializer(0, 0.02)) g = tf.reshape(g, (-1, 8, 15, 128)) #step 2 反卷积/上采样 到16*30*64图 4代表卷积核大小 g = tcl.conv2d_transpose(g, 64, 4,stride=2, activation_fn=tf.nn.relu, normalizer_fn=tcl.batch_norm, padding='SAME', weights_initializer=tf.random_normal_initializer(0, 0.02)) #step 3 反卷积/上采样 到32*60*1的图，此时和真实手写体的数据是一样的图 g = tcl.conv2d_transpose(g, 1, 4, stride=2, activation_fn=tf.nn.sigmoid, padding='SAME', weights_initializer=tf.random_normal_initializer(0, 0.02)) print(g.shape) return g @property def vars(self): return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name) 注意：这里你会看到一个call函数，它是咋用呢？一个类下面有个call函数，你就可以生成一个对象后，直接把它当成方法用。例如class G(): call(x): print(x)这样的话你就A = G()，然后再A(1)就打印了1。其实就是说这个类弄好了，之后可以直接当函数用。 好，然后我们看一下discriminator 2.3 discriminator和generator干了差不多的事情，他要把X和GX输进去，然后搭建一个卷积网络判别真假。 123456789101112131415161718192021222324class D_conv_mnist(object): def __init__(self): self.name = 'D_conv_mnist' def __call__(self, x, reuse=False): with tf.variable_scope(self.name) as scope: if reuse: scope.reuse_variables() size = 64 #step 1 卷积4*4卷积核 bzx30x60x1 -&gt; bzx15x30x64 shared = tcl.conv2d(x, num_outputs=size, kernel_size=4, stride=2, activation_fn=lrelu) #step 2 卷积4*4卷积核 bzx15x30x64 -&gt; bzx7x15x128 shared = tcl.conv2d(shared, num_outputs=size * 2, kernel_size=4, stride=2, activation_fn=lrelu, normalizer_fn=tcl.batch_norm) #step 3 展开向量 bzx7x15x128 -&gt; bzx6372 shared = tcl.flatten(shared) #step 4 全连接层 d = tcl.fully_connected(shared, 1, activation_fn=None, weights_initializer=tf.random_normal_initializer(0, 0.02)) q = tcl.fully_connected(shared, 128, activation_fn=lrelu, normalizer_fn=tcl.batch_norm) q = tcl.fully_connected(q, 10, activation_fn=None) # 10 classes return d, q @property def vars(self): return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name) 2.4 数据的导入改写我下载的代码是直接导入的minist数据集，我们可能要导入图片数据集哈，这里我做了一些改变。这里加了个next_batch函数，先生成随机序列，然后读取batch图像，存到数据集中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class mnist(): def __init__(self, flag='conv', is_tanh = False): self.datapath = prefix + 'bus_data/' self.X_dim = 784 # for mlp self.z_dim = 100 self.y_dim = 10 self.sizex = 32 # for conv self.sizey = 60 # for conv self.channel = 1 # for conv #self.data = input_data.read_data_sets(datapath, one_hot=True) self.flag = flag self.is_tanh = is_tanh self.Train_nums = 17 def __call__(self,batch_size): batch_imgs = self.next_batch(self.datapath,batch_size) #batch_imgs,y = self.next_batch(prefix,batch_size) if self.flag == 'conv': batch_imgs = np.reshape(batch_imgs, (batch_size, self.sizex, self.sizey, self.channel)) if self.is_tanh: batch_imgs = batch_imgs*2 - 1 #return batch_imgs, y return batch_imgs def next_batch(self,data_path, batch_size): #def next_batch(self,data_path, lable_path, batch_size): train_temp = np.random.randint(low=0, high=self.Train_nums, size=batch_size) # 生成元素的值在[low,high)区间，随机选取 train_data_batch = np.zeros([batch_size,self.sizex, self.sizey]) # 其中[img_row, img_col, 3]是原数据的shape，相应变化 #train_label_batch = np.zeros([batch_size, self.size, self.size]) # count = 0 # 后面就是读入图像，并打包成四维的batch #print(data_path) img_list = os.listdir(data_path) #print(img_list) for i in train_temp: img_path = os.path.join(data_path, img_list[i]) # 图片文件 img = cv.imread(img_path) gray = cv.cvtColor(img,cv.COLOR_RGB2GRAY) train_data_batch[count, :, :] = cv.resize(gray,(self.sizey, self.sizex)) count+=1 return train_data_batch#, train_label_batch def data2fig(self, samples): if self.is_tanh: samples = (samples + 1)/2 fig = plt.figure(figsize=(4, 4)) gs = gridspec.GridSpec(4, 4) gs.update(wspace=0.05, hspace=0.05) for i, sample in enumerate(samples): ax = plt.subplot(gs[i]) plt.axis('off') ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_aspect('equal') plt.imshow(sample.reshape(self.sizex,self.sizey), cmap='Greys_r') return fig 2.5 WGAN网络首先是搭网络NET，discriminator分别把真实的正样本X投进去，把噪声产生的G_sample投进去，得到正负结果。 12345# nets self.G_sample = self.generator(self.z) self.D_real, _ = self.discriminator(self.X) self.D_fake, _ = self.discriminator(self.G_sample, reuse = True) 然后就是计算损失。我们利用上面结果分别计算D和G的损失，然后有两个优化器，分别对于D和G123456# loss self.D_loss = - tf.reduce_mean(self.D_real) + tf.reduce_mean(self.D_fake) self.G_loss = - tf.reduce_mean(self.D_fake) self.D_solver = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(self.D_loss, var_list=self.discriminator.vars) self.G_solver = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(self.G_loss, var_list=self.generator.vars)这里网络就搭建好了，我们要看一下train函数。其主要是先优化D再优化G这个步骤，这里我么此优化G和D的次数相同，你也可以去调整这个n_d。1234567891011121314151617181920for epoch in range(training_epoches): # update D n_d = 20 if epoch &lt; 250 or (epoch+1) % 500 == 0 else 10 for _ in range(n_d): #X_b, _ = self.data(batch_size) X_b= self.data(batch_size) self.sess.run(self.clip_D) self.sess.run( self.D_solver, feed_dict={self.X: X_b, self.z: sample_z(batch_size, self.z_dim)} ) # update G for _ in range(n_d): #X_b, _ = self.data(batch_size) X_b= self.data(batch_size) self.sess.run(self.clip_D) self.sess.run( self.G_solver, feed_dict={self.z: sample_z(batch_size, self.z_dim)} )对于WGAN的全部代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class WGAN(): def __init__(self, generator, discriminator, data): self.generator = generator self.discriminator = discriminator self.data = data self.z_dim = self.data.z_dim self.sizex = self.data.sizex self.sizey = self.data.sizey self.channel = self.data.channel self.X = tf.placeholder(tf.float32, shape=[None, self.sizex, self.sizey, self.channel]) self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim]) # nets self.G_sample = self.generator(self.z) self.D_real, _ = self.discriminator(self.X) self.D_fake, _ = self.discriminator(self.G_sample, reuse = True) # loss self.D_loss = - tf.reduce_mean(self.D_real) + tf.reduce_mean(self.D_fake) self.G_loss = - tf.reduce_mean(self.D_fake) self.D_solver = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(self.D_loss, var_list=self.discriminator.vars) self.G_solver = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(self.G_loss, var_list=self.generator.vars) # clip self.clip_D = [var.assign(tf.clip_by_value(var, -0.01, 0.01)) for var in self.discriminator.vars] gpu_options = tf.GPUOptions(allow_growth=True) self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) def train(self, sample_folder, training_epoches = 100000, batch_size = 5): i = 0 self.sess.run(tf.global_variables_initializer()) for epoch in range(training_epoches): # update D n_d = 20 if epoch &lt; 250 or (epoch+1) % 500 == 0 else 10 for _ in range(n_d): #X_b, _ = self.data(batch_size) X_b= self.data(batch_size) self.sess.run(self.clip_D) self.sess.run( self.D_solver, feed_dict={self.X: X_b, self.z: sample_z(batch_size, self.z_dim)} ) # update G for _ in range(n_d): #X_b, _ = self.data(batch_size) X_b= self.data(batch_size) self.sess.run(self.clip_D) self.sess.run( self.G_solver, feed_dict={self.z: sample_z(batch_size, self.z_dim)} ) # print loss. save images. if epoch % 100 == 0 or epoch &lt; 100: D_loss_curr = self.sess.run( self.D_loss, feed_dict={self.X: X_b, self.z: sample_z(batch_size, self.z_dim)}) G_loss_curr = self.sess.run( self.G_loss, feed_dict={self.z: sample_z(batch_size, self.z_dim)}) print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'.format(epoch, D_loss_curr, G_loss_curr)) if epoch % 1000 == 0: samples = self.sess.run(self.G_sample, feed_dict={self.z: sample_z(16, self.z_dim)}) print(samples.shape) fig = self.data.data2fig(samples) plt.savefig('{}/{}.png'.format(sample_folder, str(i).zfill(3)), bbox_inches='tight') i += 1 plt.close(fig) （三）实验结果我找了17张车的图片~~原谅我比较懒，如下图所示。基本都是差不多样子的。然后代码跑起来~我们把它resize到（30，60），主要是为了让我的机器跑快些，本来就怂。一开始是一堆噪声图如下图所示。其实在训练一段时间后如下所示，可以看出具有一定车的样子，中间黑车身，貌似也能看到个车轱辘。哈哈。初见效果~损失的结果如下所示： （四）总结通过这个实验对于GAN有了初步的了解，如果有什么写的不对的地方，还请指出。这里附上代码：https://github.com/Harryjun/wgan_demo","link":"/posts/WGAN%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"title":"YouTube-8M feature extraction bug","text":"源码编译OPENCV C++版本 linux YouTube-8M feature extraction bug记录 Error 1 fatal error: opencv2/core.hpp: No such file or directory1234567ERROR: /data/menglingjun/mediapipe/mediapipe/framework/formats/BUILD:163:1: C++ compilation of rule '//mediapipe/framework/formats:image_frame_opencv' failed (Exit 1) gcc failed: error executing command /usr/local/gcc/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 70 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandboxIn file included from ./mediapipe/framework/formats/image_frame_opencv.h:20:0, from mediapipe/framework/formats/image_frame_opencv.cc:15:./mediapipe/framework/port/opencv_core_inc.h:18:36: fatal error: opencv2/core/version.hpp: No such file or directorycompilation terminated. 没有找到opencv2/core/version.hpp解决方案：修改./mediapipe/framework/port/opencv_core_inc.h中的路径，我的是opencv4，在路径/opencv/include/opencv4/opencv2下，所以改为123456#include &lt;opencv4/opencv2/core/version.hpp&gt;#ifdef CV_VERSION_EPOCH // for OpenCV 2.x#include &lt;opencv4/opencv2/core/core.hpp&gt;#else#include &lt;opencv4/opencv2/core/core.hpp&gt; Error2 fatal error: libavcodec/avcodec.h: No such file or directory12345ERROR: /data/menglingjun/mediapipe/mediapipe/util/BUILD:67:1: C++ compilation of rule '//mediapipe/util:audio_decoder' failed (Exit 1) gcc failed: error executing command /usr/local/gcc/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 81 argument(s) skipped)Use --sandbox_debug to see verbose messages from the sandboxIn file included from mediapipe/util/audio_decoder.cc:15:0:./mediapipe/util/audio_decoder.h:33:32: fatal error: libavcodec/avcodec.h: No such file or directory","link":"/posts/YouTube-8M%20feature%20extraction%20bug/"},{"title":"anaconda + python3.6安装","text":"anaconda + python3.6安装 清华镜像连接https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/","link":"/posts/anconda%E7%89%88%E6%9C%AC%E5%8F%B7%E5%AF%B9%E5%BA%94python%E7%89%88%E6%9C%AC/"},{"title":"bazel 总结","text":"最近在用Utube-8M的mediapipe框架提取视频特征，用到了bazel编译，遇到一些问题做一些总结。 WORKSPACEWORKSPACE位于你的项目根目录，标志你的工作区。用于引入外部依赖关系，通常是源代码位于主存储库之外。 BUILD文件BUILD文件给出了编译目标的规则，包括target名称，源文件srcs，头文件hdrs，依赖deps12345678cc_binary( name = &quot;hello-world&quot;, srcs = [&quot;hello-world.cc&quot;], hdrs = [&quot;hello-world.hpp&quot;, &quot;helo.hpp&quot;, ], deps = [&quot;//lib:hello-time&quot;])具体规则可以参考源网站https://docs.bazel.build/versions/master/be/c-cpp.html#cc_binary deps依赖可以包含内部依赖也可以包含外部依赖，内部依赖这里的路径是以WORKSPACE的路径开始的相对路径，例如deps = [“//lib:hello-time”]代表WORKSPACE路径下有lib文件夹，lib里面有hello-time.cc，并且有BUILD文件，这个就是内部依赖，还没有编译，在你编译主文件时他会自动链接先编译这个文件。 外部依赖就是可以调用你安装好的包，比如opencv包，这时你需要先改WORKSPACE加入环境，再写一个BUILD调用。1、WORKSPACE加入环境，它指定了名字，path就是环境遍历，build_file就是build的文件再third_party文件夹下opencv_linux.BUILD中。12345new_local_repository( name = &quot;linux_opencv&quot;, build_file = &quot;@//third_party:opencv_linux.BUILD&quot;, path = &quot;/home/menglingjun/menglingjun/software/opencv&quot;,)2、改opencv_linux.BUILD加入源文件，头文件，include啥的，都是相对于你前面的path123456789101112131415161718192021cc_library( name = &quot;opencv&quot;, srcs = glob( [ &quot;lib64/libopencv_core.so&quot;, &quot;lib64/libopencv_highgui.so&quot;, &quot;lib64/libopencv_imgcodecs.so&quot;, &quot;lib64/libopencv_imgproc.so&quot;, &quot;lib64/libopencv_video.so&quot;, &quot;lib64/libopencv_videoio.so&quot;, ], ), hdrs = glob( [ &quot;include/opencv4/opencv2/**/*.h*&quot;, &quot;include/opencv4/opencv2/*.h*&quot;, ]), includes = [&quot;include/opencv4&quot;,], # [&quot;include&quot;], linkstatic = 1, visibility = [&quot;//visibility:public&quot;],)3、你要编译的目标如何依赖把opencv加上就可以了。12345678cc_binary( name = &quot;hello-world&quot;, srcs = [&quot;hello-world.cc&quot;], hdrs = [&quot;hello-world.hpp&quot;, &quot;helo.hpp&quot;, ], deps = [&quot;//third_party:opencv&quot;]) 编译文件Test 1编译单独一个目标在你的WORKSPACE所在目录下运行bazel build，运行main文件夹下的hello-world.ccstage1 ├── main │ ├── BUILD │ └── hello-world.cc └── WORKSPACE1bazel build //main:hello-world Test 2编译带有依赖的目标如果hello-world.cc的文件需要调用hello-greet的包此时就需要加入deps依赖。build文件如下所示12345678910111213cc_library( name = &quot;hello-greet&quot;, srcs = [&quot;hello-greet.cc&quot;], hdrs = [&quot;hello-greet.h&quot;],)cc_binary( name = &quot;hello-world&quot;, srcs = [&quot;hello-world.cc&quot;], deps = [ &quot;:hello-greet&quot;, ],)这样再编译hello-world，bazel就会首先编译hello-greet再对hello-world.cc进行编译。指令我们还是一样1bazel build //main:hello-world","link":"/posts/bazel%E5%B0%8F%E6%80%BB%E7%BB%93/"},{"title":"csv文件转换xml格式文件","text":"csv文件转换xml格式文件 CSV文件格式12345678910image_name,x1,y1,x2,y2,classdb1.jpg,6,1550,145,1704,objectdb1.jpg,151,1545,288,1704,objectdb1.jpg,291,1552,472,1704,objectdb1.jpg,733,1600,863,1704,objectdb1.jpg,881,1592,1015,1704,objectdb10.jpg,865,592,1043,848,objectdb10.jpg,1823,1228,1958,1442,objectdb10.jpg,1147,1237,1323,1446,objectdb10.jpg,1708,583,1840,832,object转xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# .csv--&gt;.xml# ! /usr/bin/python# -*- coding:UTF-8 -*-import os, sysimport globfrom PIL import Imageimport csvimport osimport numpy as npimport randomimport requests# VEDAI 图像存储位置src_img_dir = &quot;/home/train/dataset-expand/park_voc/VOC2007/JPEGImages&quot;# VEDAI 图像的 ground truth 的 txt 文件存放位置src_txt_dir = &quot;/home/train/dataset-expand/label_expand&quot;src_xml_dir = &quot;/home/train/dataset-expand/park_voc/VOC2007/Annotations&quot;img_Lists = glob.glob(src_img_dir + &apos;/*.jpg&apos;)# read csv 1 without width and heightfile_path = &quot;WebMarket_coco_gt_object.csv&quot;with open(file_path) as csvfile: csv_reader = csv.reader(csvfile) # 使用csv.reader读取csvfile中的文件 birth_header = next(csv_reader) # 读取第一行每一列的标题 count = 0 img_pre = &apos;&apos; flag = 0 for row in csv_reader: # 将csv 文件中的数据保存到birth_data中 #print(row) #first if flag == 0: img_pre = row[0].split(&apos;.&apos;)[0] flag = 1 xml_file = open((&apos;./WebMarket_coco_gt_object/&apos; + img_pre + &apos;.xml&apos;), &apos;w&apos;) xml_file.write(&apos;&lt;annotation&gt;\\n&apos;) xml_file.write(&apos; &lt;folder&gt;VOC2007&lt;/folder&gt;\\n&apos;) xml_file.write(&apos; &lt;filename&gt;&apos; + str(img_pre) + &apos;.jpg&apos; + &apos;&lt;/filename&gt;\\n&apos;) img = row[0].split(&apos;.&apos;)[0] # new file if img != img_pre: # close file xml_file.write(&apos;&lt;/annotation&gt;&apos;) xml_file.close() # new file xml_file = open((&apos;./WebMarket_coco_gt_object/&apos; + img + &apos;.xml&apos;), &apos;w&apos;) xml_file.write(&apos;&lt;annotation&gt;\\n&apos;) xml_file.write(&apos; &lt;folder&gt;VOC2007&lt;/folder&gt;\\n&apos;) xml_file.write(&apos; &lt;filename&gt;&apos; + str(img) + &apos;.jpg&apos; + &apos;&lt;/filename&gt;\\n&apos;) #print(img) &apos;&apos;&apos; xml_file.write(&apos; &lt;size&gt;\\n&apos;) xml_file.write(&apos; &lt;width&gt;&apos; + str(width) + &apos;&lt;/width&gt;\\n&apos;) xml_file.write(&apos; &lt;height&gt;&apos; + str(height) + &apos;&lt;/height&gt;\\n&apos;) xml_file.write(&apos; &lt;depth&gt;3&lt;/depth&gt;\\n&apos;) xml_file.write(&apos; &lt;/size&gt;\\n&apos;) &apos;&apos;&apos; xml_file.write(&apos; &lt;object&gt;\\n&apos;) xml_file.write(&apos; &lt;name&gt;&apos; + str(row[5]) + &apos;&lt;/name&gt;\\n&apos;) xml_file.write(&apos; &lt;pose&gt;Unspecified&lt;/pose&gt;\\n&apos;) xml_file.write(&apos; &lt;truncated&gt;0&lt;/truncated&gt;\\n&apos;) xml_file.write(&apos; &lt;difficult&gt;0&lt;/difficult&gt;\\n&apos;) xml_file.write(&apos; &lt;bndbox&gt;\\n&apos;) xml_file.write(&apos; &lt;xmin&gt;&apos; + str(row[1]) + &apos;&lt;/xmin&gt;\\n&apos;) xml_file.write(&apos; &lt;ymin&gt;&apos; + str(row[2]) + &apos;&lt;/ymin&gt;\\n&apos;) xml_file.write(&apos; &lt;xmax&gt;&apos; + str(row[3]) + &apos;&lt;/xmax&gt;\\n&apos;) xml_file.write(&apos; &lt;ymax&gt;&apos; + str(row[4]) + &apos;&lt;/ymax&gt;\\n&apos;) xml_file.write(&apos; &lt;/bndbox&gt;\\n&apos;) xml_file.write(&apos; &lt;/object&gt;\\n&apos;)","link":"/posts/csv%E6%96%87%E4%BB%B6%E8%BD%AC%E6%8D%A2xml%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6/"},{"title":"from tensorflow.python.keras.utils import tf_utils ImportError':' cannot import name 'tf_utils'","text":"Debug:在安装keras出现问题 问题：12from tensorflow.python.keras.utils import tf_utilsImportError: cannot import name 'tf_utils' 原因：Seems like it was a problem with keras 2.3.0, I installed keras 2.1.5 using pip and it works fine. 解决方案：1pip install keras==2.1.5 -i https://pypi.tuna.tsinghua.edu.cn/simple 参考来源https://stackoverflow.com/questions/57985406/cannot-import-name-tf-utils-when-using-importing-keras","link":"/posts/from%20tensorflow.python.keras.utils%20import%20tf_utils%20ImportError_%20cannot%20import%20name%20'tf_utils'/"},{"title":"glib源码编译安装与libffi源码编译安装","text":"源码安装！啊！glib源码编译安装与libffi源码编译安装 安装glib安装glib 下载glib http://ftp.acc.umu.se/pub/GNOME/sources/glib/1234tar -xvf glib-2.48.1.tar.xz./configure --prefix=/home/meng/software/glibmake -j12make install之后需要配置环境遍历。1export PKG_CONFIG_PATH=$HOME/menglingjun/software/glib/lib/pkgconfig 你可能会会遇到这个错误，缺少libffi，直接到安装libffi12345678No package 'libffi' foundConsider adjusting the PKG_CONFIG_PATH environment variable if youinstalled software in a non-standard prefix.Alternatively, you may set the environment variables LIBFFI_CFLAGSand LIBFFI_LIBS to avoid the need to call pkg-config.See the pkg-config man page for more details. 你可能会会遇到这个错误，缺少libffi，直接到安装libpcre123configure: error: Package requirements (libpcre &gt;= 8.13) were not met:No package 'libpcre' found 安装libffi去官网下载libffi http://sourceware.org/libffi/，然后解压，configure，编译make，安装install。这里需要注意如果不是root就需要装在自己的目录下，config需要加上prefix参数12345tar xf libffi-3.0.0.tar.gzcd libffi-3.0.0./configure --prefix=/home/meng/software/libffimake -j12make install之后需要配置环境遍历。1234export LIBFFI_CFLAGS=-I$HOME/menglingjun/software/libffi/lib/libffi-3.2.1/includeexport LIBFFI_LIBS=$HOME/menglingjun/software/libffi/lib/libffi.laexport C_INCLUDE_PATH=$HOME/menglingjun/software/libffi/lib/libffi-3.2.1/include/export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/menglingjun/software/libffi/lib/pkgconfig 安装libpcrehttps://sourceforge.net/projects/pcre/files/pcre/ 1234tar -jxvf pcre-8.39.tar.bz2./configure --prefix=/home/menglingjun/menglingjun/software/pcre/make -j12make install 配置环境变量1export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/menglingjun/software/pcre/lib/pkgconfig 参考博客https://www.cnblogs.com/pcat/p/5520317.html","link":"/posts/glib%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E4%B8%8Elibffi%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/posts/hello-world/"},{"title":"hexo部署博客到阿里云服务器","text":"由于疫情原因，阿里云免费送了半年的服务器，不得拿来玩玩，然后我就先把我的博客从github上迁移到阿里云上来，以后有问题再换。 先前条件：1、服务器2、本地电脑已经安装hexo，并且前期已经上传过GitHub（这样的话，你的本地已经有了密钥） 以下部分全是在服务器端的操作！以下部分全是在服务器端的操作！以下部分全是在服务器端的操作！ 1、配置服务器安全组规则由于阿里云是默认不授权80端口的访问的，所以我们要手动配置。 打开阿里云服务器管理控制台-&gt;点击左侧安全组-&gt;点击右侧的配置规则-&gt;点击添加安全组规则 如下图 2、配置代理nginx因为我们是拿nginx做Web服务器，所以我们需要安装部署好nginx，如果没有安装。以下为centos命令，其他自己百度。12345安装执行命令如下yum install -y nginx启动服务器：systemctl start nginxsystemctl enable nginx现在可以访问一下我们的公网IP，会进入一个默认的nginx界面，我的转到了centos介绍界面。 但是我们实际上是想要让这个地址指向我们的博客，而不是nginx的默认网址，这就需要我们去配置nginx的配置文件。 12cd /etc/nginxvim nginx.conf 找到server，更改如下123456789101112# Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server { listen 80 default_server; listen [::]:80 default_server; server_name www.cloudcver.com;###改成你的域名；没有域名改成服务器公网IP root /home/git/projects/blog;## 改为服务器上存博客的地址，按照我这写就行。 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; 3、搭建服务器git仓库3.1 安装node12curl -sL https://rpm.nodesource.com/setup_10.x | bash -yum install -y nodejs 3.2 安装git1yum install git 3.3 创建git用户先添加一个git用户1adduser git修改用户权限12chmod 740 /etc/sudoersvi /etc/sudoers添加以下内容保存退出后 将sudoers文件权限改回原样1chmod 400 /etc/sudoers设置git用户的密码1sudo passwd git 3.4 创建SSH密钥我们本地电脑已经生成过密钥，直接复制到服务器上即可。123456su gitmkdir ~/.sshvim ~/.ssh/authorized_keys#然后将电脑中~/.ssh/id_rsa.pub内容复制过来粘贴。chmod 600 ~/.ssh/authorzied_keyschmod 700 ~/.ssh 3.5 创建git仓库在git用户下操作12345mkdir -p projects/blog # 把项目目录建立起来mkdir repos &amp;&amp; cd reposgit init --bare blog.git # 创建仓库cd blog.git/hooksvim post-receive # 创建一个钩子钩子内容：12#!/bin/shgit --work-tree=/home/git/projects/blog --git-dir=/home/git/repos/blog.git checkout -f保存文件，文件加权限123chmod +x post-receive # 添加可执行权限exit # 返回到root用户chown -R git:git /home/git/repos/blog.git # 给git用户添加权限测试以下，在本地电脑上测试。sever_ip为你的公网IP1ssh git@server_ip 4本地电脑上传在hexo上添加一个部署的端1234567deploy: type: git repo: fuwuqi: git@server_ip:/home/git/repos/blog.git coding: git@e.coding.net:cloudcver/blogme.git github: git@github.com:Harryjun/Harryjun.github.io.git branch: master 5线上更新下服务器上reload一下1nginx -s reload打开自己网站会发现快了很多。www.cloudcver.com","link":"/posts/hexo%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"title":"insightface中recognition训练过程","text":"insightface中recognition训练过程 1 config文件到目录/recognition/下有sample_config.py文件，这是给了个样例配置文件，我们复制一下起名config.py然后编辑 config.py 12cp sample_config.py config.pyvim config.py config下做了一些配置 default默认配置config配置network网络模型参数loss损失函数选择dataset数据集配置generate_config生成配置文件，把network、loss、dataset配置都加载config上。 然后看train.py文件 2 main主函数比较简单，参数填进去就可以了。这里需要特别注意下参数这一块。1234def main(): global args args = parse_args() train_net(args) 到最开始那里有参数，如下，我们注意先来三个参数dataset、network、loss然后有一个generate_config生成配置，吧三个参数组合到config里，这里这三个参数要和config.py里面每个参数起的名字一样哦例如dataset : emore retina 当然你也可以自己改配置文件自己加network :r100 r50 r100fc… 1234567891011121314151617181920212223def parse_args(): parser = argparse.ArgumentParser(description='Train face network') # general parser.add_argument('--dataset', default=default.dataset, help='dataset config') parser.add_argument('--network', default=default.network, help='network config') parser.add_argument('--loss', default=default.loss, help='loss config') args, rest = parser.parse_known_args() generate_config(args.network, args.dataset, args.loss) parser.add_argument('--models-root', default=default.models_root, help='root directory to save model.') parser.add_argument('--pretrained', default=default.pretrained, help='pretrained model to load') parser.add_argument('--pretrained-epoch', type=int, default=default.pretrained_epoch, help='pretrained epoch to load') parser.add_argument('--ckpt', type=int, default=default.ckpt, help='checkpoint saving option. 0: discard saving. 1: save when necessary. 2: always save') parser.add_argument('--verbose', type=int, default=default.verbose, help='do verification testing and model saving every verbose batches') parser.add_argument('--lr', type=float, default=default.lr, help='start learning rate') parser.add_argument('--lr-steps', type=str, default=default.lr_steps, help='steps of lr changing') parser.add_argument('--wd', type=float, default=default.wd, help='weight decay') parser.add_argument('--mom', type=float, default=default.mom, help='momentum') parser.add_argument('--frequent', type=int, default=default.frequent, help='') parser.add_argument('--per-batch-size', type=int, default=default.per_batch_size, help='batch size in each context') parser.add_argument('--kvstore', type=str, default=default.kvstore, help='kvstore setting') args = parser.parse_args() return args 然后主要看train.py文件 3 train前面会选择GPU，然后make 存模型的文件夹，然后读数据集bin文件 加载预训练模型，如果有预训练模型，就直接加载checkpoint模型参数到arg_params，aux_params，然后sym = get_symbol(args)走一个前向传播，如果没有模型则声明个none。参数在后面fit时有用。 1234567891011if len(args.pretrained)==0: arg_params = None aux_params = None sym = get_symbol(args) if config.net_name=='spherenet': data_shape_dict = {'data' : (args.per_batch_size,)+data_shape} spherenet.init_weights(sym, data_shape_dict, args.num_layers)else: print('loading', args.pretrained, args.pretrained_epoch) _, arg_params, aux_params = mx.model.load_checkpoint(args.pretrained, args.pretrained_epoch) sym = get_symbol(args) 这里有个get_symbol前面有定义，我们到前面能看到，写了一大堆，我们看一点这里先获取你的网络的模型net_name这一套在/recognition/symbol有resnet等等的get_symbol，这个特征提完后，加全连接层，分别有softmax等等。12345678910111213def get_symbol(args): embedding = eval(config.net_name).get_symbol() all_label = mx.symbol.Variable(&apos;softmax_label&apos;) gt_label = all_label is_softmax = True if config.loss_name==&apos;softmax&apos;: #softmax _weight = mx.symbol.Variable(&quot;fc7_weight&quot;, shape=(config.num_classes, config.emb_size), lr_mult=config.fc7_lr_mult, wd_mult=config.fc7_wd_mult, init=mx.init.Normal(0.01)) if config.fc7_no_bias: fc7 = mx.sym.FullyConnected(data=embedding, weight = _weight, no_bias = True, num_hidden=config.num_classes, name=&apos;fc7&apos;) else: _bias = mx.symbol.Variable(&apos;fc7_bias&apos;, lr_mult=2.0, wd_mult=0.0) fc7 = mx.sym.FullyConnected(data=embedding, weight = _weight, bias = _bias, num_hidden=config.num_classes, name=&apos;fc7&apos;) 回到train上来然后计算算力，这部分时flops_counter.py在/common下，有兴趣可以看一下。 1234567#计算消耗算力if config.count_flops: all_layers = sym.get_internals() _sym = all_layers['fc1_output'] FLOPs = flops_counter.count_flops(_sym, data=(1,3,image_size[0],image_size[1])) _str = flops_counter.flops_str(FLOPs) print('Network FLOPs: %s'%_str) 后面创建模型contex默认选择CPU，给ctxGPU序号symbol给出计算的输出即可。symbol: the network definitioncontext: the device (or a list of devices) to use for executiondata_names : the list of input data variable nameslabel_names : the list of input label variable names 1234model = mx.mod.Module( context = ctx, symbol = sym,) 主要看model.fit 1234567891011121314model.fit(train_dataiter, begin_epoch = begin_epoch, num_epoch = 999999, eval_data = val_dataiter, eval_metric = eval_metrics, kvstore = args.kvstore, optimizer = opt, #optimizer_params = optimizer_params, initializer = initializer, arg_params = arg_params, aux_params = aux_params, allow_missing = True, batch_end_callback = _batch_callback, epoch_end_callback = epoch_cb ) 参数解释如下train_data (DataIter) – Train DataIter.eval_data (DataIter) – 验证集数据，每个epoch会验证一下，可以选择noneepoch_end_callback (function or list of functions) – 每一次epoch调用一次batch_end_callback (function or list of function) – 每个batch调用一次，可以存放一些打印函数或者我们更新学习率kvstore (str or KVStore) – Defaults to ‘local’.optimizer (str or Optimizer) – Defaults to ‘sgd’.优化器initializer (Initializer) – The initializer is called to initialize the module parameters when they are not already initialized.arg_params (dict) – 初始化参数，默认没有aux_params (dict) – aux参数同上allow_missing (bool) – 在有预训练参数时是否允许丢失，也就是如果允许，则上面的参数万一有丢的，我们就随机初始化。begin_epoch (int) – 开始的次数，从几开始计数num_epoch (int) – 训练epoch 然后看到model每个epoch调用了回调函数，做了点啥呢主要时当数据量达到一定就打印数据，当测试效果好的时候就保存模型。 4运行1CUDA_VISIBLE_DEVICES=&apos;0,1,2,3&apos; python -u train.py --network r50 --loss arcface --dataset emore","link":"/posts/insightface%E4%B8%ADrecognition%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/"},{"title":"insightface数据制作全过程记录","text":"insightface数据制作全过程记录测试insightface时发现需要调整数据，insigtface/datasets属于存放数据的目录，insightface/src/data中存放了数据处理的一些代码，包括rec文件生成。 1数据对齐我们用lfw数据做实验，你也可以自己找数据。 lfw数据http://vis-www.cs.umass.edu/lfw/我下载的是这个原图像https://drive.google.com/file/d/1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp/view?usp=sharing datasets/lfwdata下存放数据lfwdata下分目录存放每一类的数据，每个人一个文件夹，里面存图。在src/align里提供了人脸对齐的代码，检测加对齐1python3 align_lfw.py --input-dir ../../datasets/lfwdata --output-dir ../../datasets/lfw2注意：这里cfp-fp数据集的对齐和lfw不一样，原因在于目录层级不一样，cfp-fp数据目录下每个person还分正脸侧脸，所以在遍历时不太一致，我们又写了align_cfp.py文件。这里我们复制一份align_lfw.py到align_cfp.py。然后改main函数里1、数据路径遍历方式变为ytf，这个你去看face_image有定义12dataset = face_image.get_dataset('ytf', args.input_dir)print('dataset size', 'lfw', len(dataset))2、113行改一下生成新的路径存储图片这里123img = img[:,:,0:3]_paths = fimage.image_path.split('/')a,b,c = _paths[-3], _paths[-2],_paths[-1] 2生成list文件这里insightface提供的face2rec2不能生成list，所以我们找了个程序稍后上传，用im2rec.py，这个程序也是生成rec的，我们加上参数—list 就生成lst文件 —recursive代表遍历文件下所有目录。../../datasets/lfw/lfw输出目录最后lfw代表list的名字，不用加后缀。../../datasets/lfw2图片目录1python3 im2rec.py --list --recursive ../../datasets/lfw/train ../../datasets/lfw2 3生成rec &amp;idx 文件（依托于list）生成rec文件，把—list去掉../../datasets/lfw/train.lst代表lst的目录../../datasets/lfw2原图存在的目录1python3 im2rec.py ../../datasets/lfw/train.lst ../../datasets/lfw2 4创建property配置文件直接创建一个名为property的文件，没有后缀写1000,112,112代表ID数量,尺寸,尺寸 目前datasets/lfw/目录下存在lfw.lst lfw.rec lfw.idx 5 创建pair文件为了做测试我们需要生成验证集用的bin文件，bin文件生成前需要做pair文件，就是一对一对的数据，每一行分别是图A的目录 空格 图B的目录 空格 标志0/1（代表两张图类别一致否）利用generate_image_pairs.py（源文件有问题，已修改）稍后上传，附录有../../datasets/lfw2对齐图像目录../../datasets/lfw/train.txt存放txt100要多少个数据 1python3 generate_image_pairs.py --data-dir ../../datasets/lfw2 --outputtxt ../../datasets/lfw/train.txt --num-samepairs 100 注意：这里生成pairs的方法不太好，数据集给了一些标准的pairs文件，我们可以写一个脚本取解读，具体如下： lfw在insightface里面有pair.txtcfp没有，只有一组FP对，需要我们自己写个脚本，这里我写好了上传来。我们依然放到src/data下cfp_make_bin.pycfp给了这个目录有一组对 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import mxnet as mxfrom mxnet import ndarray as ndimport argparseimport pickleimport sysimport osparser = argparse.ArgumentParser(description=&apos;Package LFW images&apos;)# generalparser.add_argument(&apos;--data-dir&apos;, default=&apos;&apos;, help=&apos;&apos;)parser.add_argument(&apos;--image-size&apos;, type=str, default=&apos;112,112&apos;, help=&apos;&apos;)parser.add_argument(&apos;--output&apos;, default=&apos;&apos;, help=&apos;path to save.&apos;)parser.add_argument(&apos;--num-samepairs&apos;,default=100)args = parser.parse_args()data_dir = args.data_dirimage_size = [int(x) for x in args.image_size.split(&apos;,&apos;)]pairs_end = []def get_paths(): pairs = [] prefix = os.path.join(data_dir,&apos;Protocol/&apos;) #prefix = &quot;/Split/&quot; prefix_F = os.path.join(prefix, &quot;Pair_list_F.txt&quot;) pairs_F = [] prefix_P = os.path.join(prefix,&quot;Pair_list_P.txt&quot;) pairs_P = [] pairs_end = [] issame_list = [] #读pairlist文件 with open(prefix_F, &apos;r&apos;) as f: for line in f.readlines()[0:]: pair = line.strip().split() pairs_F.append(pair[1]) print(len(pairs_F)) with open(prefix_P, &apos;r&apos;) as f: for line in f.readlines()[0:]: pair = line.strip().split() pairs_P.append(pair[1]) print(len(pairs_P)) #读pair文件 prefix = os.path.join(data_dir,&quot;Protocol/Split/FP&quot;) folders_1 = os.listdir(prefix) for folder in folders_1: sublist = [] same_list = [] pairtxt = os.listdir(os.path.join(prefix, folder)) for pair in pairtxt: img_root_path = os.path.join(prefix, folder, pair) with open(img_root_path, &apos;r&apos;) as f: for line in f.readlines()[0:]: #print(line) pair1 = line.strip().split(&apos;,&apos;) #print(pair) pairs_end += (os.path.join(data_dir,&apos;Protocol/&apos;,pairs_F[int(pair1[0])-1]),os.path.join(data_dir,&apos;Protocol/&apos;,pairs_P[int(pair1[1])-1])) #print(pair) if pair == &apos;same.txt&apos;: #print(&apos;ok!&apos;) issame_list.append(True) else: issame_list.append(False) return pairs_end,issame_list 6 生成验证集bin文件完事后利用/src/data/下的 lfw2pack.py生成bin文件但是有点问题，需要修改下，参考这篇博客https://blog.csdn.net/hanjiangxue_wei/article/details/86566348修改lfw2pack.py中19行，打#的为更改的，改为两个参数，一个是txt读出来的列表，另一个是总数量。注意：cfp跳过就可以了1234567891011121314151617181920212223242526272829303132333435363738import mxnet as mxfrom mxnet import ndarray as ndimport argparseimport pickleimport sysimport ossys.path.append(os.path.join(os.path.dirname(__file__), &apos;..&apos;, &apos;eval&apos;))import lfwparser = argparse.ArgumentParser(description=&apos;Package LFW images&apos;)# generalparser.add_argument(&apos;--data-dir&apos;, default=&apos;&apos;, help=&apos;&apos;)parser.add_argument(&apos;--image-size&apos;, type=str, default=&apos;112,112&apos;, help=&apos;&apos;)parser.add_argument(&apos;--output&apos;, default=&apos;&apos;, help=&apos;path to save.&apos;)parser.add_argument(&apos;--num-samepairs&apos;,default=100)args = parser.parse_args()lfw_dir = args.data_dirimage_size = [int(x) for x in args.image_size.split(&apos;,&apos;)]lfw_pairs = lfw.read_pairs(os.path.join(lfw_dir, &apos;train.txt&apos;))print(lfw_pairs)lfw_paths, issame_list = lfw.get_paths(lfw_pairs,int(args.num_samepairs)+1)#, &apos;jpg&apos;)lfw_bins = []#lfw_data = nd.empty((len(lfw_paths), 3, image_size[0], image_size[1]))print(len(issame_list))i = 0for path in lfw_paths: with open(path, &apos;rb&apos;) as fin: _bin = fin.read() lfw_bins.append(_bin) #img = mx.image.imdecode(_bin) #img = nd.transpose(img, axes=(2, 0, 1)) #lfw_data[i][:] = img i+=1 if i%1000==0: print(&apos;loading lfw&apos;, i)with open(args.output, &apos;wb&apos;) as f: pickle.dump((lfw_bins, issame_list), f, protocol=pickle.HIGHEST_PROTOCOL)对应的get_paths这个文件存在src\\eval\\lfw.py下，把他也改了1234567891011121314151617181920212223def get_paths(pairs, same_pairs): nrof_skipped_pairs = 0 path_list = [] issame_list = [] cnt = 1 for pair in pairs: path0 = pair[0] path1 = pair[1] if cnt &lt; same_pairs: issame = True else: issame = False if os.path.exists(path0) and os.path.exists(path1): # Only add the pair if both paths exist path_list += (path0,path1) issame_list.append(issame) else: print(&apos;not exists&apos;, path0, path1) nrof_skipped_pairs += 1 cnt += 1 if nrof_skipped_pairs&gt;0: print(&apos;Skipped %d image pairs&apos; % nrof_skipped_pairs) return path_list, issame_list之后再运行1python3 lfw2pack.py --data-dir ../../datasets/lfw --output ../../datasets/lfw/lfw.bin --num-samepairs 300 附录generate_image_pairs.py、 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# coding:utf-8import sysimport osimport randomimport timeimport itertoolsimport pdbimport argparse#src = '../../datasets/lfw2'#dst = open('../../datasets/lfw/train.txt', 'a')parser = argparse.ArgumentParser(description='generate image pairs')# generalparser.add_argument('--data-dir', default='', help='')parser.add_argument('--outputtxt', default='', help='path to save.')parser.add_argument('--num-samepairs',default=100)args = parser.parse_args()cnt = 0same_list = []diff_list = []list1 = []list2 = []folders_1 = os.listdir(args.data_dir)dst = open(args.outputtxt, 'a')count = 0dst.writelines('\\n')# 产生相同的图像对for folder in folders_1: sublist = [] same_list = [] imgs = os.listdir(os.path.join(args.data_dir, folder)) for img in imgs: img_root_path = os.path.join(args.data_dir, folder, img) sublist.append(img_root_path) list1.append(img_root_path) for item in itertools.combinations(sublist, 2): for name in item: same_list.append(name) if len(same_list) &gt; 10 and len(same_list) &lt; 13: for j in range(0, len(same_list), 2): if count &lt; int(args.num_samepairs):#数量可以修改 dst.writelines(same_list[j] + ' ' + same_list[j+1]+ ' ' + '1' + '\\n') count += 1 if count &gt;= int(args.num_samepairs): breaklist2 = list1.copy()# 产生不同的图像对diff = 0print(count)# 如果不同的图像对远远小于相同的图像对，则继续重复产生，直到两者相差很小while True: random.seed(time.time() * 100000 % 10000) random.shuffle(list2) for p in range(0, len(list2) - 1, 2): if list2[p] != list2[p + 1]: dst.writelines(list2[p] + ' ' + list2[p + 1] + ' ' + '0'+ '\\n') diff += 1 if diff &gt;= count: break #print(diff) if diff &lt; count: #print('--') continue else: break 有问题请留言，最近一两周在做这个可以更新帖子及问题 Next：insightface测试验证集效果全过程https://blog.csdn.net/CLOUD_J/article/details/98882718","link":"/posts/insightface%E6%95%B0%E6%8D%AE%E5%88%B6%E4%BD%9C%E5%85%A8%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/"},{"title":"insightface测试recognition验证集效果全过程","text":"insightface测试recognition验证集效果全过程 本过程在insightface代码下作实验，源代码参考https://github.com/deepinsight/insightface#pretrained-models实验测试recognition下的eval验证 1数据准备数据准备参考博文：insightface数据制作全过程记录https://blog.csdn.net/CLOUD_J/article/details/98769515 2eval验证在/recognition/eval下verification.py文件 1python3 verification.py --data-dir ../../datasets/lfw --model ../../models/model-r50-am-lfw/model,0 --nfolds 10 运行时出现了点问题，主要在于我们之前准备数据集时没有打乱数据导致正样本挤在一起，会使这一部分数据负样本为0，我们计算正确率会用负样本识别正确比上总负样样本数，分母出现0，所以做了更改calculate_val_far函数 1234567891011121314151617def calculate_val_far(threshold, dist, actual_issame): predict_issame = np.less(dist, threshold) true_accept = np.sum(np.logical_and(predict_issame, actual_issame)) false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame))) n_same = np.sum(actual_issame) n_diff = np.sum(np.logical_not(actual_issame)) #print(true_accept, false_accept) #print(n_same, n_diff) if n_same == 0: val = 1 else: val = float(true_accept) / float(n_same) if n_diff == 0: far = 0 else: far = float(false_accept) / float(n_diff) return val, far 3代码解析3.1主函数主函数首先做了模型载入，数据载入bin文件，然后对载入的模型分别做测试，检测各个模型数据效果。核心在这里，遍历ver_list不同数据集，遍历nets不同模型12345678910111213141516if args.mode==0: for i in range(len(ver_list)): results = [] for model in nets: acc1, std1, acc2, std2, xnorm, embeddings_list = test(ver_list[i], model, args.batch_size, args.nfolds) print('[%s]XNorm: %f' % (ver_name_list[i], xnorm)) print('[%s]Accuracy: %1.5f+-%1.5f' % (ver_name_list[i], acc1, std1)) print('[%s]Accuracy-Flip: %1.5f+-%1.5f' % (ver_name_list[i], acc2, std2)) results.append(acc2) print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results))) elif args.mode==1: model = nets[0] test_badcase(ver_list[0], model, args.batch_size, args.target) else: model = nets[0] dumpR(ver_list[0], model, args.batch_size, args.target)然后再提示几点，1--model', default='../../models/model-r50-am-lfw/model,50该参数代表模型路径的名字加上训练的epoch，../../models/model-r50-am-lfw是路径，然后model是名字；后面的50是epoch就是你可能在训练时会把多个epoch的结果输出，你可能验证不同epoch当时模型参数的效果。 3.2 test函数测试函数首先前向传播得到输出特征，然后计算它的范数，之后计算他的准确率 前向传播主要在这！这里对你的多个数据集遍历，当然你要是只有一个数据集就一次楼。data数据集，ba和bb随便起的名字，然后就这样不断取batchsize进行前向传播model.forward(db, is_train=False)然后将输出存到embeddings这个东西里，最后将多个数据集都存到embeddings_list1234567891011121314151617181920212223242526for i in range( len(data_list) ): data = data_list[i] embeddings = None ba = 0 while ba&lt;data.shape[0]: bb = min(ba+batch_size, data.shape[0]) count = bb-ba _data = nd.slice_axis(data, axis=0, begin=bb-batch_size, end=bb) #print(_data.shape, _label.shape) time0 = datetime.datetime.now() if data_extra is None: db = mx.io.DataBatch(data=(_data,), label=(_label,)) else: db = mx.io.DataBatch(data=(_data,_data_extra), label=(_label,)) model.forward(db, is_train=False) net_out = model.get_outputs()#获取输出 _embeddings = net_out[0].asnumpy() time_now = datetime.datetime.now() diff = time_now - time0 time_consumed+=diff.total_seconds() #print(_embeddings.shape) if embeddings is None:#第一次的话先创建一个列表 embeddings = np.zeros( (data.shape[0], _embeddings.shape[1]) ) embeddings[ba:bb,:] = _embeddings[(batch_size-count):,:]#补进去 ba = bb embeddings_list.append(embeddings) 第二步，做了个范数计算计算一下特征的总平均范数12345678910_xnorm = 0.0_xnorm_cnt = 0for embed in embeddings_list: for i in range(embed.shape[0]): _em = embed[i] _norm=np.linalg.norm(_em) #print(_em.shape, _norm) _xnorm+=_norm _xnorm_cnt+=1_xnorm /= _xnorm_cnt第三步 计算准确率这里传入特征列表和标签列表还有nrof_folds，啥意思，这个是做K折检测的，分K份检测。12_, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=nfolds)acc2, std2 = np.mean(accuracy), np.std(accuracy)这里有一点注意，文中有个 embeddings = embeddings_list[0] + embeddings_list[1]我理解把两个数据集组合验证。 3.3evaluate评估函数首先先将数据集分了两块，就是原来是这样A1 A2 A3 A4 B1 B2这样A1和A2对比同类1改成这样A1 A3 B1奇数放一起A2 A4 B2偶数放一起 python中a::b代表从a开始以b单位增长 这里还搞了个thresholds作为阈值，会在评估函数里遍历寻找最好的阈值。完事做了两个评估calculate_roccalculate_val 1234567891011def evaluate(embeddings, actual_issame, nrof_folds=10, pca = 0): # Calculate evaluation metrics thresholds = np.arange(0, 4, 0.01) embeddings1 = embeddings[0::2] embeddings2 = embeddings[1::2] tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), nrof_folds=nrof_folds, pca = pca) thresholds = np.arange(0, 4, 0.001) val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds) return tpr, fpr, accuracy, val, val_std, far 3.4 calculate_roc第一步 先生命一个K折数据类1、这里assert是断言的意思，就是说后面那句话不对就直接报错；2、LFold在前面有声明类，就是调用kfold这个包3、12345678910assert(embeddings1.shape[0] == embeddings2.shape[0]) assert(embeddings1.shape[1] == embeddings2.shape[1]) nrof_pairs = min(len(actual_issame), embeddings1.shape[0]) nrof_thresholds = len(thresholds) k_fold = LFold(n_splits=nrof_folds, shuffle=False) tprs = np.zeros((nrof_folds,nrof_thresholds)) fprs = np.zeros((nrof_folds,nrof_thresholds)) accuracy = np.zeros((nrof_folds)) indices = np.arange(nrof_pairs) 第二步，求了下范数距离欧式距离 123if pca==0: diff = np.subtract(embeddings1, embeddings2)#做减法 dist = np.sum(np.square(diff),1)#求平方和 第三步 遍历thresholds寻找最好的阈值。k_fold.split(indices)是分数据函数，用训练集取找好的阈值，用测试机打分。tprs暂时没用，关注accuracy准确率12345678910111213141516171819202122232425262728for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)): #print('train_set', train_set) #print('test_set', test_set) if pca&gt;0: print('doing pca on', fold_idx) embed1_train = embeddings1[train_set] embed2_train = embeddings2[train_set] _embed_train = np.concatenate( (embed1_train, embed2_train), axis=0 ) #print(_embed_train.shape) pca_model = PCA(n_components=pca) pca_model.fit(_embed_train) embed1 = pca_model.transform(embeddings1) embed2 = pca_model.transform(embeddings2) embed1 = sklearn.preprocessing.normalize(embed1) embed2 = sklearn.preprocessing.normalize(embed2) #print(embed1.shape, embed2.shape) diff = np.subtract(embed1, embed2) dist = np.sum(np.square(diff),1) # Find the best threshold for the fold acc_train = np.zeros((nrof_thresholds)) for threshold_idx, threshold in enumerate(thresholds):#遍历找最好阈值 _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set]) best_threshold_index = np.argmax(acc_train) #print('threshold', thresholds[best_threshold_index]) for threshold_idx, threshold in enumerate(thresholds): tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set]) _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set]) 3.5 准确率计算核心函数在这，我们的改动也在这里。 说几个注意 np.less求最小值，求每个值与阈值相比，如果比阈值小则真。np.logical_and代表逻辑与的意思，就是两个numpy进行与，把预测和真实进行与一下得到tp，就是预测正确的正样本truepositive。np.logical_not(actual_issame)代表取反，给真是样本取反，fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))这样代表预测和真实样本取反的与就是错误预测的正样本。tn，fn一样。 1234567891011def calculate_accuracy(threshold, dist, actual_issame): predict_issame = np.less(dist, threshold) tp = np.sum(np.logical_and(predict_issame, actual_issame)) fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame))) tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame))) fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame)) tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn) fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn) acc = float(tp+tn)/dist.size return tpr, fpr, acc 最后结果如下Accuracy没有，我们只有一个数据集，这里我理解的是Acuuracy是单个数据集准确率，Accuracy-Flip和其他数据集混在一起，这里就看0.99675即可。 前面阈值打印出来是1.39 4结果 LFW CFP-FP renet-r50 99.63%(99.80%) 92.66%(92.74%) renet-r100 99.81%(99.77%) 95.94%(98.27%) 注意：括号内为github作者的结果，括号前为我的结果。结果取batchsize=16 目前对于差别有些疑问，还等待发现，如有大神能指点，还请指导。 5问题1、内存问题考虑减小batchsize1python3 verification.py --data-dir ../../datasets/lfw2/ --model ../../models/model-r100-ii/model,0 --nfolds 10 --batch-size 16","link":"/posts/insightface%E6%B5%8B%E8%AF%95recognition%E9%AA%8C%E8%AF%81%E9%9B%86%E6%95%88%E6%9E%9C%E5%85%A8%E8%BF%87%E7%A8%8B/"},{"title":"labelme多目标标注问题","text":"在labelme标注多目标时存在不匹配的问题，任何一张图片都是从1开始，不能自定义标签号，考虑改写json2datasets文件，使他根据标签名称去定义标签号。 labelme多目标标注问题 问题：在labelme标注多目标时存在不匹配的问题，任何一张图片都是从1开始，不能自定义标签号，考虑改写json2datasets文件，使他根据标签名称去定义标签号。 解决：我的目录层级是 /datasets_new/img1/img1_fuse.jpg/datasets_new/img1/img1_fuse.json/datasets_new/img2/img2_fuse.jpg/datasets_new/img2/img2_fuse.json… 执行脚本就把目录下所有json转换完毕，大家可以根据不同的目录方式，可以自行修改代码，123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import osimport randomimport shutilimport reimport argparseimport jsonimport osimport os.path as ospimport warnings import PIL.Imageimport yaml from labelme import utilsimport base64 def main(): warnings.warn(\"This script is aimed to demonstrate how to convert the\\n\" \"JSON file to a single image dataset, and not to handle\\n\" \"multiple JSON files to generate a real-use dataset.\") parser = argparse.ArgumentParser() parser.add_argument('data_path') parser.add_argument('-o', '--out', default=None) args = parser.parse_args() #对不同标签定义一个号，每个号有自己的颜色...从1开始排就可以 label_name_to_value = {'_background_': 0, 'change':1, 'disappear':2, 'add':3} count = [] for i in os.listdir(args.data_path): path1 = os.path.join(args.data_path,i) for j in os.listdir(path1): path2 = os.path.join(path1, j) if path2.split('.')[-1] != 'json': continue else: count.append(path2) for i in range(0, len(count)): path = count[i] if os.path.isfile(path): data = json.load(open(path)) if data['imageData']: imageData = data['imageData'] else: imagePath = os.path.join(os.path.dirname(path), data['imagePath']) with open(imagePath, 'rb') as f: imageData = f.read() imageData = base64.b64encode(imageData).decode('utf-8') img = utils.img_b64_to_arr(imageData) for shape in data['shapes']: label_name = shape['label'] if label_name in label_name_to_value: label_value = label_name_to_value[label_name] else: label_value = len(label_name_to_value) label_name_to_value[label_name] = label_value # label_values must be dense label_values, label_names = [], [] for ln, lv in sorted(label_name_to_value.items(), key=lambda x: x[1]): label_values.append(lv) label_names.append(ln) assert label_values == list(range(len(label_values))) lbl = utils.shapes_to_label(img.shape, data['shapes'], label_name_to_value) captions = ['{}: {}'.format(lv, ln) for ln, lv in label_name_to_value.items()] lbl_viz = utils.draw_label(lbl, img, captions) out_dir = osp.basename(count[i]).replace('.', '_') out_dir = osp.join(osp.dirname(count[i]), out_dir) if not osp.exists(out_dir): os.mkdir(out_dir) print(out_dir) name=out_dir.split('/') rename=name[2]+'_'+name[1] PIL.Image.fromarray(img).save(osp.join(out_dir, rename+'_'+'img.png')) #PIL.Image.fromarray(lbl).save(osp.join(out_dir, rename+'label.png')) utils.lblsave(osp.join(out_dir,rename+'_'+'label.png'), lbl) PIL.Image.fromarray(lbl_viz).save(osp.join(out_dir,rename+'_'+'label_viz.png')) with open(osp.join(out_dir, 'label_names.txt'), 'w') as f: for lbl_name in label_names: f.write(lbl_name + '\\n') warnings.warn('info.yaml is being replaced by label_names.txt') info = dict(label_names=label_names) with open(osp.join(out_dir, 'info.yaml'), 'w') as f: yaml.safe_dump(info, f, default_flow_style=False) print('Saved to: %s' % out_dir)if __name__ == '__main__': main() 执行完毕后，目录变为 /datasets_new/img1/img1_fuse.jpg/datasets_new/img1/img1_fuse.json/datasets_new/img1/img1_fuse_json/….(生成的东西)/datasets_new/img2/img2_fuse.json 想把它拷贝到上一级目录，可以执行脚本处理1234567891011121314import osimport shutilfor index in os.listdir('./dataset_new'): print(index) #path = os.path.join('./dataset_test',index,index+'_mask_jpg') path = os.path.join('./dataset_new', index,index+'_fuse_json', index+'_dataset_new_label.png') print(path) #if os.path.isdir(path): # os.rmdir(path) if (os.path.exists(path)): #os.remove(os.path.join('./dataset_new', index, index+'_fuse.png')) shutil.copyfile(path, os.path.join('./dataset_new', index, index + '_label.png')) #os.rename(path,os.path.join('./dataset_new', index, index+'_fuse.json'))最后变为 /datasets_new/img1/img1_fuse.jpg/datasets_new/img1/img1_fuse.json/datasets_new/img1/img1_label.png","link":"/posts/labelme%E5%A4%9A%E7%9B%AE%E6%A0%87%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98/"},{"title":"linux服务器下一些操作","text":"本文总结了最近使用linux服务器用到的一些基本操作 1 链接服务器1ssh menglingjun@hpcgpu9.ai.lycc.qihoo.net 输入密码… 2 创建软连接12345readlink -f ./insightface/产生软连接/data/menglingjun/insightface复制一下，然后到Home下ln -s /data/menglingjun/insightface . 3 查内存情况1df -h 4 查看文件详情123ll /~ home目录pwd显示当前工作目录绝对路径 5 查看GPU情况1nvidia-smi 6 安装PIP包到个人用户下1pip3 install packget --user 71pip --default-timeout=100 install -U packget 8 查看本机IP12ifconfigls -lht查看当前文件夹下文件信息 9 windows把文件传到linux下1234567scp -rp menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/datasets/lfw2/Micky_Arison/Micky_Arison_0001.jpg d:/ascp /d:/1.txt menglingjun@10.160.167.27:/home/menglingjun/ascp -r /d:/a menglingjun@10.160.167.27:/home/menglingjun/a#拷贝整个目录scp /d:/im2rec.py menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/src/data/scp menglingjun@10.173.226.203 :/home/menglingjun/a/d:/1.txt /d:/ascp /d:/generate_image_pairs.py menglingjun@10.173.226.203:/home/menglingjun/insightface/insightface-master/src/data/ 10 删除文件夹1rm -rf //","link":"/posts/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/"},{"title":"linux软连接","text":"linux中有个操作叫做软连接，类似于windows中的快捷方式，对于文件，可以相当于同一个文件用一个内存存储；对于文件夹，内存在A处，软链到B处，我们在B处操作，所有的空间内存都存储到A处； 创建软链接1ln -s [源文件或目录] [目标文件或目录]例如，在当前目录创建test1，软链到var路径下的test，这样内存就占用test的内存。（一般用于节省home的内存，将home下的数据软链到其他硬盘）1ln –s /var/www/test test1 删除软连接1rm –rf test修改软连接1ln –snf [新的源文件或目录] [目标文件或目录]","link":"/posts/linux%E8%BD%AF%E8%BF%9E%E6%8E%A5/"},{"title":"mediapipe bazel build 编译提示编译找不到opencv4/opencv2/core/version.hpp","text":"解决mediapipe bazel build 编译提示编译找不到opencv4/opencv2/core/version.hpp 问题：mediapipe bazel build 编译提示编译找不到opencv4/opencv2/core/version.hpp分析：主要原因在于没有配置好bazel的第三方库的环境变量解决方案：1、需要修改根目录下WORKSAPCE的linux_opencv仓库的path，改到你安装的opencv环境位置，例如我安装opencv4需要修改如下12345new_local_repository( name = &quot;linux_opencv&quot;, build_file = &quot;@//third_party:opencv_linux.BUILD&quot;, path = &quot;/home/myname/software/opencv&quot;,)2、需要修改third_party/opencv_linux.BUILD找到opencv123456789101112131415161718cc_library( name = &quot;opencv&quot;, srcs = glob( [ &quot;lib/libopencv_core.so&quot;, &quot;lib/libopencv_highgui.so&quot;, &quot;lib/libopencv_imgcodecs.so&quot;, &quot;lib/libopencv_imgproc.so&quot;, &quot;lib/libopencv_video.so&quot;, &quot;lib/libopencv_videoio.so&quot;, ], ), hdrs = glob([&quot;include/opencv4/opencv2/*.h*&quot;， &quot;include/opencv4/opencv2/**/*.h*&quot; ， ]), includes = [&quot;include/opencv4/&quot;], linkstatic = 1, visibility = [&quot;//visibility:public&quot;],)","link":"/posts/mediapipe%20bazel%20build%20%E7%BC%96%E8%AF%91%E6%8F%90%E7%A4%BA%E7%BC%96%E8%AF%91%E6%89%BE%E4%B8%8D%E5%88%B0opencv4_opencv2_core_version.hpp/"},{"title":"pip换镜像安装 清华镜像 豆瓣镜像","text":"pip换镜像安装 清华镜像 豆瓣镜像 豆瓣镜像源https://pypi.douban.com/simple/清华镜像源https://pypi.tuna.tsinghua.edu.cn/simple示例代码安装keras（注意：—user代表安装在个人目录下，不影响root目录）1pip3 install keras -i https://pypi.douban.com/simple/ --user","link":"/posts/pip%E6%8D%A2%E9%95%9C%E5%83%8F%E5%AE%89%E8%A3%85%20%E6%B8%85%E5%8D%8E%E9%95%9C%E5%83%8F%20%E8%B1%86%E7%93%A3%E9%95%9C%E5%83%8F/"},{"title":"python + web操作 爬虫 自动填写表单","text":"最近想做一个自动填表单提交的程序，用到了webdriver这个东西，做了简答的总结。 Pre install package安装webdriver库(chorme版本)：找到你的版本，下载好放到Google/Chrome/Application文件夹下http://npm.taobao.org/mirrors/chromedriver/配置环境变量C:\\Users\\menglingjun\\AppData\\Local\\Google\\Chrome\\Application把这个加到环境变量，可能你那不一样，就是把chromedriver在的文件夹路径。 安装python selenium库1pip3 install selenium 基本操作导入web库1from selenium import webdriver创建driver对象并读取某网页12driver= webdriver.Chrome()driver.get('http://www.baidu.com')获取网页上某元素并改内容网页上有个审查元素，可以右键看审查元素，这个大家都会。google浏览器中叫做“检查” 点击下图中那个按钮，就可以进入选取状态，此时我们选取左侧某个内容，右侧就会对应到他那一行代码，我们可以看到它的id，name等信息。根据这些id、name找到它的位置。这里可以by_id也可以by_name等等，然后send_key更改信息，代码如下：12name = driver.find_element_by_id(\"id_username\")name.send_keys(\"data_operation\") 按钮类操作获取按钮，然后点击。12login_button = driver.find_element_by_class_name(\"submit-row\")login_button.click() 总结通过这些操作，我们可以实现网页表单自动填写。","link":"/posts/python%20+%20web%E6%93%8D%E4%BD%9C%20%E7%88%AC%E8%99%AB%20%E8%87%AA%E5%8A%A8%E5%A1%AB%E5%86%99%E8%A1%A8%E5%8D%95/"},{"title":"python 参数模块argparse使用","text":"python 参数模块argparse使用最近在用，做下总结！ argparse是python的一个命令行解析包，用于编写可读性非常好的程序 1创建参数导入包；创建参数12import argparseparser = argparse.ArgumentParser(\"name\") 2添加参数添加参数，包括参数名data_dir表示为—data-dirdefault默认值help注意：所有参数均为字符串型1234parser.add_argument('--data-dir', default='', help='')parser.add_argument('--image-size', type=str, default='112,112', help='')parser.add_argument('--output', default='', help='path to save.')parser.add_argument('--num-samepairs',default=100) 3程序中用参数创建对象，然后获取它的值12args = parser.parse_args()lfw_dir = args.data_dir 4命令行如何用1python3 verification.py --data-dir ../lfw --nfolds 10 --target lfw 5参数分组设置123456789101112add_argument_group()参数分组设置。当有分组命令的需求时可用，输入参数将归于所属分组下.parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='Create an image list or \\ make a record database by reading from an image list') parser.add_argument('prefix', help='prefix of input/output lst and rec files.') cgroup = parser.add_argument_group('Options for creating image lists') cgroup.add_argument('--list', action='store_true', help='') 6参数action有的参数带有action表示在命令行时直接加上这个参数，不赋值就直接按照action行动。例如：下面的行动是store_true，设置list为真，这样1cgroup.add_argument('--list', action='store_true',help=' ')这样程序中可以12if args.list: make_list(args)运行时直接加—list代表设置为真1python3 im2rec.py --list --recursive ../../datasets/lfw/lfw ../../datasets/lfw2","link":"/posts/python%20%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9D%97argparse%E4%BD%BF%E7%94%A8/"},{"title":"python 文件操作OS总结","text":"本文总结了关于python文件系统的操作。 python 文件操作OS总结最近在用，坐下总结！ os的一些操作 os.listdir(path)列出该文件夹下面的目录，一般这么用123folders_1 = os.listdir(prefix)for folder in folders_1: ...#遍历这个目录 os.makedirs(path)创建该路径1os.makedirs(path, mode=0o777) os.path() 模块文件路径模块1、路径拆分1os.path.split(path) 把路径分割成 dirname 和 basename，返回一个元组2、路径合并1os.path.join(path1, path2,...) 把目录和文件名合成一个路径3、查看路径是否存在1os.path.exists(path) 路径存在则返回True,路径损坏返回False一般这么用12if not os.path.exists(out_dir): os.makedirs(out_dir) open()打开文件1open(name[, mode[, buffering]])模式如下例如： 123456789with open(img_root_path, 'r') as f: for line in f.readlines()[0:]: pair = line.strip().split(',') pairs_end += pairs_F[int(pair[0])],pairs_N[int(pair[1])] if pair == 'same': issame_list.append('True') else: issame_list.append('False')return pairs_end,issame_list 创建文件对象，又会衍生出他的方法 file 对象方法file.read([size])：size 未指定则返回整个文件，如果文件大小 &gt;2 倍内存则有问题，f.read()读到文件尾时返回””(空字串)。file.readline()：返回一行。这个在上面那个例子提到，我们一行一行读取。file.readlines([size]) ：返回包含size行的列表, size 未指定则返回全部行。for line in f: print line ：通过迭代器访问。f.write(“hello\\n”)：如果要写入字符串以外的数据,先将他转换为字符串。f.tell()：返回一个整数,表示当前文件指针的位置(就是到文件头的比特数)。f.seek(偏移量,[起始位置])：用来移动文件指针。偏移量: 单位为比特，可正可负起始位置: 0 - 文件头, 默认值; 1 - 当前位置; 2 - 文件尾f.close() 关闭文件","link":"/posts/python%20%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9COS%E6%80%BB%E7%BB%93/"},{"title":"pytorch安装linux下","text":"总结装机必要包 pytorch安装linux12345678# CUDA 10.0pip install torch==1.2.0 torchvision==0.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple# CUDA 9.2pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -i https://pypi.tuna.tsinghua.edu.cn/simple# CPU onlypip install torch==1.2.0+cpu torchvision==0.4.0+cpu -i https://pypi.tuna.tsinghua.edu.cn/simple","link":"/posts/pytorch%E5%AE%89%E8%A3%85linux/"},{"title":"screen功能 linux后台运行程序","text":"screen功能 linux后台运行程序 1创建screen进程1screen #进入screen环境 2进程下运行文件1python a.py#运行a.py 3分离窗口先ctrl + a 进入命令模式，再按d分离窗口 4列出当前进程12345screen -ls #列出当前进程There are screens on: 22013.pts-37.hpcgpu30 (Detached) 9915.pts-33.hpcgpu30 (Detached) 5再次进入窗口1screen -r 22013 6杀死某个窗口杀死22013窗口1kill -9 22013","link":"/posts/screen%E5%8A%9F%E8%83%BD%20linux%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/"},{"title":"tensorflow加载预训练模型数据","text":"tensorflow加载预训练模型数据 1、自己写图或者修改后的图加载参数Tips 1自己重写了一遍图，加载别人训过的参数，要注意图的变量名字要一致！Tips 2自己写一个图，部分图的变量用预训练的参数。 123456789101112131415# 1去除某些参数exclude = ['MobilenetV1/Logits/Conv2d_1c_1x1/biases','global_step']variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=exclude)saver = tf.train.Saver(variables_to_restore)# 2只加载当前搭建的图中部分变量variables_to_restore = [val for val in tf.global_variables() if 'Adam' not in val.name and 'power' not in val.name and 'global' not in val.name and 'Logits/' not in val.name]saver = tf.train.Saver(variables_to_restore)#加载参数with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 装载参数 saver.restore(sess, ckpt_path) # 此时改为所有变量以便于之后保存... variables_to_restore = [val for val in tf.global_variables()] saver = tf.train.Saver(variables_to_restore)","link":"/posts/tensorflow%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE/"},{"title":"tensorflow自建数据集dataset使用","text":"tensorflow自建数据集dataset使用 1 Datasettensorflow中提供了两个dataset的API，一个是做一个数据源，另一个是做一个管道用来不断提取数据。 tf.data.Dataset：表示一串元素（elements），其中每个元素包含了一或多个Tensor对象。例如：在一个图片pipeline中，一个元素可以是单个训练样本，它们带有一个表示图片数据的tensors和一个label组成的pair。有两种不同的方式创建一个dataset：创建一个source (例如：Dataset.from_tensor_slices())， 从一或多个tf.Tensor对象中构建一个dataset应用一个transformation（例如：Dataset.batch()），从一或多个tf.data.Dataset对象上构建一个datasettf.data.Iterator：它提供了主要的方式来从一个dataset中抽取元素。通过Iterator.get_next() 返回的该操作会yields出Datasets中的下一个元素，作为输入pipeline和模型间的接口使用。最简单的iterator是一个“one-shot iterator”，它与一个指定的Dataset相关联，通过它来进行迭代。对于更复杂的使用，Iterator.initializer操作可以使用不同的datasets重新初始化（reinitialize）和参数化（parameterize）一个iterator ，例如，在同一个程序中通过training data和validation data迭代多次。 2、tf.data.Dataset一般我们可以从tensor序列直接导入到Dataset中，如下几个例子，直接是tensor 12345678910111213dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))print(dataset1.output_types) # ==&gt; \"tf.float32\"print(dataset1.output_shapes) # ==&gt; \"(10,)\"dataset2 = tf.data.Dataset.from_tensor_slices( (tf.random_uniform([4]), tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))print(dataset2.output_types) # ==&gt; \"(tf.float32, tf.int32)\"print(dataset2.output_shapes) # ==&gt; \"((), (100,))\"dataset3 = tf.data.Dataset.zip((dataset1, dataset2))print(dataset3.output_types) # ==&gt; (tf.float32, (tf.float32, tf.int32))print(dataset3.output_shapes) # ==&gt; \"(10, ((), (100,)))\" 在做图像这方面时，我们可以把图像的path和标签导入，再进行批量处理，dataset有个map函数，对所有的数据执行同一函数，这样我们可以再读取图片，解码图片，resize等等。 1234567def load_and_preprocess_from_path_label(path, label): image = tf.read_file(path) # 读取图片 image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize_image_with_crop_or_pad(image, args.img_height, args.img_width) # 原始图片大小为(266, 320, 3)，重设为(192, 192) # image /= 255.0 # 归一化到[0,1]范围 return image, labeldataset = dataset.map(load_and_preprocess_from_path_label) 3、创建iterator创建完Dataset API，我们可以利用iterator访问数据，有四种iterator：one-shotinitializablereinitializablefeedable 3.1 make_one_shot_iterator()one-shot iterator是最简单的iterator，它只支持在一个dataset上迭代一次的操作，不需要显式初始化。举个例子，我8个数据，就能遍历八次，我搞十次，当第9次就会报错。123456789import tensorflow as tfdata = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]dataset = tf.data.Dataset.from_tensor_slices(data)dataset = dataset.shuffle(8).batch(4).repeat()##不断重复，这样就可以遍历完数据继续遍历。it = dataset.make_one_shot_iterator()next_val = it.get_next()with tf.Session() as sess: for i in range(10): print(sess.run(next_val))结果：因为有17个数据，遍历完一次，最后一次不够batch，就打出一个数，我在训练数据时，加了一个shape判断，shape等于batchsize，再feed，不然就再找下一个iterrator即可。 3.2 make_initializable_iterator()initializable需要显式初始化，他可以对数据加上个参数，feed时候可以给参数。 4、举例123456789101112131415161718192021222324252627282930313233343536373839404142# read datasetsdata_path = pathlib.Path(args.traindata_dir)all_image_paths = list(data_path.glob('*.jpg'))all_image_paths = [str(path) for path in all_image_paths] # 所有图片路径的列表## 读取csv文件all_image_labels = []with open(args.label_path, 'r') as f: reader = csv.reader(f) first = True for row in reader: if first == True: first = False else: all_image_labels.append(row[1])## 打乱数据random_index = []for i in range(len(all_image_paths)): random_index.append(i)random.shuffle(random_index) # 打散temp = []temp2 = []for i in range(len(random_index)): temp.append(all_image_paths[random_index[i]]) temp2.append(all_image_labels[random_index[i]])all_image_paths = tempall_image_labels = temp2## 创建datasetdataset = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))def load_and_preprocess_from_path_label(path, label): image = tf.read_file(path) # 读取图片 image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize_image_with_crop_or_pad(image, args.img_height, args.img_width) # 原始图片大小为(266, 320, 3)，重设为(192, 192) # image /= 255.0 # 归一化到[0,1]范围 return image, labeldataset = dataset.map(load_and_preprocess_from_path_label)dataset = dataset.shuffle(2 * args.train_batch_size).batch(args.train_batch_size).repeat()iterator = dataset.make_initializable_iterator()img_next = iterator.get_next() 训练时1234567sess.run(iterator.initializer)print('Start training...')for step in range(args.train_steps): train_batch_data, train_batch_labels = sess.run(img_next) if train_batch_data.shape[0] != args.train_batch_size: train_batch_data, train_batch_labels = sess.run(img_next) start_time = time.time()","link":"/posts/tensorflow%E8%87%AA%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86dataset%E4%BD%BF%E7%94%A8/"},{"title":"Xml文件读取与编写","text":"Xml文件读取与编写 Xml文件读取与编写xml文件一般如下所示，&lt;&gt;为node；有可能还有属性Attribute，一般网页中会有，我们只是存储信息，就没有涉及，例如；123456789101112131415161718192021&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; &lt;filename&gt;test_1.jpg&lt;/filename&gt; &lt;size&gt; &lt;width&gt;1920&lt;/width&gt; &lt;height&gt;2560&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;object&gt; &lt;name&gt;object&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;457&lt;/xmin&gt; &lt;ymin&gt;436&lt;/ymin&gt; &lt;xmax&gt;574&lt;/xmax&gt; &lt;ymax&gt;526&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; xml文件读取标配代码如下，加载包，加载文件：1234from xml.dom.minidom import parse #加载包domTree = parse(&quot;./customer.xml&quot;)# 文档根元素rootNode = domTree.documentElement1、按照节点名称查找12# 通过名称object获取属性值list1 = rootNode.getElementsByTagName(&quot;object&quot;)此时得到是一个node list2、子节点查找1child = list1[0].childNodes 注意：1、object子节点为name、…、bndbox等。2、这里也可以继续搜按照名字搜list1[0].getElementsByTagName(“xmin”)3、我们要想打印节点内的text需要对节点继续找子节点list1[0].getElementsByTagName(“xmin”).childNodes[0].value xml文件写入1、创建一个节点，设置属性123# 新建一个customer节点customer_node = domTree.createElement(&quot;customer&quot;)customer_node.setAttribute(&quot;ID&quot;, &quot;C003&quot;)2、子节点12345# 创建name节点,并设置textValuename_node = domTree.createElement(&quot;name&quot;)name_text_value = domTree.createTextNode(&quot;kavin&quot;)name_node.appendChild(name_text_value) # 把文本节点挂到name_node节点customer_node.appendChild(name_node)得到如下：123&lt;customer ID=&quot;C003&quot;&gt; &lt;name&gt;kavin&lt;/name&gt; &lt;/customer&gt;","link":"/posts/xml%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E7%BC%96%E5%86%99/"},{"title":"The Summary Of Face Recognition","text":"通俗易懂教你如何做人脸识别 The Summary Of Face Recognition[全文共计7162字，预计阅读15分钟，消耗脑容量20MB] 人脸识别包含人脸检测、人脸对齐、人脸识别三个步骤。首先利用人脸检测(FaceDetiction)识别图像中所有人脸，回归人脸的boundbox。然后对于识别出的人脸，可能会有歪头的，如果直接做recognition可能准确率较低，所以&gt;我们需要做一步人脸对齐(FaceAlign)，把所有人脸转为一个标准（正方形内，两眼对齐，嘴巴两侧对齐…）。最后我们需要对所有的人脸做识别，提取特征与员工特征库进行对比。 Part 1 FaceDetiction人脸检测（待总结…） Part 2 FaceRecognition 其实人脸识别可以从两个角度做，之前人们一直是当成分类任务做，现在更多的是1：1特征对比；先说分类任务，我们先对数据进行人脸检测以及对齐得到一个对齐后的人脸数据集，在训练集中每个类（标签/人）下有许多图片（都是对齐好的），然后对数据做一个分类任务，常规的分类模型有googlenet、resnet、densenet、mobilenet（移动端）等等，目标函数用softmax损失函数。后者1：1比对，把分类网络当成特征提取器提取特征，和人脸库对比，欧氏距离小于一定阈值就认为比对成功。这里注意的是人脸库里面是标准照片直接前向传播得到的embedding 特征层。 Part 2.1 InsightFace框架我们先看一下insightface的框架是怎么做的。这我自己画的一个简单的图，其实很简单，人脸识别分类代码分为两部分，一部分在特征提取网络，用成熟的分类网络提取特征，得到embedding向量，可以看为FaceImage2Vector。这个维度可以自己设计，大家都选的512（抱歉，一个特别的日子，我们一起默哀一会。）然后常规做法，我们会用全连接层转到class_num维度，这时搞一个softmax去做损失即可。后来，大佬们就觉得损失不够好开始改，这个我在2.2中去介绍一下这一系列损失函数的变化。然后接下来，我们看一下人脸识别代码怎么评测？ Part 2.2 模型评测对于普通分类问题，可能我们考察一个模型好坏，主要看测试集准确率就可以；对于人脸识别问题，我们有开集和闭集之说，闭集是指训练集与测试集人物一样，不会出现不认识的人，一般只在我们的某个领域，比如某个公司不会来外人。但实际不可能，人脸是个很大的开集，我们的训练集不可能囊括所有人，你训练一个模型，比如公司内部，万一有个别的人来了，这破机器认为他就是内部的人，因为可能模型不能很好的区分开。所以这就有了开集测试的重要性。 实际我们做人脸识别考察是在我们的库里训练一个更好的特征提取器，这个提取器拥有很好的区分能力。如果只在闭集测试，那就看准确率就行；如果在开集测试，我们就要看他在其他数据集上的区分度，怎么测？我们取样N对图像，看每一对两个图像是否属于一个类，我们搞一个阈值，两个图像的特征（由特征提取器提取）欧氏距离小于阈值就认为属于一个类，否则不属于一个类。我们看他能实现多高的准确率。 做1：1认证，我们需要准备一个pair测试文件，格式如下： 图A地址 图B地址 0/1（代表着是否为同类） 这里注意两张图片是不同图片，大概可能会有3000正例（同类），3000负例。做比对之后我们会有一个结果 TP代表正确识别为正例的样本（原来是正例） TN正确识别为负例的样本（原来是负例）FP代表错误识别为正例的样本（原来是负例） FN错误识别为负例的样本（原来是正例）TN+FP代表测试负例，也就是类间测试次数TP+FN代表测试正例，也就是类内测试次数（这里类内测试是指同属于一个类的样本测试，看他们是否足够接近，类间测试是对于不同类的测试，看他们距离是否足够大）FN为错误识别为负例的样本，也是错误接受的次数FP同理代表错误拒绝的次数 然后我们会用以下指标考查：（1）准确率：实验中直接采用(TP+TN)/SUM；考察越高越好。 TPR召回率：TP/(TP+FN)；考察越高越好 FPR假正例率：FP/(TN+FP)；考察越低越好 （2）FAR误识率：FAR = 错误接受次数/类间测试次数=FN/(TN+FP)；考察越低越好。 （3）FRR拒识率：FRR = 错误拒绝次数/类内测试次数=FP/(TP+FN)；考察越低越好。 Part 2.3 损失函数回顾 人脸识别和一般的分类任务不太一样，他更大的特点在于类目多，我们训练13亿人，要让他对地球人进行适应，并且类间差距可能也小，样本有时不均衡，数据不多。所以对于人脸识别要求有更好的泛化能力、容错能力。我们从基本的softmax一点一点看一看 Part 2.3.1 softmax损失softmax损失函数是最经典的损失函数，用于多分类问题；softmax损失作用于标签所对应类Y_i的得分，优化提高它的概率得分，所以对于闭集的分类能有一个好的结果；然而我们了解人脸识别一般是开集的，测试集是所有可能进入你的检测系统的人，有两个问题？一则来了一个外人，你是否能把他排出去？二则稍微脸上变了点，你是否能很好地分开？softmax做损失在训练集能很好的分开，但是不能很好的增加类外间距，减小类内间距。 这里补充一下什么是类内间距，类外间距？划分多个类，无非是把它映射到某一个featuremap，然后做一个超平面，把它们分开，类内间距，就是一个类自己的划分区域最大伸展距离，其实就是它的空间。类外间距就是每个类区域之间的间距。我们想要的结果是他们之间的间隔大一点，这样会有一定的容错能力，泛化能力；类内的间隔小一些，这样聚类效果更好一些。 随后大家就开始增大类外间距，减小类内间距的探索。 Part 2.3.2 Triplet LossTriplet loss属于Metric Learning, 相比起softmax, 它可以方便地训练大规模数据集，不受显存的限制。缺点是过于关注局部，导致难以训练且收敛时间长 这里提一下Metric Learning的概念，它是根据不同的任务来自主学习出针对某个特定任务的度量距离函数。通过计算两张图片之间的相似度，使得输入图片被归入到相似度大的图片类别中去。通常的目标是使同类样本之间的距离尽可能缩小，不同类样本之间的距离尽可能放大。 Triplet loss是这么干的，搞三种图片进去，图A，与图A同类的图，与图A不是同类的图，然后损失函数如下，目的就是让同类的图更近，不同类的图更远。 $$L=max(d(a,p)−d(a,n)+margin,0)$$ Part 2.3.3 L-Softmax到L-Softmax，人们就开始直接搞一个间距给损失函数，下图中第一个是softmax做十分类的二维特征图，可以看出来是分开了，但是很拥挤。如果放一张训练集之外的图可能就分错了。 这里我对这个图做一个解释，首先数据实验是将损失函数前的embedding层定为二维（X1,X2）也就对应着图里的横纵坐标；然后在正式损失之前有个全连接层，对于这个W维度应该是（2，class）这里实验是对手写数字体做实验，所以class=10，这里我们认为W=(W_1,W_2,W_3,…,W_10)代表10个子权重，其实我们去看$W\\cdot X= ||W|||X| cos\\theta = ( ||W_1|||X| cos\\theta_1,||W_2|||X| cos\\theta_2,…)$最后每一项代表该类的得分，也就是哪一项值大就更属于哪一类。此时权重是一个可调量，我们认为他是平面内十个向量，然后同样一个X看他和谁的点乘更大就更好。再者由于cos在0-180度内是单调函数，所以角度越小越好，||W||越大越好。 L-Softmax就对$\\theta$做了个间距，如下公式，对于不是本类的还是原来操作（注意这里写为$W\\cdot X=||W||||X||cos\\theta$），对于本类，我们把角度$\\theta$变成$m\\theta$，此处m代表提升的倍数，这样让他本来可能模型认为分类不错，但是我们强行把角度增大，使得效果不是很好，这会模型只能说好吧，按照这个继续去优化，去适应这个间隙。 另外为了损失函数单调，我们搞了个条件限制mt然后我们再看上面那张彩图，对于不同m，投射出来的间隙确实不一样，之间的间隙越来越大。m越大分割的类间距越大，类内间距越小，聚类的效果越好，当然这个值也不能太大，可能会不收敛。 Part 2.3.4 SphereFace( A-softmax)SphereFace是在L-Softmax的基础上将权重进行归一化，L-Softmax会同时从角度和权重长度上区分不同类别，而SphereFace只从角度上去区分不同类别（因为||W|| = 1了）这里可以用他论文里这张图，这里（a）和（b）是softmax loss的结果；（c）和（d）是限制W1和W2向量的模等于1，而且b1和b2偏置等于0，称为modified softmax loss；（e）和（f）是angular softmax loss；我们能看出softmax点比较散，类间距几乎没有，我们把W1标准化，b=0发现，产生了一定间距，进而他做了margin发现间距更大了。之前的L-softmax没有对权重归一化，使得类的结果还要考虑W，这样测间距就比较复杂，A-softmax只需要衡量角度即可。 Part 2.3.5 CosFace( LMSL: Large Margin Cosine Loss)CosFace的思想和SphereFace( A-softmax)的思想接近，其中主要做了以下三点的改进： loss的形式做了稍微的改变，将超参数m由乘法运算变成了减法运算 不仅对权重进行了正则化，还对特征进行了正则化。 对归一化后的值乘上一个scale参数，超球面过小时，分类映射到超球面上不好分类，这个scale参数可以扩大超球面体积，这是后面的损失一直在用的观点。 Part 2.3.6 ArcFaceArcFace是在CosFace基础上又改了下，前面有过对于角度乘以一个margin，这里它直接在角度上加一个margin，得到了一个很好的结果，如下所示。论文中对多个损失函数做了一个对比，如下图所示，普通softmax就是分开即可，类间距几乎没有，SpherFace在角度上乘以margin可以增加内间距，但是存在角度为0的地方会有交集，这是一个问题；ArcFace也是在角度上做的margin，他是直接加一个margin，就解决了0度角的交集；CosFace则是在角度的cos值基础上加了一个margin也能在cosi基础上做好间距。 Part 3 InsightFacePart 3.1 代码结构我做了这么一张图，大概展示代码的整个结构，Insightface源码在这：https://github.com/deepinsight/insightface#pretrained-models我们如果只关注人脸识别部分，更多的要去看src以及recognition部分代码 Part 3.2 人脸识别实验这里我们以搭建一个人脸识别方案的例子展开，首先需要准备数据，数据准备完后我们需要对数据进行人脸识别+对齐，这部分可以采用MTCNN方法，在src/align中，附上我的实际操作总结：insightface数据制作全过程记录然后对于对齐好的数据，我们可以进行用insightface提供的模型做初步验证，可以参考：insightface测试recognition验证集效果全过程测试结束，我们可以进行训练，可以参考：insightface中recognition训练过程 Part 4 人脸识别部署（待补充） 幸福是靠自己正确的！用黄晓明的一句话，我不管你怎么想，我只说我要怎样！加油！","link":"/posts/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E9%82%A3%E7%82%B9%E4%BA%8B---%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%96%B9%E6%A1%88summary/"},{"title":"候选区域寻找选择性寻找selective search","text":"目标检测时需要筛选候选区域，常见的方法有滑窗法，选择搜索法。本文简单介绍一下候选区域寻找的方法。 候选区域寻找selective search关于候选区域的选择有多种方法，最简单的方法为滑窗法，就是以一定间距不断横纵遍历得到所有的窗口，但是所有区域都遍历，而且有时候物体不一样大？这就太浪费时间了。论文章所采用的方法是选择搜索法。算法具体如下图所示step0：生成区域集R。 step1：计算区域集R里每个相邻区域的相似度S={s1,s2,…}//循环以下步骤step2：找出相似度最高的两个区域，将其合并为新集，添加进Rstep3：从S中移除所有与step2中有关的子集step4：计算新集与所有子集的相似度step5：S若为空则跳出循环，否则跳至step2 相似度计算那么关键点在于相似度怎么判断？图像之间的特征有很多，颜色？纹理特征？尺寸等？对于相似度的判断，选择性搜索考虑了颜色、纹理、尺寸和空间交叠这4个参数。1、颜色特征将色彩空间转为HSV，每个通道下以bins=25计算直方图，这样每个区域的颜色直方图有25*3=75个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：上式如何理解？简单说一下，我们归一化后每个通道的数据后，一维特征向量的和为1.0，比较两个区域的特征向量每一位，把每一位的最小值加起来。如果全都一样那么结果肯定是1啊！如果不一样，每一位都取最小值，那就肯定比1小啊！哈哈就是这么理解。 2、纹理相似度（texture similarity） 论文这里的纹理采用SIFT-Like特征，采用方差为1的高斯微分在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8310=240（使用RGB色彩空间）。计算公式如下所示： 3、尺度相似度尺度相似度其实是实现优先合并小的区域，也就是减少大区域合并小区域。4、交叠相似度（shape compatibility measure）上述完成后，要看一下两个区域是否爱挨着，如果距离十万八千里，那就没有合并的必要。如何计算？先找一个外接矩形把它框起来，如果两个矩形相距较远，则外接矩形就很大，所以我们设计这样的指标，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形BBij）减去两个矩形的尺寸，得到相对差距。其计算方式：候选区域排序相似度计算后还有一个问题，如何对候选区域排序呢，通过上述的步骤我们能够得到很多很多的区域，但是显然不是每个区域作为目标的可能性都是相同的，因此我们需要衡量这个可能性，这样就可以根据我们的需要筛选区域建议个数啦。 这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给他们乘以一个随机数，毕竟3分看运气嘛，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。这样我就得到了所有区域的目标分数，也就可以根据自己的需要选择需要多少个区域了。 参考文献[1]选择性搜索（selective search）https://blog.csdn.net/guoyunfei20/article/details/78723646[2]第三十三节，目标检测之选择性搜索-Selective Searchhttps://www.cnblogs.com/zyly/p/9259392.html","link":"/posts/%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%E5%AF%BB%E6%89%BE%E9%80%89%E6%8B%A9%E6%80%A7%E5%AF%BB%E6%89%BEselective%20search/"},{"title":"图像分割FCN全卷积神经网络","text":"FCN全卷积神经网络是图像分割的基础网络FCN实现了端到端的网络，输入mxm图像，输出为mxmxc的图像，其中c代表种类。 1思想概述FCN叫做全卷积神经网络，顾名思义所有曾都是卷积层！也就是抛掉了全连接层，这是第一个改变；再者卷积神经网络卷到最后特征图尺寸越来越少，分辨率比较小，不适合我们做图像分割，好，这里引入一个上采样的做法，卷积完之后再上采样到大尺寸图；网络又考虑层数不断叠加后原图的信息丢失的比较多，那么我们这里引入一个跳层结构，把前面的第四层、第三层特征引过来叠加一下。有种resnet思想。 2FCN网络图我们再看一下下面这个图网络的主题，从左到右卷积、池化一顿操作猛如虎（蓝色卷积，绿色池化），到了最右边16*16*4096，全连接层丢了，先给他弄到21通道再上采样到34*34*21然后合并featuremap4后再上采样，最后经过一系列得到最后500*500*21的特征图。之后怎么做？很多人没有说好，我们平时做分类是得到全连接的1*21这样对他进行softmax，21类，每类有个分数存在1*21的向量中，好我们这500*500*21是像素级别的分类，总共有500*500个像素，每个像素属于21个类别的分别得分是啥，这样用softmax计算损失。所以我们看那些标注图是每个像素有个颜色代表它所属于的类别。端到端！伟大！ 端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。 那么接下来就要解决细节问题，上采样怎么实现？怎么合并数据？ 3上采样对于低分辨率的特征图，我们常常采用上采样的方式将它还原高分辨率，这里陈述三种方法。 3.1双线性插值上采样首先线形插值是什么？线形插值就是知道两个点的值，连一条直线，来确定中间的点的值，具体怎么做，我们找(x1,y1)、(x2,y2)连成一条直线，[x1,x2]中的点就可以用线上的点表示。双线性插值是一个三维的坐标系，我们找到四个点来确定中心点坐标，如下图为网上找的一个例子。 这种方法计算简单，无需训练。 3.2反卷积上采样3.2.1 怎样上采样普通的卷积操作，会使得分辨率降低，如下图3*3的卷积核去卷积4*4得到2*2的输出。上采样的过程也是卷积，那么怎么会得到分辨率提高呢？之前我们看卷积时有个保持输出与输入同分辨率的方法就是周围补0，嗯嗯。那么你是否灵机一动，那要是让分辨率提高呢？是不是再多补一些0，对的看看图2。 图1 常规卷积操作图2 上采样操作（四周补0） 其实呢作者在这又换了个方法，你想一下，只在四周补0会导致最边上的信息不太好，那我们把这个信息平均下，在每个像素与像素之间补0呢，看图3就是这么操作的。 图3 反卷积插零 3.2.2 反卷积补多少零？然后我们看一下参数的计算，先看一下四周补0的情况。下图给出了发卷积的参数，我们要保持转置卷积的输出核卷积的输入相同也就是i=o’，转置卷积就是反卷积。这里主要算出p为多少就是补多少个0，看我图5给出了计算。图4 四周补零反卷积图5 四周补0计算 再看一下反卷积间隙补0的计算如图6所示。其中插零输入是先往里面插入i‘-1个0。再计算p’ 图6 间隙补0反卷积图7 间隙补零反卷积计算 3.3反池化上采样反池化可以用下图来理解，再池化时需要记录下池化的位置，反池化时把池化的位置直接还原，其他位置填0。 3.4 小结三种方法各有优缺，双线性插值方法实现简单，无需训练；反卷积上采样需要训练，但能更好的还原特征图； 4跳层结构Skip-Layer跳层结构是啥呢，就是把第四层的featuremap考虑进来和最后一层得到的featuremap上采样合并信息再进行上采样。这个结构叫做跳层。那么实际你可以再结合第三层featuremap这样上次样倍数就需要提高，考虑的源信息就越多。 5再看FCN网络之后我们再看FCN这个网络，如果你只考虑最后一层信息，进行上采样，得到下图，这是在ALEXnet上做了修改，把最后的全连接层去掉改为卷积操作，最后卷出来16*16*21的featuremap进行了一个步长为32的双线性插值上采样得到500*500*21的图，可想而知这家伙肯定差太多了，中间这么多点都是插值插出来的！继续，改一下，这里把那个上采样改为2倍的反卷积采样，上采样到34*34*21，然后考虑第四层featuremap对他进行1*1*21卷积得到34*34*21的图，之后再合并两个信息，直接对应元素相加，得到34*34*21然后进行步长16的上采样。在这之后考虑还不行就在考虑第三个特征层加进去！ 6实验结果实验分别对FCN32、16、8的结构做了实验，可以看出考虑浅层的信息越多，实验效果越好！ 7小结FCN给我们提供了一个上采样的思路来解决图像分辨率低的问题，以及跳层结构来考虑浅层网络的特征来考虑多图像信息。 参考文献[1]FCN中反卷积、上采样、双线性插值之间的关系https://blog.csdn.net/u011771047/article/details/72872742/","link":"/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2FCN%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"图像分类CNN经典网络（一）AlexNet与VGG","text":"本篇文章总结了CNN领域用于目标检测的两个网络AlexNet与VGGNet。 在图像分类领域，CNN有了很好的应用，下图展示了2012年之后的一些经典的网络架构。本篇文章总结了CNN领域用于图像分类的两个网络AlexNet与VGGNet。 AlexNetAlexNet是打开卷积神经网络大门的第一个作品，它重新将卷积神经网络带入计算机视觉的科研中。他给出了卷积神经网络在目标检测中的基本思想。主要特点有：1、ReLU、双GPU运算：提高训练速度。（应用于所有卷积层和全连接层）2、重叠pool池化层：提高精度，不容易产生过度拟合。（应用在第一层，第二层，第五层后面）3、局部响应归一化层(LRN)：提高精度。（应用在第一层和第二层后面）4、Dropout：减少过度拟合。（应用在前两个全连接层） VGGNetVGGNet由牛津大学计算机视觉组合和Google DeepMind公司研究员一起研发的深度卷积神经网络。它探索了卷积神经网络的深度和其性能之间的关系，通过反复的堆叠33的小型卷积核和22的最大池化层，成功的构建了16~19层深的卷积神经网络。VGGNet获得了ILSVRC 2014年比赛的亚军和定位项目的冠军，在top5上的错误率为7.5%。 VGGNet的网络结构如下图所示。VGGNet包含很多级别的网络，深度从11层到19层不等，比较常用的是VGGNet-16和VGGNet-19。VGGNet把网络分成了5段，每段都把多个3*3的卷积网络串联在一起，每段卷积后面接一个最大池化层，最后面是3个全连接层和一个softmax层。 我们以VGG16做一个分析下图给出了VGG16的一个内存占用和参数量的分析。 VGG特点总结如下： 1、至今，VGGNet仍然是现如今搭建基本的CNN网络的基本网络，很多网络构建初期都是在VGG的基础上搭建。2、网络在创新上提出了用小的卷积核代替大卷积的思想。用两个3*3的卷积核去替代5*5的卷积核，用三个3*3的卷积核代替7*7的卷积核。如何理解？可以看下图原来一个55的区域要用一个核卷积，如今先用个33的核来卷积得到第二层的图，图纸考虑是步长为1的情况，此时最底层移动三次可以把5*5的区域遍历到，然后得到第二层3*3的图。然后再用33的核对第二层卷积得到顶层一个。这样就达到用用两个3\\3的卷积核去替代5*5的卷积核。很巧妙！！使用尺寸小的卷积的好处？（1）更少的参数量；（2）更多的非线性变换，使得CNN对特征的学习能力更强表达能力增强；（3）隐式的正则化效果（收敛速度要快）。 参考文献[1]AlexNet详细解读https://blog.csdn.net/qq_24695385/article/details/80368618[2]VGGNet介绍https://blog.csdn.net/u013181595/article/details/80974210[3]卷积神经网络的网络结构——VGGNethttps://www.imooc.com/article/34700[4]大话CNN经典模型：https://my.oschina.net/u/876354/blog/1634322","link":"/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89AlexNet%E4%B8%8EVGG/"},{"title":"图像分类CCNN经典网络（三）ResNet","text":"众所周知，网络的层数越低，网络的性能会越来越好。恰面我们看到了经典的四个网络架构，层数最多的也就22层。是不是可以造出更深的网络来呢？为此很多人继续去做实验，人们发现网络性能没有提高反而降低了，考虑其原因可能是梯度爆炸或者梯度消失等，为此有人提出了残差网络的思想。 在前面两篇文章总结了经典的CNN四个模型，其网络层次如下所示：众所周知，网络的层数越低，网络的性能会越来越好。恰面我们看到了经典的四个网络架构，层数最多的也就22层。是不是可以造出更深的网络来呢？为此很多人继续去做实验，人们发现网络性能没有提高反而降低了，考虑其原因可能是梯度爆炸或者梯度消失等，为此有人提出了残差网络的思想。 ResNet残差网络那么我们作这样一个假设：假设现有一个比较浅的网络（Shallow Net）已达到了饱和的准确率，这时在它后面再加上几个恒等映射层（Identity mapping，也即y=x，输出等于输入），这样就增加了网络的深度，并且起码误差不会增加，也即更深的网络不应该带来训练集上误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想，便是著名深度残差网络ResNet的灵感来源。 ResNet的主要思想在于残差的思想，如下图所示：我总结了两方面理解这个思想，emmm不知道对不对，欢迎指正。（1）我们先看这个结构，如果说浅层网络已经训练的比较好了，深层网络要做的就是保持这个效果，保持效果即使要用一个恒等映射，也就是我们的残差网络H=F+X，这个等式如果我们让F=0，则H=X就能恒等映射浅层网络的东西了，所以残差网络训练到后期就会训练F=0这个过程。 （2）那么对于一个数据集，我们不太清楚训练多少层比较好，但是我们知道一个浅层的网络达到比较好的笑过后，再往后加网络层数不知道效果如何，但是如果加一个恒等映射的话应该是保持浅层的效果的！get到。（3）好，那么我们就构造一个深层次的带残差的网络当训练到一定时间后，浅层的网络会优化的很好，深层的网络会逐渐地调节使F趋于0.（4）残差对于收敛速度的影响X是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。 好了，之后又对残差网络做了优化，两层的残差网络参数量太大，下图右侧为三层的残差网络，其先降维，然后再卷积，再升维。左图参数量3x3x256x64x2（？？对不对）右图1x1x256x64+3x3x64x64+1x1x64x256=69632的参数量。参数量大大减小！ ResNeXtResNeXt是再残差网络上做了一定的修改，它是借鉴了Lenet的思想，将卷积网络按照通道拆分开，如下图所示。左图为ResNet，右图为RexNext。它考虑左边是直接串联完成了所有的步骤先降维得到64通道再用33卷积核卷积再升维度。右边是分为并联的几个小块，分别降维到4个通道，然后卷积33，这时参量才334432个相比于3364*64小多了emmm，案后再升维度，再组合到一起。也是一个很好的思想。 如下展示了对ResNet与ResNeXt的对比，其在没有提高参数量的同时提高了预测的正确率。 参考文献[1]大话深度残差网络（DRN）ResNet网络原https://blog.csdn.net/rogerchen1983/article/details/79353972[2]浅析深度ResNet有效的原理https://blog.csdn.net/u014296502/article/details/80438616","link":"/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89ResNet/"},{"title":"图像分类CNN经典网络（二）GoogLeNet","text":"在GoogLeNet之前，人们一直想着去创造更深层的网络去改善CNN模型，但是这样会导致参数量不断增大也不一定有一个很好的效果；随着网络层数增加可能会导致梯度消失现象。 GoogLeNet在GoogLeNet之前，人们一直想着去创造更深层的网络去改善CNN模型，但是这样会导致参数量不断增大也不一定有一个很好的效果；随着网络层数增加可能会导致梯度消失现象。 *小贴士：梯度消失是什么样？在我们卷积网络中层数不断增加时可能会导致梯度消失的情况，比如如果我们选区sigmoid激活函数，其导数的最大值为0.25，如果层数增大，可能会导致随着后向传播，浅层的网络梯度不断减小，对于参数的更改也就变动十分小，甚至没有。当然产生梯度消失的原因不只这一个，这个是由于激活函数的硬伤导致的，可以换用relu、leakrelu、elu等激活函数。也可以加入Batchnorm。* V1版本1、Inception结构GoogLeNet提出了一种Inception结构来解决这个问题。下图为google团队提出的早起的inception基础结构，该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积池化后的尺寸是一样的，然后把其通道叠在一起即可），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。(不需要我们去实验选择用什么卷积核，让网络自己去优化抉择哪一个卷积层好。) 然而这个Inception原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，这也就形成了Inception v1的网络结构，如下图所示： *小贴士：1x1的卷积层作用是什么？1x1的卷积层有降低维度的作用，比如上一层网络的输出为100x100x100，如果我们直接用5x5x50去卷积，参数量为100x5x5x50=125000。如果我们先用1x1x50的卷积降维得到100x100x50再进行5x5x50卷积，参数量为100x1x1x50+50x5x5x50=67500，减少了两倍。所以这样加入一个1x1的卷积层能够在降低维度的情况下又能使用5x5的卷积层。* 思考：为什么VGG通常作为最基础的网络架构，Inception这么好不能作为基础？答：主要在于Inception训练好它会更加的适用于某一个场景数据，当换了场景可能需要修改许多参数；而VGG做出的网络更具有普遍性。 2、GAP全局平均池化第二个问题是如何处理全连接层权重过大，这里googLeNet提出了全局平均池化的方法，就是把得到的最后特征，每一层求平均后用一个点代替原来一层的特征，这样就减少了大量的全连接层参数。比如特征为7*7*1024此时要得到1*4096的特征，那就得用7*7*1024*4096参数，如果先全局平均池化的话即先得到1*1*1024的特征，再用1*1*1024*4096即可。 3、辅助分类器当网络层数过长时会存在 梯度消失的现象，googLeNet引入了两个辅助分类器，它感觉可能在某一层会出现梯度消失时，在这里加了一个分类器，最后的分类结果会有一个权重考虑，最终分类器全重大0.5，两个辅助分类器03。这样也类似于一个模型融合提高了系统的泛化能力。也给系统增加了额外的梯度。最终V1版本的googLeNet模型结构图GoogLeNet网络结构明细表解析如下：0、输入原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。1、第一层（卷积层）使用7x7的卷积核（滑动步长2，padding为3），64通道，输出为112x112x64，卷积后进行ReLU操作经过3x3的max pooling（步长为2），输出为((112 - 3+1)/2)+1=56，即56x56x64，再进行ReLU操作2、第二层（卷积层）使用3x3的卷积核（滑动步长为1，padding为1），192通道，输出为56x56x192，卷积后进行ReLU操作经过3x3的max pooling（步长为2），输出为((56 - 3+1)/2)+1=28，即28x28x192，再进行ReLU操作3a、第三层（Inception 3a层）分为四个分支，采用不同尺度的卷积核来进行处理（1）64个1x1的卷积核，然后RuLU，输出28x28x64（2）96个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x96，然后进行ReLU计算，再进行128个3x3的卷积（padding为1），输出28x28x128（3）16个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x16，进行ReLU计算后，再进行32个5x5的卷积（padding为2），输出28x28x32（4）pool层，使用3x3的核（padding为1），输出28x28x192，然后进行32个1x1的卷积，输出28x28x32。将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x2563b、第三层（Inception 3b层）（1）128个1x1的卷积核，然后RuLU，输出28x28x128（2）128个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x128，进行ReLU，再进行192个3x3的卷积（padding为1），输出28x28x192（3）32个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x32，进行ReLU计算后，再进行96个5x5的卷积（padding为2），输出28x28x96（4）pool层，使用3x3的核（padding为1），输出28x28x256，然后进行64个1x1的卷积，输出28x28x64。将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480 第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与3a、3b类似，在此就不再重复。 V2版本V2版本做了一些小的改动BatchNormalization 以往的神经网络，我们会对数据的输入做归一化处理，降低数据之间的差异性，来提高网络的收敛速度。BatchNormalization提出普通的网络的隐含层的输出也可以加入标准化，这样也能提高网络的收敛速度，我参考了文献中的几篇博客对BN说明如下： 在我们训练时，隐含层可能训练到后期，输出的数据可能偏向于某一侧，导致sigmoid激活函数梯度减小甚至消失，导致其收敛速度大大减小，这时我们用一个均值方差归一化做处理是数据集中到正态分布中间来，便能提高收敛的速度。但是这样是不是有了一个问题？？所有数据都归一到中间，中间部位其实是趋于一个线形函数的，那么就导致非线性的操作消失了，模型的表达能力变差。这是我们不想要的！BN的一个精髓就出来了，他引入一个偏移，让数据便宜一下到非线性区域。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。 整理后BN算法大致思想如下图所示，求取上一级输出的平均值与方差，对所有数据做归一化处理，但是这样搞完后，数据都偏移了，所以这里还要加入一个尺度变换纠正偏移。 V3版本V3版本相比之前增加了卷积拆解的思想。具体怎么说呢？ 大卷积拆解为小卷积在实际我们设计网络时，用大的卷积核会有比较好的视野，每一个点所综合考虑的因素多一些，但是这样的话，参数量就会提升。为此V2提出了一种将nn的卷积核拆解为1n后再n1卷积的方法，实验证明这个方法在卷积核复杂的时候比较好用（12&lt;n&lt;18）（下图为参考文献中的一个图，之前VGG中提出过用两个33代替5*5的网络。） 参考文献[1]大话CNN经典模型：GoogLeNet（从Inception v1到v4的演进）https://my.oschina.net/u/876354/blog/1637819[2]神经网络中的梯度消失https://www.cnblogs.com/mengnan/p/9480804.html[3]【深度学习】深入理解Batch Normalization批标准https://www.cnblogs.com/guoyaohua/p/8724433.html[4]【深度学习】批归一化（Batch Normalization）https://www.cnblogs.com/skyfsm/p/8453498.html[5]","link":"/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89GoogLeNet/"},{"title":"图像分类CNN经典网络（四）CNN总结","text":"前面我们分析了CNN的经典网络模型，在此对CNN设计的一些技巧做一些总结。 1、避免遇到瓶颈HxW逐渐变小C逐渐变大缓慢！2、计算量控制（卷积核控制）减小计算量我们可以改变的因素有输入特征数量（但是一般不好控制）输出的特征（与分类有关）卷积核（我们需要去改变的）A考虑小卷积表达大卷积等方法。B考虑11降维再33C并联3、感受视野要足够大这样才能捕捉更大尺寸的内容，为此考虑可以用小卷积和来表达大卷积核。这样既能降低参数，提高视野，而且还用了更多的非线性激活。 4、通道拆分并联 关键词：卷积的视野计算量、参数量网络的表现能力","link":"/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBCCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E5%9B%9B%EF%BC%89CNN%E6%80%BB%E7%BB%93/"},{"title":"多光谱知识总结","text":"多光谱知识总结多光谱知识总结1、什么是多光谱？多光谱成像技术是基于液晶可调谐滤波（LCTF)技术，可在不同波长处（420一720nm）检测荧光探针或染料在细胞或组织中的分布，可获取各种波长范围内的所有信号，通过光谱学的原理，可将每一种色原或染料分离成单独的图像，完全去除样本的自发荧光，可明显提高组织切片成像的信噪比，还可将每种信号进行单独分离、定量，为多指标检测提供可行性，使之在病理诊断中有着很好的应用价值。简单的说就是看看它在不同波段下是否比RGB显示情况下表现更清晰。2、多光谱成像图像的评价指标2.1 图像亮度信息法对多光谱图像的每个波段来讲，人眼识别灰度图像的最大限度取决于灰度图像的亮度值大小和图像的灰度分布。可以用平均灰度值衡量第i波段灰度图像的亮度，用图像的标准差来度量灰度图像的离散程度 2.2 判断分类法可以用深度学习的方法对图像做分类实验，测试其对分类的影响，或者看其提取的特征的欧式距离差别的大小。3、 一些论文应用3.1 马铃薯的最佳光谱范围论文通过对多端波长进行了实验最终给出了马铃薯的最佳光谱范围为475，558，680，715，750，800，850（单位nm）。3.2 四季豆叶片的特征波段对400～720nm波段范围的多光谱图像波段数据进行优化，选取了灰度分布离散、图像亮度信息大且与其他波段相关性小的四季豆叶片特征波段。根据各波段图像的波段指数值和可识别度，从大量波段数据中通过比较两种方法获取了四季豆叶片的特征波段，得出545、630、645、720、650 和570nm波段对四季豆叶片的分类均比较理想。","link":"/posts/%E5%A4%9A%E5%85%89%E8%B0%B1/"},{"title":"巧妙运用GoogleDrive谷歌云盘GPU免费训练","text":"巧妙运用GoogleDrive谷歌云盘GPU免费训练&gt; 谷歌云盘配置：存储：15GB（超出花钱）GPU：Tesla T4 *1你的要求：网络：外网 巧妙运用GoogleDrive谷歌云盘GPU免费训练Tips 1 登陆GoogleDrive创建Colaboratory转到网站https://drive.google.com，创建一个文件夹，你随意，在文件夹内创建一个Colaboratory notebook，右键，新建-更多-Colaboratory注意：如果找不到Colaboratory的话，点“关联更多应用”，找到Colaboratory再下载后继续上面操作 Tips 2 配置Colaboratory双击打开笔记本，到如下界面，点击修改-笔记本设置，修改为python3与GPU，保存即可。 Tips 3 授权到笔记本正文，复制以下代码运行，进行授权。123456789101112!apt-get install -y -qq software-properties-common python-software-properties module-init-tools!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null!apt-get -y install -qq google-drive-ocamlfuse fusefrom google.colab import authauth.authenticate_user()from oauth2client.client import GoogleCredentialscreds = GoogleCredentials.get_application_default()import getpass!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} &lt; /dev/null 2&gt;&amp;1 | grep URLvcode = getpass.getpass()!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}执行会出现下面界面，点击链接登陆会有一个验证码，填到这里即可。 Tips 4 跑代码挂载云盘中的文件12!mkdir -p drive!google-drive-ocamlfuse drive这样你把文件传到云盘就可以啦，剩下的就是常规指令。譬如：安装某包1!pip install -q keras运行某文件1!python 1.py执行python指令1import tensorflow as tf 参考文献[1] https://zhuanlan.zhihu.com/p/33344222","link":"/posts/%E5%B7%A7%E5%A6%99%E8%BF%90%E7%94%A8GoogleDrive%E8%B0%B7%E6%AD%8C%E4%BA%91%E7%9B%98GPU%E5%85%8D%E8%B4%B9%E8%AE%AD%E7%BB%83/"},{"title":"探讨卷积的感受视野以及sPPnet中ROI映射到featuremap","text":"最近在学习目标检测sPPnet时看到ROI映射到featuremap中的方法，文中对此叙述较少，所以就此问题差了一些资料，在知乎上发现一片理解较好的文章(文末给出了参考文献)，于此做一总结。先谈一下感受视野，在学习卷积的时候，我们比较熟悉的是上一层图映射到下一层之后的尺寸，很少谈及感受野，这其实是一个重要的概念。 卷积中的感受视野首先，我们知道上一层图的尺寸，去推测下一层的尺寸，如下公式即可，这个大家应该很熟悉output field size = ( input field size - kernel size + 2*padding ) / stride + 1(output field size 是卷积层的输出，input field size 是卷积层的输入，stride步长，padding填充像素，kernelsize卷积核尺寸)随后，我们想如果知道后一层的尺寸，是不是就可以知道前一层的尺寸答案必然是： input field size = （output field size - 1） stride - 2padding + kernel size那么此时如果我们把最后一层的尺寸定为1，向前推是不是可以知道最后一层的一个小单位格子对应到原图的尺寸，也就是感受野了。 卷积神经网络CNN中，某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field。感受野的大小是由kernel size，stride，padding , outputsize 一起决定的。 我们可以看下面这张图更好的理解下虽然原图很大，但我们考虑原图中的11*11的尺寸向后映射到map2变为77的尺寸，再向后映射到11.7=(11-5)/1+1；1=(7-7)/1+1；是不是~~之后我们反过来证明一下最后一层向map2映射7 = (1-1)*1+7map2向map1映射11 = (7-1)*1+5 了解了感受野，我们再看一下坐标变换 坐标变换在感受野上面做坐标变换，实则是对中心位置做变换，如下图map3的一个点（位置为p3）它向上映射到map2的范围内中心位置是多少呢?我们可以用下面这个公式来理解对于 Convolution/Pooling layer: $p_i = s_i \\cdot p_{i+1} +( (k_i -1)/2 - padding)$（P代表位置，k表示卷积核大小，s代表步长）对于Neuronlayer(ReLU/Sigmoid/..) : $p_i = p_{i+1}$ 在此画了一个图方便理解，下面一层的绿色点反映一个1*1的点中对应到上一层绿色点，他其实是在最左侧的基本格子向右平移p(p就是下面那层的坐标)个stride（本层的步长）所以这部分坐标就是$s_i \\cdot p_{i+1}$然后还差一个基本格子的半个坐标就用$( (k_i -1)/2 - padding)$来决定哈哈！完美！ 我们最后再看一下下面这个图给出的计算！理解一下它的计算！ 上面是感受野的映射公式，到了ROI对于上面的公式，何凯明在SPP-net中采用做了一个简化。其实就是巧妙的化简一下公式$pi = s_i \\cdot p_{i+1} +( (k_i -1)/2 - padding)$令每一层的padding都为$padding = \\lfloor k_i /2 \\rfloor\\Rightarrow$$pi = s_i \\cdot p_{i+1} +( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor)$当$k_i$ 为奇数 $( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor) = 0$ 所以 $p_i = s_i \\cdot p_{i+1}$当k_i 为偶数 $( (k_i -1)/2 - \\lfloor k_i /2 \\rfloor) = -0.5$ 所以 $p_i = s_i \\cdot p_{i+1} -0.5$而 $p_i$ 是坐标值，不可能取小数 所以基本上可以认为$p_i = s_i \\cdot p_{i+1}$。公式得到了化简：感受野中心点的坐标$p_i$只跟前一层 $p_{i+1}$ 有关。 ROI映射变换ROI映射的时候是将左上角和右小角映射到featuremap然后确定最后的区域。考虑原图上的点不可能每一个点在featuremap中都有对应，所以我们就找最近的点。例如图中左上角的点（x,y）映射到 feature map上的(x’,y’) ： 使得 (x’,y’) 在原始图上感受野（上图绿色框）的中心点 与（x,y）尽可能接近。在上面每层都填充padding/2 得到的简化公式 ： $p_i = s_i \\cdot p_{i+1}$我们考虑从featuremap传到原图得到$p_0 = S \\cdot p_{i+1}$ 其中 $(S = \\prod_{0}^{i} s_i)$(所有步长量乘积~~)呵呵简单了不少。把式子反过来就变为：$x’ = \\lfloor x/S \\rfloor ,\\;y’ = \\lfloor y/S \\rfloor$。这样就实现了ROI坐标变换，sPPnet中改了一下：$x’ = \\lfloor x/S \\rfloor +1 ,\\;y’ = \\lfloor y/S \\rfloor +1$,加了一个1，有什莫用呢？笔者认为相当于把得到的坐标向里面缩一下保证准确性。 参考文献[1]原始图片中的ROI如何映射到到feature map?https://zhuanlan.zhihu.com/p/24780433[2]目标检测CNN经典网络（二）SPPnethttps://blog.csdn.net/CLOUD_J/article/details/89893140","link":"/posts/%E6%8E%A2%E8%AE%A8%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%84%9F%E5%8F%97%E8%A7%86%E9%87%8E%E4%BB%A5%E5%8F%8AsPPnet%E4%B8%ADROI%E6%98%A0%E5%B0%84%E5%88%B0featuremap/"},{"title":"深度学习基础问题总结（一）","text":"激活函数；梯度消失梯度爆炸问题；损失函数；反向传播； 一、激活函数1.什么是激活函数？激活函数是干什么的？激活函数是在神经网络每一层卷积或者全连接后的非线性函数部分。她是干什么的？如果没有激活函数只有前面的全连接的话，相当于整个神经网络只有线性函数，每一层都是权重与输入的乘机，网络的拟合能力有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。 2.常见的激活函数记住下图就可以了 3.为啥relu用的比较多？relu的优点？1、sigmoid激活函数以及tanh激活函数容易产生梯度消失现象；relu解决了梯度消失问题。2、sigmoid计算量较大，而relu计算量较小。速度快3、relu会使得负数为0，使得网络具有一定稀疏性，提高网络表达能力，抗过拟合特性。（但是这也使得一些神经元未被激活….） 4.为啥sigmoid会有梯度显示现象？看第二部分“梯度消失” 二、梯度消失、梯度爆炸1.梯度消失的例子举个例子，对于一个含有20层隐藏层的简单神经网络来说，当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。注意：浅层是指靠近输出端；深层是靠近输入端的； 2.梯度消失与梯度爆炸梯度计算前层上的梯度的计算来自于后层上梯度的乘积（链式法则）。当层数很多时，就容易出现不稳定。如果激活函数求导后与权重相乘的积大于1，那么层数增多的时候，最终的求出的梯度更新信息将以指数形式增加，即发生梯度爆炸，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。 3.分析sigmoid梯度消失sigmoid的函数求其导数得到下图，可以看出当x为0时梯度最大为0.25，而且当x稍微偏大或者偏小会趋近于0。两个角度理解：A）一个梯度最大为0.25，当层数增加后，不断向深层传播会出现0.25*0.25…越来越小，梯度消失。B）某一层梯度可能会因为输入过大导致出现为0的时候向前传播就危险了！梯度会随着输入的变化而变化。那么为啥人家relu就好呢！！！relu其实长得非常像一个线性函数。（认真看！） 1）sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。输入大于0就是1，输入小于0就截止了。（这一点呢，后来出现leakRelu）2）它让负数输入的时候为0.1x，稍微激活一下…缓解relu的负数为0的问题，但是我考虑按照sigmoid的第一条问题的话leakrelu也有，负数为0.1，不断传播就越来越小，但是呢，他没有趋近于0的时候，所以相对于sigmoid好太多！！！！！ 4.解决梯度消失的方法1、换激活函数2、残差网络（这个后面详细说）就是下面这个图，当初就是有了它，网络的层数瓶颈打开了！！可以看出后一层有一个 +x ，梯度就是+1，这个好，梯度不会变的越来越小了。网络再深也不会消失了。3、BN批量规范化（BatchNormalization）老牛逼了，现在几乎所有的网络都加个这个！后面详细说4、LSTM网络这个循环神经网络具有记忆功能，可以缓解梯度消失。5、权重初始化注意一下比如用个高斯分布初始化；因为有一部分原因在于权重初始化值过高导致函数的值发生梯度爆炸 5.实验过程中怎么发现是否有梯度消失或者梯度爆炸？1、我们会看一下某些层的梯度变化，是否出现越来越小，是否出现爆表。2、权重是否会变为Nan？3、梯度快速变大变小。 三、损失函数1.交叉熵深度学习中一般用交叉熵这个损失函数。交叉熵损失函数刻画的是两个概率分布之间的距离。如下式，交叉熵刻画的的是通过概率分布q来表达概率分布p的困难程度，其中p为真实分布，q为预测，交叉熵越小，两个概率分布越接近。在深度学习中，对于一个多分类问题，一般用softmax将最后的全连接层啥的转为一个概率分布的情况，softmax如下式。其实就是用了个指数，然后分母是每个数据所对应各类得分的总和，分子是他的标签类所对应的分数，这样就转为一个概率了。再用上面那个交叉熵。实际 2.为啥人们选择交叉熵交叉熵的曲线如下图所示，他是一个凸函数，有一个最低点，利于loss梯度下降法优化。 3.回归问题损失函数均方差（MES, Mean square error）均方根误差RMSE 参考文献[1]深度学习中常用损失函数https://blog.csdn.net/Tianlock/article/details/88232467","link":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"深度学习基础问题总结（二）","text":"BN；linux常用指令；正则化； 一、BN批量标准化（手撕！！）1.1 独立同分布（1）去除特征之间的相关性 —&gt; 独立； （2）使得所有特征具有相同的均值和方差 —&gt; 同分布。 在训练时我们希望输入的数据能够是独立同分布的话最好，独立同分布的数据之间具有独立性，每个特征不会相互影响，都是最后结果的一个条件，这一点是好的；第二点，对于每一个特征应该具有同分布，这样训练起来使得网络训练更快！收敛更快。 1.2 ICS影响深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。 简而言之，每个神经元的输入数据不再是“独立同分布”。 其一，上层参数需要不断适应新的输入数据分布，降低学习速度。 其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。 其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。 1.3 BN来了BN的算法如下所示，某一层网络的输出会得到特征x，然后计算这一个batchsize下的均值，m代表batchsize，xi是每个数据的该特征。然后求均值，然后就标准化，到这大家都理解。后面它又来了可尺度变换和缩放，相当于又变道一个服从N(gama,beta)的分布了！这里的gama和beta都是参数，是需要训练的，相当于我们为每一层网络的输出每一个特征训练了一个分布。哈哈 1.4手撕代码主要是反向传播推到！ 反向传播求梯度： 因为： {y^{(k)} } = {\\gamma ^{(k)} }{\\widehat x^{(k)} } + {\\beta ^{(k)} }所以： { {\\partial l} \\over {\\partial { {\\widehat x}_i} } } = { {\\partial l} \\over {\\partial {y_i} } }\\gamma因为： {\\widehat x_i} = { { {x_i} - {\\mu _B} } \\over {\\sqrt {\\sigma _B^2 + \\varepsilon } } }所以： { {\\partial l} \\over {\\partial \\sigma _B^2} } = {\\sum\\limits_{i = 1}^m { { {\\partial l} \\over {\\partial { {\\widehat x}_i} } } ( {x_i} - {u_B}) { { - 1} \\over 2}(\\sigma _B^2 + \\varepsilon )} ^{ - {3 \\over 2} } }{ {\\partial l} \\over {\\partial {u_B} } } = \\sum\\limits_{ {\\rm{i = 1} } }^m { { {\\partial l} \\over {\\partial { {\\widehat x}_i} } } } { { - 1} \\over {\\sqrt {\\sigma _B^2 + \\varepsilon } } }因为： {\\mu _B} = {1 \\over m}\\sum\\limits_{i = 1}^m { {x_i} }和 \\sigma _B^2 = {1 \\over m}\\sum\\limits_{i = 1}^m {({x_i} } - {\\mu _B}{)^2}所以： { {\\partial l} \\over {\\partial {x_i} } } = { {\\partial l} \\over {\\partial { {\\widehat x}_i} } }{1 \\over {\\sqrt {\\sigma _B^2 + \\varepsilon } } } + { {\\partial l} \\over {\\partial \\sigma _B^2} } { {2({x_i} - {\\mu _B} ) } \\over m} + { {\\partial l} \\over {\\partial {u_B} } }{1 \\over m}所以： { {\\partial l} \\over {\\partial \\gamma } } = \\sum\\limits_{i = 1}^m { { {\\partial l} \\over {\\partial {y_i} } } } {\\widehat x_i}{ {\\partial l} \\over {\\partial \\beta } } = \\sum\\limits_{i = 1}^m { { {\\partial l} \\over {\\partial {y_i} } } }1234567891011121314151617181920212223def batchnorm_backward(dout, cache): &quot;&quot;&quot; Inputs: - dout: 上一层的梯度，维度(N, D)，即 dL/dy - cache: 所需的中间变量，来自于前向传播 Returns a tuple of: - dx: (N, D)维的 dL/dx - dgamma: (D,)维的dL/dgamma - dbeta: (D,)维的dL/dbeta &quot;&quot;&quot; x, gamma, beta, x_hat, sample_mean, sample_var, eps = cache N = x.shape[0] dgamma = np.sum(dout * x_hat, axis = 0) dbeta = np.sum(dout, axis = 0) dx_hat = dout * gamma dsigma = -0.5 * np.sum(dx_hat * (x - sample_mean), axis=0) * np.power(sample_var + eps, -1.5) dmu = -np.sum(dx_hat / np.sqrt(sample_var + eps), axis=0) - 2 * dsigma*np.sum(x-sample_mean, axis=0)/ N dx = dx_hat /np.sqrt(sample_var + eps) + 2.0 * dsigma * (x - sample_mean) / N + dmu / N return dx, dgamma, dbeta 二、Linux环境下常用指令（必备！！）1、查看当前进程1ps -aus2、vim操作1vim xxx.xx //编辑文件 进入为命令行界面，按“i”进入insert输入界面，可以编辑操作，写完后，按ESC退出，回到命令行界面； 如果不想保存按住“:q!”强制退出；如果想保存按住“:wq”即可； 命令行界面下搜索指令：“/word”;向下搜索；n继续向下搜索；“?word”向上搜索 n继续向上搜索 N反向搜索； nG到第n行 多窗口操作，打开一个窗口后，:sp yyy.xx打开另一个文件，两个文件一起看，先按住ctr+w+再上下按钮切换窗口； 3、显示信息12ls ccc/ -a //显示所有信息ls ccc/ -l // 显示列表信息 等同于 ll ccc/4、软连接1ln -s ccc/sss ./test //把他软连到test5、创建一个文件1touch 2.zz6、显示绝对路径1pwd7、查看内存情况1df -h8、查看GPU情况1nvidia-smi9、查看IP地址1ifconfig10、删除|移动123mv aaa bbb //移动文件mv -r aaa/ bbb/ //移动文件夹rm aaa //删除文件 11、 三、关于正则化问题1.什么是过拟合过拟合是指我们训练的网络过度拟合以满足在训练集有一个完美的效果，但是过度完美他会使得一些噪声也能拟合好，这样对于测试集不利。下图所示。其实就是欠拟合代表拟合效果过于简单，过拟合就是拟合效果过于复杂 1.2 解决过拟合的方法1、减少特征数量：其实也等同于网络简单化，网络太复杂，参数多固然拟合效果足够好，所以针对不同的数据会对应不同复杂度的网络，这样才能减少过拟合。2、正则化项：正则化也叫惩罚因子，目标函数追求越小越好，可能他会无法无天的改变模型，我们加一个惩罚控制模型的趋势，让他模型不要太复杂，所以就有了L1、L2正则化3、dropout在2012年，Hinton在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。那么具体到代码怎么干呢？在没有dropout时，代码如下图所示，就是简单的一个感知机。引入dropout后，我们需要首先生成一个概率参数，值为0/1的一个序列，让y去乘这个概率，实际就让某些输入变为0，其实就是上一层的神经元白计算了，没有向后传播。这样搞有一个固定的比例k代表我们要禁止多少比例神经元，如果禁止掉0.4，那么相当远其他神经元的权重作用就会增加，测试时我们还需要对每个神经元乘(1-k)。让他发挥0.6的作用（好好理解下） 为什么说dropout具有防止过拟合的效果：（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。（2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。（3）Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。 贴个代码，大家看看12345678910111213141516171819202122232425import numpy as np# dropout函数的实现def dropout(x, level): if level &lt; 0. or level &gt;= 1: # level是概率值，必须在0~1之间 raise Exception('Dropout level must be in interval [0, 1[.') retain_prob = 1. - level # 我们通过binomial函数，生成与x（x表示输入数据，要对其dropout）一样的维数向量。 # binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样 # 硬币正面的概率为p，n表示每个神经元试验的次数 # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 # 即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了 sample = np.random.binomial(n=1, p=retain_prob, size=x.shape) print(sample) # 【0,1,0,1。。。】有点像独热 # 0、1与x相乘，我们就可以屏蔽某些神经元，让它们的值变为0。1则不影响 x *= sample print(x) # 对余下的非0的进行扩大倍数,因为p&lt;0。0/x=0,所以0不影响 x /= retain_prob return(x)# 对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果x = np.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32)dropout(x, 0.2) 参考文献[1] 详解深度学习中的Normalization，BN/LN/WN:https://zhuanlan.zhihu.com/p/33173246[2]深度学习中Dropout原理解析[3]Batch Normalization反方向传播求导[4]Batch Normalization梯度反向传播推导","link":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"源码编译OPENCV C++版本 linux","text":"源码编译OPENCV C++版本 linux Tips 1 下载 OPENCV12git clone https://github.com/opencv/opencvgit clone https://github.com/opencv/opencv Tips 2 创建build文件夹123cd opencvmkdir buildcd build Tips 3 Cmake编译注意：一般编译过程会有两个文件比较难下载，最好提前下载下来，这里参考Error2，Error4先解决再走下面流程。 其中CMAKE_INSTALL_PREFIX代表你的安装路径-D BUILD_opencv_python3配置是否安装python版的。12345cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=../../OpenCV \\ -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \\-D BUILD_opencv_python3=OFF \\-D BUILD_opencv_python2=OFF \\OPENCV_GENERATE_PKGCONFIG=YES .. Tips 4 make安装j4代表用几个CPU加速123make -j12#long time ...make install Tips 5 环境变量1234567vim ~/.bashrcPKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/menglingjun/software/opencv/lib64/pkgconfigexport PKG_CONFIG_PATHexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/menglingjun/software/opencv/lib64/source ~/.bashrc 测试版本信息1pkg-config --modversion opencv Error1 Cmake3找不到123456789101112131415161718192021222324-D WITH_CUBLAS=ON ..-- The CXX compiler identification is GNU 5.5.0-- The C compiler identification is GNU 5.5.0-- Check for working CXX compiler: /usr/local/gcc/bin/g++-- Check for working CXX compiler: /usr/local/gcc/bin/g++ -- brokenCMake Error at /usr/share/cmake3/Modules/CMakeTestCXXCompiler.cmake:45 (message): The C++ compiler &quot;/usr/local/gcc/bin/g++&quot; is not able to compile a simple test program. It fails with the following output: Change Dir: /home/menglingjun/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp Run Build Command:&quot;/usr/bin/gmake&quot; &quot;cmTC_39633/fast&quot; /usr/bin/gmake -f CMakeFiles/cmTC_39633.dir/build.make CMakeFiles/cmTC_39633.dir/build gmake[1]: Entering directory `/data/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp&apos; /bin/sh: /usr/bin/cmake3: No such file or directory gmake[1]: *** [CMakeFiles/cmTC_39633.dir/testCXXCompiler.cxx.o] Error 127 gmake[1]: Leaving directory `/data/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp&apos; gmake: *** [cmTC_39633/fast] Error 2 CMake will not be able to correctly generate this project. 关注这里/bin/sh: /usr/bin/cmake3: No such file or directory我查看了一下/usr/bin/下有cmake，查一下cmake版本（cmake —version）发现cmake是3版本，我觉得这个cmake就是3，所以创建一个软链（副本）1ln -s cmake cmake3我这里就ok了，如果你有别的问题，百度其他地方把。 Error2 ippicv下载失败问题1234567CMake Warning at cmake/OpenCVDownload.cmake:202 (message): IPPICV: Download failed: 28;&quot;Timeout was reached&quot; For details please refer to the download log file: /home/menglingjun/menglingjun/software/opencv-master/build/CMakeDownloadLog.txt 离线下载链接：https://pan.baidu.com/s/1f6x2-S-XHKr8K0qv5inKIg&amp;shfl=sharepset提取码：s9mz复制这段内容后打开百度网盘手机App，操作更方便哦 存到一个目录下，随意 /home/menglingjun/menglingjun/software12345678910111213# 打开终端，输入 vim ~/menglingjun/software/opencv-master/3rdparty/ippicv/ippicv.cmake #就是这个文件的路径 # 将47行的 &quot;https://raw.githubusercontent.com/opencv/opencv_3rdparty/${IPPICV_COMMIT}ippicv/&quot; # 改为步骤1中手动下载的文件的本地路径(也就是将网络下载的模式改为本地文件下载的模式): &quot;file: /home/menglingjun/menglingjun/software/opencv-master&quot; #（仅供参考，根据自己的路径填写） Error3 fatal error: sys/videoio.h: No such file or directoryCmake时把这个加上1-D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.2.0/module Error4 face_landmark_model.dat下载失败问题百度云下载地址，失效请评论提醒我更新：链接：https://pan.baidu.com/s/1uhIDqIbMuIawrE6mXuSrnA&amp;shfl=sharepset提取码：re72复制这段内容后打开百度网盘手机App，操作更方便哦找到/opencv_contrib/modules/face下面的CMakeLists.txt文件并打开，如下，修改红色标注的网站，改为本地目录（你需要提前下载好存在这个目录下），如下所示。12345678910111213ocv_download( FILENAME face_landmark_model.dat HASH ${__file_hash} URL &quot;${OPENCV_FACE_ALIGNMENT_URL}&quot; &quot;$ENV{OPENCV_FACE_ALIGNMENT_URL}&quot; &quot;file:/home/menglingjun/menglingjun/software/opencv_contrib/&quot; DESTINATION_DIR &quot;${CMAKE_BINARY_DIR}/${OPENCV_TEST_DATA_INSTALL_PATH}/cv/face/&quot; ID &quot;data&quot; RELATIVE_URL STATUS res)","link":"/posts/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91OPENCV%20C++%E7%89%88%E6%9C%AC%20linux/"},{"title":"源码编译opencv 找不到cmake3","text":"源码编译opencv 找不到cmake3 123456789101112131415161718192021222324-D WITH_CUBLAS=ON ..-- The CXX compiler identification is GNU 5.5.0-- The C compiler identification is GNU 5.5.0-- Check for working CXX compiler: /usr/local/gcc/bin/g++-- Check for working CXX compiler: /usr/local/gcc/bin/g++ -- brokenCMake Error at /usr/share/cmake3/Modules/CMakeTestCXXCompiler.cmake:45 (message): The C++ compiler &quot;/usr/local/gcc/bin/g++&quot; is not able to compile a simple test program. It fails with the following output: Change Dir: /home/menglingjun/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp Run Build Command:&quot;/usr/bin/gmake&quot; &quot;cmTC_39633/fast&quot; /usr/bin/gmake -f CMakeFiles/cmTC_39633.dir/build.make CMakeFiles/cmTC_39633.dir/build gmake[1]: Entering directory `/data/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp&apos; /bin/sh: /usr/bin/cmake3: No such file or directory gmake[1]: *** [CMakeFiles/cmTC_39633.dir/testCXXCompiler.cxx.o] Error 127 gmake[1]: Leaving directory `/data/menglingjun/software/opencv-master/build/CMakeFiles/CMakeTmp&apos; gmake: *** [cmTC_39633/fast] Error 2 CMake will not be able to correctly generate this project. 关注这里/bin/sh: /usr/bin/cmake3: No such file or directory我查看了一下/usr/bin/下有cmake，查一下cmake版本（cmake —version）发现cmake是3版本，我觉得这个cmake就是3，所以创建一个软链（副本）1ln -s cmake cmake3我这里就ok了，如果你有别的问题，百度其他地方把。","link":"/posts/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91opencv%20%E6%89%BE%E4%B8%8D%E5%88%B0cmake3/"},{"title":"玉渊潭樱花之旅","text":"三月初春在北京待的第二个春天，想起去年我还在考研复试，如今已经在北京待了一年了，今年春天来得比较早，玉渊潭的樱花又开了，老司机当然要去看看了，赏花是每年不能错过的节目。主演：🧑&amp;👩地点：玉渊潭公园🌸时间：2019.3.17 线路地铁：清早乘坐2号线转1号线可以到木樨地站，下车步行800米可以到达东门，或者做地铁4号线转9号线可以到达白堆子站，下车步行600米可以到达北门。 公交：每个地方不一样哈，自己找路吧 皂片发来国人旅游就是牌照，嘿嘿，咱也不例外，我们俩说是来观景，拍照倒是消耗了许久，对这一片竹子林照了好久~~ 桃花&amp;樱花篇首先，来几张花照，今天还没有到花的旺盛季节，人比较少，还比较好拍~还有些含苞未放的花骨朵。在樱花下少不了一张合照来记录~~ 相机摆拍篇一直看网上有相机摆拍的照片，今天让我们也来玩耍一下，拍的不好，嘿嘿第一次，以后会拍好的。让我聚焦你认真的时刻嘿嘿，我也假装拍照一下~~ 蓝天篇北京的蓝天是真的美，也许是在一段时间雾霾后出现蓝天映衬的，反正是好看。 结束工作之余多出去走走~~爱上生活","link":"/posts/%E7%8E%89%E6%B8%8A%E6%BD%AD%E6%A8%B1%E8%8A%B1%E4%B9%8B%E6%97%85/"},{"title":"目标检测CNN经典网络（一）R-CNN","text":"在早期计算机视觉研究时，人们很好的将CNN应用在目标分类的问题中，但是一直不能很好的解决目标检测的问题，目标检测不仅要分析出其是什么东西，而且还要回归出其位置。这是其在CNN训练的痛点。为此出现了R-CNN网络。 R-CNN思路R-CNN简单理解就是先训练一个CNN分类器，然后再对每幅图片挑选2000个预选区域，让这个预选区域经过R-CNN前向传播得到特征，特征先保留到硬盘中，此时与我们的标签进行对比，重叠率高的为正样本，重叠率低的为负样本，随后单独进行SVM训练。R-CNN的整体思路如下：（1）给定一张输入图片，从图片中提取 2000 个类别独立的候选区域。（selective search）（2）对于每个区域利用CNN网络抽取一个固定长度的特征向量。（这个CNN是提前训练好的一个网络）（3）再对每个区域所对应的特征向量利用 SVM 进行目标分类。 候选区域寻找selective search关于候选区域的选择有多种方法，最简单的方法为滑窗法，就是以一定间距不断横纵遍历得到所有的窗口，但是所有区域都遍历，而且有时候物体不一样大？这就太浪费时间了。论文章所采用的方法是选择搜索法。算法具体如下图所示step0：生成区域集R。 step1：计算区域集R里每个相邻区域的相似度S={s1,s2,…}//循环以下步骤step2：找出相似度最高的两个区域，将其合并为新集，添加进Rstep3：从S中移除所有与step2中有关的子集step4：计算新集与所有子集的相似度step5：S若为空则跳出循环，否则跳至step2 相似度计算那么关键点在于相似度怎么判断？图像之间的特征有很多，颜色？纹理特征？尺寸等？对于相似度的判断，选择性搜索考虑了颜色、纹理、尺寸和空间交叠这4个参数。1、颜色特征将色彩空间转为HSV，每个通道下以bins=25计算直方图，这样每个区域的颜色直方图有25*3=75个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：上式如何理解？简单说一下，我们归一化后每个通道的数据后，一维特征向量的和为1.0，比较两个区域的特征向量每一位，把每一位的最小值加起来。如果全都一样那么结果肯定是1啊！如果不一样，每一位都取最小值，那就肯定比1小啊！哈哈就是这么理解。 2、纹理相似度（texture similarity） 论文这里的纹理采用SIFT-Like特征，采用方差为1的高斯微分在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8310=240（使用RGB色彩空间）。计算公式如下所示： 3、尺度相似度尺度相似度其实是实现优先合并小的区域，也就是减少大区域合并小区域。4、交叠相似度（shape compatibility measure）上述完成后，要看一下两个区域是否爱挨着，如果距离十万八千里，那就没有合并的必要。如何计算？先找一个外接矩形把它框起来，如果两个矩形相距较远，则外接矩形就很大，所以我们设计这样的指标，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形BBij）减去两个矩形的尺寸，得到相对差距。其计算方式：候选区域排序相似度计算后还有一个问题，如何对候选区域排序呢，通过上述的步骤我们能够得到很多很多的区域，但是显然不是每个区域作为目标的可能性都是相同的，因此我们需要衡量这个可能性，这样就可以根据我们的需要筛选区域建议个数啦。 这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给他们乘以一个随机数，毕竟3分看运气嘛，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。这样我就得到了所有区域的目标分数，也就可以根据自己的需要选择需要多少个区域了。 CNN网络训练上述过程我们得到了大量候选区域（每个候选区域都提供了矩形框的位置情况），怎么训练呢？第一步、数据标注：我们对于每张图片标注一下boundbox四维向量和所属类别编号。第二步、预训练：论文在大型辅助训练集ILSVRC2012分类数据集（分类数据集没有约束框数据，每幅图片对应一个类别标签）上预训练了CNN。预训练采用了Caffe的CNN库。第三步、微调CNN：特定领域的参数调优第一步训练了一个分类器，但是我们要做的是目标检测，而且我们换了另一套数据集，所以为了让我们的CNN适应新的任务（即检测任务）和新的领域（变形后的候选窗口）。我们只使用变形后的候选区域对CNN参数进行SGD训练。1、网络修改：ImageNet是用了专用的1000-way分类层，这里我们换成21-way分类层，（其中20是VOC的类别数，1代表背景），卷积部分都没有改变。2、数据集选择性搜索算法会提取一些候选区域，如何区分正例，负例？这里我们考虑如果其和真实标注的框的IoU&gt;= 0.5就认为是正例，否则就是负例。OK这样前面选择性搜索的框到了后面就利用IOU自动标注，然后去softmax训练这样就训练了一个比较适合我们的候选框的分类器。完美！但是好像不好耶，重合度0.5就是正例，那不是好多一半框选目标就被认为合格了，一张图片会有好多的框合格对！那么你会想我们去把阈值调高点，不久解决了！这里作者考虑如果IOU太高那样正例就太少了，负例太多，譬如对于一张图片的2000候选框，1张合格，1999张不合格；那么训练会过拟合的！所以考虑这个，这里作者降低了IOU阈值（后面也有说）那么作者咋解决的？第四步~~第四步、多目标分类器这里作者就又搞了20个SVM分类器，上面第三步训练好了CNN了，我们把所有的图片走一遍CNN把最后的特征留下，不训练，把它保留当成特征，去训练SVM。这里把IOU&lt;0.3的认为是负样本。 另外样本数据很大，每幅图片都对应着很多的候选区域特征emmm这里论文采用了hard negative mining method训练时有时模型会把重叠率低的也当成正样本，这就属于顽固负样本，需要继续投进去训练。 【难负例挖掘算法，用途就是正负例数量不均衡，而负例分散代表性又不够的问题，hard negative就是每次把那些顽固的棘手的错误,再送回去继续练,练到你的成绩不再提升为止.这一个过程就叫做’hard negative mining‘】 这里你会想到为啥不直接CNN时直接搞一个分类器，还用啥SVM？对于这个问题我也有所疑惑，然后查了一下，大概这么解释还可以理清。 因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm。一旦CNN f7层特征被提取出来，那么我们将为每个物体累训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到20004096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包好了4096个W)，就可以得到结果了。 Bounding-box回归上面完成了分类，但其实回归效果不是很好，作者又在第五层卷积层采用了Bounding-box回归对候选区域做了修正，提高了边框的回归效果。 下图，绿色的框表示Ground Truth, 红色的框为Selective Search提取的Region Proposal。那么很明显框定位不准(IoU&lt;0.5)， 如果我们能对红色的框进行微调， 使得经过微调后的窗口跟Ground Truth 更接近， 这样岂不是定位会更准确。 确实，Bounding-box regression 就是用来微调这个窗口的。那么怎么变换呢?我们可以简单的看出可以通过平移、伸缩两种操作来实现。如下图所示，我们需要目标框的四维向量和合格的预选区域的向量，然后要训练一个线形回归模型，下图左边那个公式，这里计算损失公式如右下角所示。训练好后就可以用来预测了。具体哪个BoundingBox模型可以参考参考文献来学习。 测试环节非极大值抑制(Non-Maximum Suppression, NMS)训练好了怎么测试？测试阶段，在测试图像上使用selective search抽取2000个推荐区域（实验中，我们使用了选择性搜索的快速模式）。然后变形每一个推荐区域，再通过CNN前向传播计算出特征。然后我们使用对每个类别训练出的SVM给整个特征向量中的每个类别单独打分。然后给出一张图像中所有的打分区域，然后使用NMS（每个类别是独立进行的），拒绝掉一些和高分区域的IOU大于阈值的候选框。 非极大值抑制给出一张图片和上面许多物体检测的候选框（即每个框可能都代表某种物体），但是这些框很可能有互相重叠的部分，我们要做的就是只保留最优的框。假设有N个框，每个框被分类器计算得到的分数为Si, 1&lt;=i&lt;=N。0、建造一个存放待处理候选框的集合H，初始化为包含全部N个框；建造一个存放最优框的集合M，初始化为空集。1、将所有集合 H 中的框进行排序，选出分数最高的框 m，从集合 H 移到集合 M；2、遍历集合 H 中的框，分别与框 m 计算交并比（Interection-over-union，IoU），如果高于某个阈值（一般为0~0.5），则认为此框与 m 重叠，将此框从集合 H 中去除。3、回到第1步进行迭代，直到集合 H 为空。集合 M 中的框为我们所需。 用下面这张图可以看懂测试的流程。 总结本篇论文虽然已经算是早起的目标检测的论文，但依然被新手所研究，算是目标检测的开篇之作把！他给出了目标检测的一个最简单的思路，但是费时费力。 参考文献[1]【深度学习】R-CNN 论文解读及个人理解https://blog.csdn.net/briblue/article/details/82012575[2]R-CNN论文详解（论文翻译）https://blog.csdn.net/v1_vivian/article/details/78599229[3]选择性搜索（selective search）https://blog.csdn.net/guoyunfei20/article/details/78723646[4]第三十三节，目标检测之选择性搜索-Selective Searchhttps://www.cnblogs.com/zyly/p/9259392.html[5]边框回归：BoundingBox-Regression(BBR)https://blog.csdn.net/v1_vivian/article/details/80292569","link":"/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89R-CNN/"},{"title":"目标检测CNN经典网络（三）Fast R-CNN ，Faster R-CNN","text":"继R-CNN与SPP-net之后又出现了Fast R-CNN与Faster R-CNN,比之前的网络有了更快的速度，其准确率也没有降低。并且把端到端的概念引入到目标检测中。 Fast R-CNNFastR-CNN借鉴SPP-net思想加入ROI感兴趣池化，并将两个任务回归和分类同时进行，进行多任务训练。 ROI池化ROI感兴趣池化可以看作SPPpool的简化，下图可以看出，ROI只对感兴趣的区域做一次池化即可。考虑到感兴趣区域（RoI）尺寸不一，但是输入图中后面FC层的大小是一个统一的固定值，因为ROI池化层的作用类似于SPP-net中的SPP层，即将不同尺寸的RoI feature map池化成一个固定大小的feature map。具体操作：假设经过RoI池化后的固定大小为是一个超参数，因为输入的RoI feature map大小不一样，假设为，需要对这个feature map进行池化来减小尺寸，那么可以计算出池化窗口的尺寸为：h/H,w/W，即用这个计算出的窗口对RoI feature map做max pooling，Pooling对每一个feature map通道都是独立的。 多任务损失函数Fast R-CNN将两个任务并行运行到一起，其损失定义如下所示，考虑损失为分类损失和回归损失。 最终论文给出其实现的性能可以看下表，其将训练时间与测试时间大大缩短，尤其是单图的测试时间大大缩短，这功劳归结于一方面采用了先卷积后 Faster R-CNNFast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。（想较少测试的时间那就把任务交给深度学习，学习出一套参数可以以后一直用。）给这么一个方法起了个名字叫做Region Proposal Network(RPN)。 网络总图先看下面这张图，它是最终的测试网络。通过上面这张图可以看出Faster R-CNN由四个部分组成：1）卷积层(conv layers)，用于提取图片的特征，输入为整张图片，输出为提取出的特征称为feature maps2）RPN网络(Region Proposal Network)，用于推荐候选区域，这个网络是用来代替之前的search selective的。输入为图片为featrue maps，输出为多个候选区域，这里的细节会在后面详细介绍。3）RoI pooling，和Fast R-CNN一样，将不同大小的输入转换为固定长度的输出，输入输出和Fast R-CNN中RoI pooling一样。4）分类和回归，这一层的输出是最终目的，输出候选区域所属的类，和候选区域在图像中的精确位置。 那么其实你可以看出来关键的部分在于候选区域的改动，那么我们主要说下候选区域的方法。 RPN网络首先作者自己设想原图上或者说featuremap上每个位置可能会产生k中可能的区域，下图所示，作者给出的方案具体来说就是每个位置可能会出现三种面积可能，三种长宽表示样式。三种面积分别是128×128，256×256,512×512，每种面积又分成3种长宽比，分别是2:1,1:2,1:1 。呵呵一算是九种！所以它的k是9！然后RPN网络是这么干的，先将一幅图片经过CNN网络得到第五个卷积层，然后将第五个卷积层用3*3卷积核卷积得到一个256通道的特征图，之后再分为两路，一路用1*1卷积降维到原尺寸下2*k个通道（这是干啥，这2*k个分别对应是/否为对的候选区域，总共有k个框，所以是2*k个），另一路用1*1的卷积降维到原尺寸下4*k个通道（这个就是左上角坐标，右下角坐标，k种）ok！注意：这里尺寸没有变哎，因为每个位置都要输出这么个得分情况哎，我们最后确定比较好的中心位置哎还有它的尺寸。 那么如果RPN一套神经网络，FastR-CNN一套网络好像也不行哦，训练费事哦。那么RPN网络和FastR-CNN是如何组合的是如何权值共享的呢？ Faster R-CNN训练总结一下Faster R-CNN的训练过程如下所示：（1）先再imagenet上预训练一个CNN模型，得到一个初始的RPN网络（2）再另外训练一个imagenet模型CNN2，然后把在步骤1得到的Regionproposals用来训练FastR-CNN模型。（3）有了一个比较好的FastR-CNN模型也就是步骤2的模型，我们把这个CNN部分固定再去矫正RPN网络，此时CNN部分就用CNN2（把CNN1直接丢掉了！）然后去调参RPN后面那一部分，调好了这时的RPN模型基本可以了。（4）完事候选区域又变了，回去在调FastR-CNN模型，此时CNN部分不动了，这两个网络已经共享了！我们去调后面的全连接啥的。ok调好了就大功告成。可以看出训练的时候费事，但是选练好了后都是神经网络，测试时间就大大减少。 Faster R-CNN的主要贡献是设计了提取候选区域的网络RPN，代替了费时的选择性搜索，使得检测速度大幅提高。 参考文献[1]第三十一节，目标检测算法之 Faster R-CNN算法详解https://www.cnblogs.com/zyly/p/9247863.html[2]Faster R-CNN文章详细解读https://blog.csdn.net/liuxiaoheng1992/article/details/81843363[3]【目标检测】Fast R-CNN论文详解（Fast R-CNN）https://www.jianshu.com/p/fbbb21e1e390","link":"/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89Fast%20R-CNN%20%EF%BC%8CFaster%20R-CNN/"},{"title":"目标检测CNN经典网络（二）SPPnet","text":"在上文介绍了R-CNN网络，可以看出其存在很多问题：卷积网络的重复计算造成计算量大耗时多，之后出现了SPPnet提出先卷积再进行框选目标区域的方法。 总体思路首先看下面这幅图，R-CNN中我们在卷积之前需要先选定预选框，然后可能大小尺寸不一，需要对其进行crop或者warp一下同意尺寸再进行卷积网络；SPPnet提出我们可以先对原图进行卷积神经网络，再利用预选框去找出对应的第五层卷积网络所对应的特征图（因为我们发现其实再训练卷积网络时，原图和每层的卷积图实际位置关系还是不变的）全连接层我们单独放在后面，在进入全连接层之前我们做了一个金字塔池化SSP（后面讲！）到统一尺寸。 空间金字塔+最大池化在图像处理中有个图像金字塔的思想可以采样图像到任意大小，这里借鉴这个思想，我们把卷积网络的输出得到的13*13*256特征提取三个不同尺寸，分别是1*1（也就是原图），2*2（原图分为四份），4*4（原图分为16份），然后对每一份选区最大的一个值代表该区域也就是最大池化（一份包含很多像素点啊）。最后得到16+4+1=21 *256 的特征！这样不管上一层输出的尺寸多大，我们都能做这个变换！ 如果像上图那样将reponse map分成4x4 2x2 1x1三张子图，做max pooling后，出来的特征就是固定长度的(16+4+1)x256那么多的维度了。如果原图的输入不是224x224，出来的特征依然是(16+4+1)x256 更加通用的认识可以认为：输入尺寸在[180,224]之间，假设最后一个卷积层的输出大小为a×a，若给定金字塔层有n×n 个bins，进行滑动窗池化，窗口尺寸为win=⌈a/n⌉，步长为str=⌊a/n⌋，使用一个网络完成一个完整epoch的训练，之后切换到另外一个网络。只是在训练的时候用到多尺寸，测试时直接将SPPNet应用于任意尺寸的图像。这样我们得到了统一尺寸的特征在进行后面的全连接层训练即可。 空间金字塔在定位实验中的应用在定位实验中，需要我们将原图上的ROI映射到featuremap中，具体可以参考我的另一篇文章探讨卷积的感受视野以及sPPnet中ROI映射到featuremap我详细介绍了ROI映射的方法。对此，我们得到映射后的图片就可以在featuremap中找到对应的区域，然后再进行空间金字塔最大池化得到特征。金字塔用了{6x6 3x3 2x2 1x1}，共50个bin，分类器也是用了SVM。最后得到了很大的提升，主要在于时间！ 总结R-CNN提取特征比较耗时，需要对每个warp的区域进行学习，而SPPNet只对图像进行一次卷积，之后使用SPPNet在特征图上提取特征。结合EdgeBoxes提取的proposal，系统处理一幅图像需要0.5s。 参考文献[1]探讨卷积的感受视野以及sPPnet中ROI映射到featuremaphttps://blog.csdn.net/CLOUD_J/article/details/89917950[2]【目标检测】SPPNet算法详解https://blog.csdn.net/bryant_meng/article/details/78615353","link":"/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89SPPnet/"},{"title":"目标检测CNN经典网络（四）R-FCN","text":"前面所说的RCNN作为最早的目标检测网络，具有一些问题，其对每一个ROI都做一遍卷积（费时）；对于感兴趣区域的筛选也费时；随后FastR-CNN、Faster-CNN给出了解决方案。之后学者又去思考后面的全连接层能不能去掉，或者不让每一个感兴趣区域都走一遍全连接，这也很费时间啊！（我们了解在图像分类中，人们都很讨厌全连接层，参数量太大），为此R-FCN出现了！ 1回顾回顾R-CNN系列的方案都是在解决一个计算量计算时间的问题，R-CNN存在一些问题，SPP-net提出金字塔池化层，将卷积放在前面，实现了一张图卷积一次就行，降低了大量时间！Fast R-CNN提出了ROI池化，引入并行的多任务损失函数，也提高了效率！到了Faster R-CNN认为区域建议网络也可以环程卷积网络，提出了RPN网络~~。本篇文章介绍下一个迭代版本R-FCN将后面的全连接层分类层也换成卷积网络！ 2位置敏感性的理解写着一部分看了好多文章都没看懂，查了好多资料总结到如下，之后有新的理解会加上，如果朋友们有好的文章解释可以推荐给我。首先，对于图片分类问题，我们用比较深的卷积网络比较好，当网络比较深的时候，分类问题对于位置的敏感性就较差，因为一张图片通过一个深的网络到了后面再到全连接层，位置的变动对于后面特征的影响比较小了，所以它的识别还是会准确，这就是位置不敏感，平移不变性。然而对于定位问题，我们需要得到它的定位，网络过于深的化就会造成我们定位不准确，平移不变！所以构造一个浅的网络可能对于分类来说比较好。 再来看个Faster R-CNN + ResNet-101结构的例子。这个是参考文献3给出的，如果在Faster R-CNN中没有ROI层，直接对整个feature map进行分类和位置的回归，由于ResNet的结构较深，平移可变性较差，检测出来的坐标会极度不准确。具体网络可以自己查一下！ 如果在ResNet中间（conv4与conv5间）加个ROI层结果就不一样了，此时到了第四卷积层才给框选区域，坐标还是有的，此时分类器离得比较近了，分类器对于位置的敏感程度比较低了！也就是平移可变性！ROI层提取出的proposal中，有的对应前景label，有的对应背景label，proposal位置的偏移就有可能造成分类器（前景和背景分类）的不同。偏移后原来的前景很有可能变成了背景，原来的背景很有可能变成了前景，换句话说分类loss对proposal的位置是敏感的，这种情况ROI层给深层网络带来了平移可变性。 如果把ROI加到ResNet的最后一层（图1 conv5后）结果又是怎样呢？conv5的第一个卷积stride是2，造成conv5输出的feature map更小，这时proposal的一个小偏移在conv5输出上很有可能都感知不到，即proposal对应的label没有改变，此时识别出来的位置准确度会很差。 作者在考虑前人的实验后，想着把ROI向后靠一下是不是就能把全连接层也共享了！实则会导致位置的准确度降低的！所以考虑这些因素，他提出了位置敏感卷积网络。 3位置敏感卷积层网络下图为R-FCN网络（这里说明一下，下面应该是还有一个回归！），其中红色部分是修改后的分类器，我们可以看出前面卷积之后得到featuremap之后有一部分向上去做RPN网络了，这个和FasterR-CNN一样。另一路去做分类了~~这里你会看到它写的是conv卷积，对没错，我们用一个卷积得到一个具有深度为K*K*(C+1)的特征。C+1代表类别，C个类别加一个背景。K是啥意思？这个KK我们表示它属于某个类别上左、上中、上右……的可能性。下图中每个颜色块代表一个方位，一共3\\3=9个方位，每个方位其实是包含C+1个的。再通俗点黄色层其实不是一层有C+1个层，分别代表它属于每个类别左上角的概率！然后它不是有很多ROI，它用ROI套住一个区域，对于第一类，他需要从这9个大层中对应这个类的小层得到一个平均结果，然后把这9个数组合投票~看他是不是这个类。看下面这个图，每个大层有C+1个，然后对ROI抽取后得到饼图，有C+1个类所以是C+1层，再求一个得分得到C+1的向量。那么这时需要解决两个问题：First 对于一个ROI如何抽成一个点Second 投票怎么投？好，接下来说这个问题。First 对于一个ROI如何抽成一个点这里它直接采取平均值的方法，所有像素值的平均值来代表一个值。 Second 投票怎么投？对于投票，他也是计算的这K*K个值的平均值。哈哈！ 4位置敏感回归前面分类有了，那么回归位置呢？思路是一样的！前面分类是卷积为K*K*(C+1),回归的话就是四个参数啊，左上角右上角坐标！，那就是K*K*4,其他的就和上面一样了。即在ResNet的共享卷积层的最后一层上面连接一个与position-sensitive score map并行的score maps，该score maps用来进行regression操作，我们将其命名为regression score map，而该regression score map的维度应当是 4KK ，然后经过Position-sensitive RoI pooling操作后，每一个RoI就能得到4个值作为该RoI的x,y,w,h的偏移量了，其思路和分类完全相同。 5损失的定义然后就是怎么去定义这个损失，只有定义好损失，网络才能按照作者的设想去跑啊！，看下面这个图（图都是从小象课堂的PPT上截取的，感谢！）。先主要说一下分类损失，得到上面的投票，C+1的向量对这个值softmax归一化，其实就是都去指数幂次，然后把正确的做分子，这样看正确类别分类的得分是否够大，不大的话，呵呵损失就大，那么网络训练就回去矫正，那么为了让损失小，就得让这个类别的得分高，那么就得让前面的位置敏感卷积层的得分高~这样从理论上应该是能实现的。 6可视化结果论文中给出了其可视化的结果，如下图所示，可以看出，当这个ROI选对了，那么对于人这个类别，它得出的9个方位图在中间，我们可以看出左上角其实不太属于人，看中间那个图高亮的区域少；中间部位其实肯定属于人了，我们看出第五个格子几乎全部高亮，以此类推可以分析其他格子，最后计算每个格子的平均得到右图，可以看出其值比较高，属于人类。图2哪个区域选歪了之后就不一样了得分低一点，也很好分析！ 7实验效果作者首先做了一个实验去测试一下不同的位置敏感池化单元尺寸对结果的影响，可以看出1*1的尺寸，也就是对于一个ROI不分块，失败！3*3也就是分成左上左中左下九份效果还可以，7*7的效果最好。之后又做了对比试验，与之前的网络做了对比。可以看出效果得到了现主的提升，在保证效果的同时，时间更短！ 8参考文献[1]小象学堂课程PPT[2]R-FCN详解https://blog.csdn.net/WZZ18191171661/article/details/79481135[3]解答关于R-FCN的所有疑惑（原创）https://www.jianshu.com/p/409fd61db9db","link":"/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BCNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%EF%BC%88%E5%9B%9B%EF%BC%89R-FCN/"},{"title":"算法岗位实习&工作需求总结","text":"本帖总结一些算法岗位实习&amp;工作的要求；陆续会补一些面经。 计算机视觉算法实习 基本条件： 熟悉深度学习框架（Tensorflow/Torch/Keras/Caffe）； 了解基本深度学习网络（CNN，RNN）（RNN应该是在视频理解或者NLP中应用多） 熟悉Python语言；有时也需要C++、JAVA； 熟悉Linux系统（公司一般都是服务器下操作），Shell编程； 熟悉常见的计算机视觉的应用方向；以下我总结的但不限于图像分类（）、目标检测（Detection）、语义分割（sematic segmentation）、模型压缩、弱监督（weakly supervised learning）、超分辨率重建（super resolution）、生成对抗网络（GAN） 熟悉经典的深度学习模型（backbone基本结构） 了解ML算法（Boosting、SVM…） 优先条件： 有顶会论文/实际项目落地经验者；（一般普通实验室没有顶会…那就得有实习经历或者老板有项目…不然就看愿不愿意培养你） 重大比赛或者竞赛有荣誉；在Github上有影响大的开源代码； 自然语言算法实习面试 简历上项目分析；听一下做了啥，以及你的工作与工作量； 基础知识：深度学习（DropOut、过拟合、各种你知道的网络…） 编程题（必须刷题，其实大家都是刷题，但是面试官几乎都会考，那就只能干了！） 面经","link":"/posts/%E7%AE%97%E6%B3%95%E5%B2%97%E4%BD%8D%E5%AE%9E%E4%B9%A0&%E5%B7%A5%E4%BD%9C%E9%9C%80%E6%B1%82%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"CVPR","slug":"CVPR","link":"/tags/CVPR/"},{"name":"MSGAN","slug":"MSGAN","link":"/tags/MSGAN/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"faiss","slug":"faiss","link":"/tags/faiss/"},{"name":"GoogleClab","slug":"GoogleClab","link":"/tags/GoogleClab/"},{"name":"Pycharm","slug":"Pycharm","link":"/tags/Pycharm/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"wgan","slug":"wgan","link":"/tags/wgan/"},{"name":"YouTube-8M","slug":"YouTube-8M","link":"/tags/YouTube-8M/"},{"name":"anaconda","slug":"anaconda","link":"/tags/anaconda/"},{"name":"bazel","slug":"bazel","link":"/tags/bazel/"},{"name":"xml","slug":"xml","link":"/tags/xml/"},{"name":"csv","slug":"csv","link":"/tags/csv/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"glib","slug":"glib","link":"/tags/glib/"},{"name":"libffi","slug":"libffi","link":"/tags/libffi/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"insightface","slug":"insightface","link":"/tags/insightface/"},{"name":"labelme","slug":"labelme","link":"/tags/labelme/"},{"name":"软连接","slug":"软连接","link":"/tags/%E8%BD%AF%E8%BF%9E%E6%8E%A5/"},{"name":"mediapipe","slug":"mediapipe","link":"/tags/mediapipe/"},{"name":"pip","slug":"pip","link":"/tags/pip/"},{"name":"清华镜像","slug":"清华镜像","link":"/tags/%E6%B8%85%E5%8D%8E%E9%95%9C%E5%83%8F/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"webdriver","slug":"webdriver","link":"/tags/webdriver/"},{"name":"argparse","slug":"argparse","link":"/tags/argparse/"},{"name":"os","slug":"os","link":"/tags/os/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"screen","slug":"screen","link":"/tags/screen/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"人脸识别","slug":"人脸识别","link":"/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"选择性搜索","slug":"选择性搜索","link":"/tags/%E9%80%89%E6%8B%A9%E6%80%A7%E6%90%9C%E7%B4%A2/"},{"name":"FCN","slug":"FCN","link":"/tags/FCN/"},{"name":"AlexNet","slug":"AlexNet","link":"/tags/AlexNet/"},{"name":"VGG","slug":"VGG","link":"/tags/VGG/"},{"name":"多光谱","slug":"多光谱","link":"/tags/%E5%A4%9A%E5%85%89%E8%B0%B1/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"Google Drive","slug":"Google-Drive","link":"/tags/Google-Drive/"},{"name":"SPPnet","slug":"SPPnet","link":"/tags/SPPnet/"},{"name":"感受野","slug":"感受野","link":"/tags/%E6%84%9F%E5%8F%97%E9%87%8E/"},{"name":"opencv","slug":"opencv","link":"/tags/opencv/"},{"name":"cmake","slug":"cmake","link":"/tags/cmake/"},{"name":"玉渊潭","slug":"玉渊潭","link":"/tags/%E7%8E%89%E6%B8%8A%E6%BD%AD/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"}],"categories":[{"name":"计算机视觉","slug":"计算机视觉","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"LINUX","slug":"LINUX","link":"/categories/LINUX/"},{"name":"GAN","slug":"计算机视觉/GAN","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/GAN/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"工具","slug":"计算机视觉/工具","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%B7%A5%E5%85%B7/"},{"name":"人脸识别","slug":"计算机视觉/人脸识别","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/categories/TensorFlow/"},{"name":"图像生成","slug":"计算机视觉/图像生成","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"},{"name":"Debug","slug":"Debug","link":"/categories/Debug/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"目标检测","slug":"计算机视觉/目标检测","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"网络爬虫","slug":"python/网络爬虫","link":"/categories/python/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"语义分割","slug":"计算机视觉/语义分割","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"},{"name":"图像分类","slug":"计算机视觉/图像分类","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"},{"name":"刷题","slug":"刷题","link":"/categories/%E5%88%B7%E9%A2%98/"},{"name":"生活随笔","slug":"生活随笔","link":"/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"},{"name":"工作","slug":"工作","link":"/categories/%E5%B7%A5%E4%BD%9C/"}]}